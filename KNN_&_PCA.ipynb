{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaN7RkNXabwO"
      },
      "outputs": [],
      "source": [
        "Theoretical\n",
        "\n",
        "1. What is K-Nearest Neighbors (KNN) and how does it work?\n",
        "K-Nearest Neighbors (KNN) Overview\n",
        "K-Nearest Neighbors (KNN) is a supervised learning algorithm used for classification and regression tasks. It is a non-parametric, instance-based (lazy learning) algorithm, meaning it does not make strong assumptions about the data and delays computations until a query is made.\n",
        "\n",
        "How KNN Works\n",
        "Training Phase:\n",
        "\n",
        "Unlike other algorithms, KNN does not explicitly learn a model during training. Instead, it just stores the training data.\n",
        "\n",
        "Prediction Phase:\n",
        "\n",
        "When a new data point (query) needs classification or regression, the algorithm:\n",
        "\n",
        "Computes the distance (e.g., Euclidean, Manhattan, or Minkowski distance) between the query point and all training points.\n",
        "\n",
        "Finds the K nearest neighbors (based on the smallest distances).\n",
        "\n",
        "Determines the output:\n",
        "\n",
        "For Classification: The majority class among the K neighbors is assigned to the query point (majority voting).\n",
        "\n",
        "For Regression: The average (or weighted average) of the K neighbors' values is used as the prediction.\n",
        "\n",
        "Key Parameters of KNN\n",
        "K (Number of Neighbors):\n",
        "\n",
        "A small K may lead to overfitting, while a large K may result in underfitting.\n",
        "\n",
        "Common practice is to choose an odd K (like 3, 5, or 7) to avoid ties in classification.\n",
        "\n",
        "Distance Metric:\n",
        "\n",
        "Euclidean Distance (default):\n",
        "ùëë\n",
        "(\n",
        "ùëù\n",
        ",\n",
        "ùëû\n",
        ")\n",
        "=\n",
        "‚àë\n",
        "(\n",
        "ùëù\n",
        "ùëñ\n",
        "‚àí\n",
        "ùëû\n",
        "ùëñ\n",
        ")\n",
        "2\n",
        "d(p,q)=\n",
        "‚àë(p\n",
        "i\n",
        "‚Äã\n",
        " ‚àíq\n",
        "i\n",
        "‚Äã\n",
        " )\n",
        "2\n",
        "\n",
        "‚Äã\n",
        "\n",
        "\n",
        "Manhattan Distance:\n",
        "ùëë\n",
        "(\n",
        "ùëù\n",
        ",\n",
        "ùëû\n",
        ")\n",
        "=\n",
        "‚àë\n",
        "‚à£\n",
        "ùëù\n",
        "ùëñ\n",
        "‚àí\n",
        "ùëû\n",
        "ùëñ\n",
        "‚à£\n",
        "d(p,q)=‚àë‚à£p\n",
        "i\n",
        "‚Äã\n",
        " ‚àíq\n",
        "i\n",
        "‚Äã\n",
        " ‚à£\n",
        "\n",
        "Minkowski Distance (generalized form)\n",
        "\n",
        "Weighting Strategy:\n",
        "\n",
        "Uniform weighting (each neighbor contributes equally)\n",
        "\n",
        "Distance-weighted (closer points contribute more)\n",
        "\n",
        "Pros and Cons\n",
        "‚úÖ Advantages:\n",
        "\n",
        "Simple and intuitive.\n",
        "\n",
        "Works well with small datasets.\n",
        "\n",
        "Can handle multi-class classification.\n",
        "\n",
        "‚ùå Disadvantages:\n",
        "\n",
        "Computationally expensive for large datasets.\n",
        "\n",
        "Sensitive to irrelevant or highly correlated features.\n",
        "\n",
        "Performance depends on the choice of K and distance metric.\n",
        "\n",
        "\n",
        "2.What is the difference between KNN Classification and KNN Regression?\n",
        "\n",
        "\n",
        "Difference Between KNN Classification and KNN Regression\n",
        "Feature\tKNN Classification\tKNN Regression\n",
        "Output Type\tDiscrete (Class Labels)\tContinuous (Numerical Values)\n",
        "Prediction Strategy\tMajority voting among K neighbors\tAverage (or weighted average) of K neighbors' values\n",
        "Use Case\tUsed for categorical outputs (e.g., genre classification)\tUsed for numerical outputs (e.g., predicting track popularity)\n",
        "Example\tClassifying a song as Hip-hop, Pop, or R&B\tPredicting a song‚Äôs popularity score based on features like duration and artist collaboration\n",
        "Decision Boundary\tCreates well-defined class regions\tProduces smoother transitions between values\n",
        "Example Scenarios\n",
        "KNN Classification:\n",
        "\n",
        "Predicting whether a song is \"popular\" or \"not popular\" based on its features.\n",
        "\n",
        "Identifying the genre of a song based on audio characteristics.\n",
        "\n",
        "KNN Regression:\n",
        "\n",
        "Predicting the exact popularity score of a track.\n",
        "\n",
        "Estimating the duration of a song based on other numerical features.\n",
        "\n",
        "\n",
        "3. What is the role of the distance metric in KNN?\n",
        "\n",
        "\n",
        "The distance metric in K-Nearest Neighbors (KNN) plays a crucial role in determining how the algorithm identifies the most similar data points. Since KNN relies on the proximity of data points to make predictions, the choice of distance metric directly affects classification and regression accuracy. Different metrics measure distance in unique ways, and selecting the appropriate one depends on the nature of the dataset.\n",
        "\n",
        "The most commonly used distance metric is Euclidean distance, which calculates the straight-line distance between two points in a multi-dimensional space. It is effective when the dataset features are continuous and have similar scales. Another frequently used metric is Manhattan distance, which sums the absolute differences between feature values. This approach works well when dealing with grid-like structures or datasets where movement is restricted to one axis at a time.\n",
        "\n",
        "For datasets that involve varying scales or distributions, Minkowski distance provides a more generalized approach, allowing a user to adjust the formula to behave like either Euclidean or Manhattan distance by changing a parameter. Additionally, Cosine similarity is often used when working with high-dimensional data, such as text or music recommendation systems, where the angle between vectors is more informative than their absolute distance.\n",
        "\n",
        "The choice of distance metric can significantly impact the performance of KNN. If features are on different scales, improper metric selection can lead to biased predictions. Standardizing or normalizing data often helps mitigate these issues, ensuring that no single feature disproportionately influences the calculations. Would you like a practical example demonstrating the effect of different distance metrics on your Spotify dataset?\n",
        "\n",
        "\n",
        "4. What is the Curse of Dimensionality in KNN?\n",
        "\n",
        "\n",
        "Curse of Dimensionality in KNN\n",
        "The Curse of Dimensionality refers to the challenges that arise when working with high-dimensional data in K-Nearest Neighbors (KNN) and other machine learning algorithms. As the number of features (dimensions) increases, the performance of KNN can degrade due to the following reasons:\n",
        "\n",
        "1. Distance Becomes Less Meaningful\n",
        "In high-dimensional spaces, all data points tend to become equidistant from each other.\n",
        "\n",
        "Since KNN relies on finding the \"nearest\" neighbors, if all points are almost equally distant, the algorithm struggles to distinguish relevant neighbors from irrelevant ones.\n",
        "\n",
        "Example: In a 2D space, the difference between two points may be clear, but in a 100D space, most points appear similarly distant.\n",
        "\n",
        "2. Increased Computational Complexity\n",
        "KNN must compute distances between the query point and every training point.\n",
        "\n",
        "As the number of dimensions increases, the computational cost grows significantly.\n",
        "\n",
        "High-dimensional datasets require more memory and time for distance calculations.\n",
        "\n",
        "3. Sparse Data Distribution\n",
        "In high dimensions, data points spread out, and meaningful clusters become harder to detect.\n",
        "\n",
        "If data is sparse, finding K meaningful neighbors becomes difficult, leading to unreliable predictions.\n",
        "\n",
        "4. Overfitting Risk\n",
        "With many dimensions, KNN can overfit due to excessive noise and irrelevant features.\n",
        "\n",
        "Some dimensions may contain little to no useful information, but they still contribute to distance calculations, reducing accuracy.\n",
        "\n",
        "How to Overcome the Curse of Dimensionality in KNN?\n",
        "‚úÖ Feature Selection ‚Äì Remove irrelevant or redundant features to reduce dimensionality.\n",
        "‚úÖ Dimensionality Reduction ‚Äì Apply Principal Component Analysis (PCA) or t-SNE to reduce the number of dimensions while preserving important information.\n",
        "‚úÖ Feature Scaling ‚Äì Normalize or standardize features to ensure fair distance calculations.\n",
        "‚úÖ Choose an Appropriate Distance Metric ‚Äì In high-dimensional spaces, Cosine Similarity may work better than Euclidean distance.\n",
        "\n",
        " 5. How can we choose the best value of K in KNN?\n",
        "\n",
        "\n",
        "How to Choose the Best Value of K in KNN?\n",
        "Choosing the right K (number of neighbors) is crucial for achieving good model performance in K-Nearest Neighbors (KNN). The value of K controls the trade-off between bias and variance:\n",
        "\n",
        "Small K (e.g., 1, 3, 5) ‚Üí More flexible, low bias, high variance (risk of overfitting).\n",
        "\n",
        "Large K (e.g., 50, 100) ‚Üí More stable, high bias, low variance (risk of underfitting).\n",
        "\n",
        "Methods to Select the Optimal K\n",
        "1Ô∏è‚É£ Elbow Method (Using Cross-Validation)\n",
        "\n",
        "Train KNN for different values of K.\n",
        "\n",
        "Plot error rate (or accuracy) vs. K.\n",
        "\n",
        "Choose K where the error starts to stabilize (\"elbow point\").\n",
        "\n",
        "2Ô∏è‚É£ Grid Search with Cross-Validation\n",
        "\n",
        "Use GridSearchCV (from sklearn) to test multiple K values.\n",
        "\n",
        "Select the K with the best cross-validation accuracy.\n",
        "\n",
        "3Ô∏è‚É£ Odd vs. Even K\n",
        "\n",
        "For classification, choose an odd K to avoid tie-breaking issues.\n",
        "\n",
        "For regression, even K values can work since averaging is used.\n",
        "\n",
        "4Ô∏è‚É£ Rule of Thumb\n",
        "\n",
        "K is often chosen as:\n",
        "\n",
        "ùêæ\n",
        "=\n",
        "ùëÅ\n",
        "K=\n",
        "N\n",
        "‚Äã\n",
        "\n",
        "where N is the number of training samples.\n",
        "\n",
        "Example: Finding the Best K for a Spotify Popularity Model\n",
        "\n",
        "6. What are KD Tree and Ball Tree in KNN?\n",
        "\n",
        "\n",
        "KD Tree and Ball Tree in KNN\n",
        "K-Nearest Neighbors (KNN) is computationally expensive because it requires calculating distances between a query point and all training points. KD Tree and Ball Tree are data structures used to speed up nearest neighbor searches, making KNN more efficient.\n",
        "\n",
        "1Ô∏è‚É£ KD Tree (K-Dimensional Tree)\n",
        "üîπ What is it?\n",
        "\n",
        "A binary tree-based data structure that recursively splits data along different dimensions.\n",
        "\n",
        "Works well when dimensions (features) are low to moderate (usually < 30).\n",
        "\n",
        "üîπ How it Works:\n",
        "\n",
        "Choose a splitting dimension (e.g., x, y, z for 3D data).\n",
        "\n",
        "Select a median value along that dimension.\n",
        "\n",
        "Recursively split data into left and right subtrees.\n",
        "\n",
        "During query, traverse the tree to find nearest neighbors efficiently.\n",
        "\n",
        "üîπ Best For:\n",
        "‚úÖ Low to moderate-dimensional data.\n",
        "‚úÖ Faster than brute-force search when dimensions are ‚â§ 30.\n",
        "\n",
        "üîπ Limitations:\n",
        "‚ùå Inefficient in high-dimensional spaces (curse of dimensionality).\n",
        "\n",
        "2Ô∏è‚É£ Ball Tree\n",
        "üîπ What is it?\n",
        "\n",
        "A hierarchical clustering structure that groups points into hyperspherical regions (‚Äúballs‚Äù).\n",
        "\n",
        "Works well for high-dimensional data (>30 dimensions).\n",
        "\n",
        "üîπ How it Works:\n",
        "\n",
        "Partition data into a tree of nested balls (spheres) based on distances.\n",
        "\n",
        "Use triangle inequality to prune unnecessary distance computations.\n",
        "\n",
        "When searching for neighbors, avoid checking far-away regions.\n",
        "\n",
        "üîπ Best For:\n",
        "‚úÖ High-dimensional data (better than KD Tree for >30 dimensions).\n",
        "‚úÖ Works well with non-axis-aligned data distributions.\n",
        "\n",
        "üîπ Limitations:\n",
        "‚ùå More complex to construct than KD Tree.\n",
        "‚ùå Slower for very low-dimensional data.\n",
        "\n",
        "Which One to Use in KNN?\n",
        "Scenario\tKD Tree\tBall Tree\n",
        "Low-dimensional data (<30)\t‚úÖ Best choice\t‚ùå Not optimal\n",
        "High-dimensional data (>30)\t‚ùå Becomes inefficient\t‚úÖ Works better\n",
        "Large datasets\t‚úÖ Faster than brute-force\t‚úÖ Works well\n",
        "Non-axis-aligned data\t‚ùå May not work well\t‚úÖ Handles better\n",
        "üëâ In Scikit-Learn (sklearn.neighbors.KNeighborsClassifier and KNeighborsRegressor), you can choose:\n",
        "\n",
        "\"auto\" ‚Üí Automatically picks the best structure (KD Tree or Ball Tree).\n",
        "\n",
        "\"kd_tree\" ‚Üí Uses KD Tree.\n",
        "\n",
        "\"ball_tree\" ‚Üí Uses Ball Tree.\n",
        "\n",
        "\"brute\" ‚Üí Uses brute-force search.\n",
        "\n",
        "7. When should you use KD Tree vs. Ball Tree?\n",
        "\n",
        "\n",
        "When Should You Use KD Tree vs. Ball Tree?\n",
        "Choosing between KD Tree and Ball Tree depends on the dataset's dimensionality, size, and structure. Here's a comparison to help decide:\n",
        "\n",
        "Criteria\tKD Tree ‚úÖ\tBall Tree ‚úÖ\n",
        "Dimensionality\tWorks best for low to moderate dimensions (‚â§30 features).\tBetter for high-dimensional data (>30 features).\n",
        "Data Distribution\tBest for axis-aligned data (features with clear split boundaries).\tWorks well for non-axis-aligned data (e.g., spherical clusters).\n",
        "Computational Speed\tFaster than Ball Tree when dimensions are low.\tFaster than KD Tree in high dimensions.\n",
        "Memory Usage\tLess memory-intensive than Ball Tree.\tRequires more memory due to hierarchical clustering.\n",
        "Training Time\tQuick to construct.\tTakes longer to construct but can be efficient for queries.\n",
        "Query Efficiency\tSlows down significantly as dimensions increase.\tMore efficient for nearest neighbor search in high-dimensional space.\n",
        "Example Use Case\tFinding similar songs based on tempo & duration (few numeric features).\tFinding similar songs based on complex audio features (many dimensions).\n",
        "If dimensions ‚â§ 30 ‚Üí Use KD Tree\n",
        "\n",
        "If dimensions > 30 ‚Üí Use Ball Tree\n",
        "\n",
        "If unsure ‚Üí Use \"auto\" in Scikit-Learn, which picks the best one automatically.\n",
        "\n",
        "\n",
        "8. What are the disadvantages of KNN?\n",
        "\n",
        "\n",
        "Disadvantages of K-Nearest Neighbors (KNN)\n",
        "Despite its simplicity, KNN has several drawbacks that can impact performance, especially on large or high-dimensional datasets. Here are the key disadvantages:\n",
        "\n",
        "1Ô∏è‚É£ Computational Cost (Slow on Large Datasets)\n",
        "KNN is a lazy learner, meaning it doesn‚Äôt learn a model during training. Instead, it stores the entire dataset and performs computations at prediction time.\n",
        "\n",
        "Time Complexity:\n",
        "\n",
        "Brute-force search: O(N √ó D) (where N = number of training samples, D = number of dimensions).\n",
        "\n",
        "KD Tree / Ball Tree can speed it up, but they degrade in high dimensions.\n",
        "\n",
        "‚úÖ Solution: Use KD Tree / Ball Tree for speedup or approximate nearest neighbors (ANN) techniques.\n",
        "\n",
        "2Ô∏è‚É£ High Memory Usage\n",
        "Since KNN stores all training data, it requires large memory when dealing with big datasets.\n",
        "\n",
        "Problem: If there are millions of data points, KNN becomes impractical.\n",
        "\n",
        "‚úÖ Solution: Reduce dataset size using feature selection or dimensionality reduction (PCA, t-SNE).\n",
        "\n",
        "3Ô∏è‚É£ Curse of Dimensionality\n",
        "As the number of features (dimensions) increases, distances become less meaningful, and all points appear similar.\n",
        "\n",
        "KD Tree and Ball Tree become ineffective for D > 30.\n",
        "\n",
        "‚úÖ Solution: Use dimensionality reduction techniques (PCA, LDA) or switch to distance metrics like Cosine Similarity.\n",
        "\n",
        "4Ô∏è‚É£ Sensitive to Noisy and Irrelevant Features\n",
        "KNN treats all features equally, even if some are irrelevant or noisy.\n",
        "\n",
        "Example: If predicting song popularity, \"track duration\" may be more important than \"track ID\", but KNN doesn‚Äôt know that.\n",
        "\n",
        "‚úÖ Solution: Apply feature selection (e.g., Mutual Information, Recursive Feature Elimination).\n",
        "\n",
        "5Ô∏è‚É£ Imbalanced Data Issues\n",
        "If one class is much more frequent than others, KNN may predict the majority class most of the time.\n",
        "\n",
        "Example: If 90% of the songs in a dataset are \"popular\" and only 10% are \"not popular,\" KNN will favor the majority class.\n",
        "\n",
        "‚úÖ Solution: Use weighted KNN, where closer neighbors have higher influence.\n",
        "\n",
        "6Ô∏è‚É£ Difficult to Choose the Best K\n",
        "Too small K ‚Üí Overfitting (high variance, sensitive to noise).\n",
        "\n",
        "Too large K ‚Üí Underfitting (high bias, loses local patterns).\n",
        "\n",
        "Finding the best K often requires trial and error or cross-validation.\n",
        "\n",
        "‚úÖ Solution: Use the Elbow Method or GridSearchCV to find the optimal K.\n",
        "\n",
        "üìå Summary of Disadvantages & Solutions\n",
        "Disadvantage\tSolution\n",
        "Slow on large datasets\tKD Tree, Ball Tree, Approximate Nearest Neighbors (ANN)\n",
        "High memory usage\tFeature selection, dimensionality reduction\n",
        "Curse of dimensionality\tPCA, LDA, t-SNE, Cosine Similarity\n",
        "Sensitive to noise & irrelevant features\tFeature scaling, feature selection\n",
        "Imbalanced data\tWeighted KNN, SMOTE (oversampling)\n",
        "Choosing K is tricky\tCross-validation, Elbow Method\n",
        "\n",
        "\n",
        "9. How does feature scaling affect KNN?\n",
        "\n",
        "\n",
        "How Does Feature Scaling Affect KNN?\n",
        "Feature scaling is critical in K-Nearest Neighbors (KNN) because KNN is a distance-based algorithm. If features have different scales, the distance metric (e.g., Euclidean distance) will be dominated by larger-scale features, leading to biased predictions.\n",
        "\n",
        "1Ô∏è‚É£ Why is Feature Scaling Important for KNN?\n",
        "KNN calculates distances between points (e.g., Euclidean, Manhattan).\n",
        "\n",
        "Unscaled features distort distance measurements, making certain features dominate.\n",
        "\n",
        "Example:\n",
        "\n",
        "Suppose a dataset has track duration (seconds) ranging from 100 to 500 and popularity score from 0 to 100.\n",
        "\n",
        "Since track duration values are much larger, they will overshadow the influence of popularity in distance calculations.\n",
        "\n",
        "KNN will end up treating \"track duration\" as more important than \"popularity.\"\n",
        "\n",
        "‚úÖ Solution: Normalize or standardize features before applying KNN.\n",
        "\n",
        "2Ô∏è‚É£ Common Feature Scaling Methods for KNN\n",
        "üîπ (1) Min-Max Scaling (Normalization)\n",
        "Formula:\n",
        "\n",
        "ùëã\n",
        "scaled\n",
        "=\n",
        "ùëã\n",
        "‚àí\n",
        "ùëã\n",
        "min\n",
        "ùëã\n",
        "max\n",
        "‚àí\n",
        "ùëã\n",
        "min\n",
        "X\n",
        "scaled\n",
        "‚Äã\n",
        " =\n",
        "X\n",
        "max\n",
        "‚Äã\n",
        " ‚àíX\n",
        "min\n",
        "‚Äã\n",
        "\n",
        "X‚àíX\n",
        "min\n",
        "‚Äã\n",
        "\n",
        "‚Äã\n",
        "\n",
        "Scales features between 0 and 1.\n",
        "\n",
        "Best when features have different ranges but no extreme outliers.\n",
        "\n",
        "Example (before and after scaling):\n",
        "\n",
        "Track Duration (sec)\tPopularity Score\tNormalized Duration\tNormalized Popularity\n",
        "200\t50\t0.25\t0.50\n",
        "300\t70\t0.50\t0.70\n",
        "500\t90\t1.00\t0.90\n",
        "üìå Use when: ‚úîÔ∏è Features have different scales.\n",
        "‚úîÔ∏è Data does not have extreme outliers.\n",
        "\n",
        "üîπ (2) Standardization (Z-score Normalization)\n",
        "Formula:\n",
        "\n",
        "ùëã\n",
        "scaled\n",
        "=\n",
        "ùëã\n",
        "‚àí\n",
        "ùúá\n",
        "ùúé\n",
        "X\n",
        "scaled\n",
        "‚Äã\n",
        " =\n",
        "œÉ\n",
        "X‚àíŒº\n",
        "‚Äã\n",
        "\n",
        "Mean-centered with unit variance (mean = 0, std = 1).\n",
        "\n",
        "Helps when features have different units and outliers.\n",
        "\n",
        "Example:\n",
        "\n",
        "Track Duration (sec)\tPopularity Score\tStandardized Duration\tStandardized Popularity\n",
        "200\t50\t-1.2\t-0.8\n",
        "300\t70\t-0.2\t0.4\n",
        "500\t90\t1.5\t1.3\n",
        "üìå Use when: ‚úîÔ∏è Data has outliers.\n",
        "‚úîÔ∏è Features have different distributions.\n",
        "\n",
        "3Ô∏è‚É£ Impact of Feature Scaling on KNN Performance\n",
        "Before Scaling ‚Üí Distances are skewed by large-scale features.\n",
        "\n",
        "After Scaling ‚Üí All features contribute equally to distance calculations.\n",
        "\n",
        "üîπ Without Scaling ‚Üí Poor classification & wrong neighbors chosen.\n",
        "üîπ With Scaling ‚Üí Improved accuracy & better nearest neighbors.\n",
        "\n",
        "üìå Key Takeaways\n",
        "Aspect\tMin-Max Scaling\tStandardization (Z-score)\n",
        "Scales Between\t0 to 1\tMean = 0, Std = 1\n",
        "Handles Outliers?\t‚ùå No\t‚úÖ Yes\n",
        "Best For\tFeatures with similar distributions\tFeatures with different distributions & outliers\n",
        "Used in KNN?\t‚úÖ Yes\t‚úÖ Yes\n",
        "\n",
        "10. What is PCA (Principal Component Analysis)?\n",
        "\n",
        "\n",
        "What is PCA (Principal Component Analysis)?\n",
        "Principal Component Analysis (PCA) is a dimensionality reduction technique used in machine learning and data analysis. It transforms high-dimensional data into a lower-dimensional space while preserving as much variance as possible.\n",
        "\n",
        "1Ô∏è‚É£ Why Use PCA?\n",
        "üîπ Curse of Dimensionality ‚Üí Too many features can slow down algorithms like KNN and lead to overfitting.\n",
        "üîπ Feature Correlation ‚Üí PCA removes redundancy by combining highly correlated features.\n",
        "üîπ Visualization ‚Üí Reduces complex datasets to 2D or 3D for easy visualization.\n",
        "üîπ Speeds Up Computation ‚Üí Reducing features improves efficiency in distance-based algorithms like KNN.\n",
        "\n",
        "‚úÖ Example in Your Spotify Dataset: If you have 30 audio features per song, PCA can reduce them to 5-10 key features without losing much information.\n",
        "\n",
        "2Ô∏è‚É£ How Does PCA Work?\n",
        "PCA finds new axes (principal components) that maximize variance while minimizing information loss.\n",
        "\n",
        "üìå Step-by-Step Process\n",
        "Standardize the Data (mean = 0, variance = 1).\n",
        "\n",
        "Compute the Covariance Matrix ‚Üí Measures how features are related.\n",
        "\n",
        "Find Eigenvalues & Eigenvectors ‚Üí Identify the most important directions in the data.\n",
        "\n",
        "Select Top K Principal Components ‚Üí Choose the top components that explain most of the variance.\n",
        "\n",
        "Transform Data ‚Üí Project original data onto the new lower-dimensional space.\n",
        "\n",
        "3Ô∏è‚É£ Example: PCA on Spotify Song Features\n",
        "Imagine you have these features:\n",
        "üéµ Danceability, Tempo, Energy, Loudness, Duration, Popularity\n",
        "\n",
        "Before PCA (Original Features - 6D)\n",
        "Song\tDanceability\tTempo\tEnergy\tLoudness\tDuration\tPopularity\n",
        "A\t0.8\t120\t0.9\t-5.0\t210\t85\n",
        "B\t0.6\t100\t0.7\t-7.2\t180\t70\n",
        "After PCA (Reduced to 2D)\n",
        "Song\tPC1 (Main Trend)\tPC2 (Secondary Trend)\n",
        "A\t1.45\t-0.30\n",
        "B\t0.92\t0.12\n",
        "üëâ PCA captures most of the variation in just two features instead of six! üöÄ\n",
        "\n",
        "4Ô∏è‚É£ When to Use PCA in KNN?\n",
        "‚úÖ Before KNN if dataset has many features (e.g., 30+ features in Spotify data).\n",
        "‚úÖ If features are highly correlated (PCA removes redundancy).\n",
        "‚úÖ When reducing dimensions improves model speed without sacrificing accuracy.\n",
        "\n",
        "\n",
        "11. How does PCA work?\n",
        "\n",
        "\n",
        "How Does PCA (Principal Component Analysis) Work?\n",
        "PCA works by transforming a high-dimensional dataset into a lower-dimensional space while preserving as much variance as possible. It does this by identifying new principal components (PCs), which are linear combinations of the original features.\n",
        "\n",
        "üìå Step-by-Step Process of PCA\n",
        "Let's break PCA down into five key steps:\n",
        "\n",
        "1Ô∏è‚É£ Standardize the Data\n",
        "Since PCA is affected by scale, we first normalize or standardize the dataset to ensure all features contribute equally.\n",
        "\n",
        "ùëã\n",
        "scaled\n",
        "=\n",
        "ùëã\n",
        "‚àí\n",
        "ùúá\n",
        "ùúé\n",
        "X\n",
        "scaled\n",
        "‚Äã\n",
        " =\n",
        "œÉ\n",
        "X‚àíŒº\n",
        "‚Äã\n",
        "\n",
        "üëâ This ensures each feature has mean = 0 and variance = 1.\n",
        "\n",
        "2Ô∏è‚É£ Compute the Covariance Matrix\n",
        "The covariance matrix captures relationships between different features.\n",
        "\n",
        "If two features are highly correlated, PCA will combine them into one principal component.\n",
        "\n",
        "Œ£\n",
        "=\n",
        "1\n",
        "ùëõ\n",
        "(\n",
        "ùëã\n",
        "ùëá\n",
        "ùëã\n",
        ")\n",
        "Œ£=\n",
        "n\n",
        "1\n",
        "‚Äã\n",
        " (X\n",
        "T\n",
        " X)\n",
        "üëâ This helps us find the directions where the data varies the most.\n",
        "\n",
        "3Ô∏è‚É£ Compute Eigenvalues & Eigenvectors\n",
        "Eigenvectors represent the direction of the new feature space (principal components).\n",
        "\n",
        "Eigenvalues represent the importance of each principal component (variance captured).\n",
        "\n",
        "üîπ We solve this equation:\n",
        "\n",
        "Œ£\n",
        "ùë£\n",
        "=\n",
        "ùúÜ\n",
        "ùë£\n",
        "Œ£v=Œªv\n",
        "where:\n",
        "\n",
        "ùë£\n",
        "v = Eigenvector (Principal Component Direction)\n",
        "\n",
        "ùúÜ\n",
        "Œª = Eigenvalue (Amount of Variance Captured)\n",
        "\n",
        "üëâ Larger eigenvalues = More important principal components.\n",
        "\n",
        "4Ô∏è‚É£ Select the Top K Principal Components\n",
        "Rank eigenvalues from highest to lowest.\n",
        "\n",
        "Choose the top K components that explain the most variance.\n",
        "\n",
        "The proportion of variance explained (PVE) by each component is:\n",
        "\n",
        "ùëÉ\n",
        "ùëâ\n",
        "ùê∏\n",
        "=\n",
        "ùúÜ\n",
        "ùëñ\n",
        "‚àë\n",
        "ùúÜ\n",
        "PVE=\n",
        "‚àëŒª\n",
        "Œª\n",
        "i\n",
        "‚Äã\n",
        "\n",
        "‚Äã\n",
        "\n",
        "üîπ Elbow Rule ‚Üí Choose K where cumulative variance stabilizes (e.g., 95% variance).\n",
        "\n",
        "5Ô∏è‚É£ Transform the Data into the New Feature Space\n",
        "The dataset is projected onto the selected principal components using matrix multiplication:\n",
        "\n",
        "ùëã\n",
        "PCA\n",
        "=\n",
        "ùëã\n",
        "‚ãÖ\n",
        "ùëâ\n",
        "ùêæ\n",
        "X\n",
        "PCA\n",
        "‚Äã\n",
        " =X‚ãÖV\n",
        "K\n",
        "‚Äã\n",
        "\n",
        "where:\n",
        "\n",
        "ùëã\n",
        "X = Original data\n",
        "\n",
        "ùëâ\n",
        "ùêæ\n",
        "V\n",
        "K\n",
        "‚Äã\n",
        "  = Matrix of top K eigenvectors\n",
        "\n",
        "ùëã\n",
        "PCA\n",
        "X\n",
        "PCA\n",
        "‚Äã\n",
        "  = Transformed lower-dimensional data\n",
        "\n",
        "üîπ Now, we have a dataset with K principal components instead of original features.\n",
        "\n",
        "üéµ Example: PCA on a Spotify Dataset\n",
        "Original Features (6D)\n",
        "Song\tDanceability\tTempo\tEnergy\tLoudness\tDuration\tPopularity\n",
        "A\t0.8\t120\t0.9\t-5.0\t210\t85\n",
        "B\t0.6\t100\t0.7\t-7.2\t180\t70\n",
        "After PCA (Reduced to 2D)\n",
        "Song\tPC1 (Main Trend)\tPC2 (Secondary Trend)\n",
        "A\t1.45\t-0.30\n",
        "B\t0.92\t0.12\n",
        "üëâ PCA reduced 6 features to just 2, preserving most of the variation. üöÄ\n",
        "\n",
        "üìå When to Use PCA?\n",
        "‚úÖ High-dimensional datasets (30+ features).\n",
        "‚úÖ Features are correlated (PCA removes redundancy).\n",
        "‚úÖ Need to speed up models (like KNN, SVM).\n",
        "‚úÖ Want to visualize data in 2D/3D.\n",
        "\n",
        "\n",
        "12. What is the geometric intuition behind PCA?\n",
        "\n",
        "\n",
        "Geometric Intuition Behind PCA\n",
        "PCA can be understood geometrically as finding the best lower-dimensional space that captures the most variance in the data. Instead of looking at individual features, PCA finds new axes (principal components) that best describe the data.\n",
        "\n",
        "üìå Key Geometric Concepts of PCA\n",
        "1Ô∏è‚É£ PCA Finds New Coordinate Axes\n",
        "Imagine a cloud of data points in a high-dimensional space.\n",
        "\n",
        "PCA rotates the coordinate system to find new axes that better capture the spread of data.\n",
        "\n",
        "These new axes are the principal components (PCs).\n",
        "\n",
        "üîπ Example (2D to 1D Reduction)\n",
        "\n",
        "Given 2D data (e.g., Tempo & Energy of songs), PCA finds a new 1D axis that best represents the data.\n",
        "\n",
        "The first principal component (PC1) is along the direction of maximum variance.\n",
        "\n",
        "The second principal component (PC2) is perpendicular to PC1 and captures the remaining variance.\n",
        "\n",
        "üìå Think of PCA as finding a tilted coordinate system that better fits the data.\n",
        "\n",
        "2Ô∏è‚É£ PCA Finds the \"Best-Fit Line\" (Higher-Dimensional Analogy)\n",
        "In 2D, PCA finds the best-fit line that minimizes distance errors.\n",
        "\n",
        "In 3D, PCA finds the best-fit plane.\n",
        "\n",
        "In higher dimensions, PCA finds the best-fit hyperplane that captures most of the variance.\n",
        "\n",
        "üîπ Analogy:\n",
        "\n",
        "If you have a sheet of paper (2D plane) inside a 3D space, PCA finds the best way to flatten your data onto that sheet while keeping as much information as possible.\n",
        "\n",
        "3Ô∏è‚É£ Principal Components = Eigenvectors of the Covariance Matrix\n",
        "PCA finds eigenvectors of the covariance matrix, which represent the directions of maximum variance.\n",
        "\n",
        "Eigenvalues tell us how much variance is captured by each eigenvector.\n",
        "\n",
        "üîπ Think of Eigenvectors as \"axes\" and Eigenvalues as \"importance\" of those axes.\n",
        "\n",
        "PC1 (First Principal Component) ‚Üí Points in the direction of most variance.\n",
        "\n",
        "PC2 (Second Principal Component) ‚Üí Perpendicular to PC1, capturing less variance.\n",
        "\n",
        "Higher PCs capture even less variance and can be discarded for dimensionality reduction.\n",
        "\n",
        "üìå In essence, PCA \"reorients\" the data along the directions where it varies the most.\n",
        "\n",
        "üéµ Example: PCA on a Spotify Dataset\n",
        "Original Data (High Dimensional)\n",
        "Imagine plotting songs based on Tempo, Energy, and Loudness (3D space).\n",
        "\n",
        "Some songs cluster along a specific direction.\n",
        "\n",
        "PCA finds a new 2D plane (PC1 & PC2) that captures most of the variation.\n",
        "\n",
        "Instead of using 3D data, we now describe songs in a 2D plane, losing minimal information.\n",
        "\n",
        "After PCA (Reduced Dimensions)\n",
        "Song\tPC1 (Main Trend)\tPC2 (Secondary Trend)\n",
        "A\t1.45\t-0.30\n",
        "B\t0.92\t0.12\n",
        "\n",
        "üìå Summary of Geometric Intuition\n",
        "Concept\tGeometric Meaning\n",
        "Principal Components\tNew rotated axes capturing maximum variance\n",
        "Dimensionality Reduction\tFinding the best-fit lower-dimensional subspace\n",
        "Eigenvectors\tDirections (axes) of maximum variance\n",
        "Eigenvalues\tAmount of variance captured by each principal component\n",
        "Flattening Data\tReducing dimensionality while preserving patterns\n",
        "\n",
        "\n",
        "13. What is the difference between Feature Selection and Feature Extraction?\n",
        "\n",
        "\n",
        "Feature Selection vs. Feature Extraction\n",
        "Both Feature Selection and Feature Extraction aim to reduce dimensionality, but they do so in different ways.\n",
        "\n",
        "1Ô∏è‚É£ Feature Selection (Choosing the Best Features)\n",
        "Feature selection involves keeping a subset of the original features while discarding irrelevant or redundant ones.\n",
        "üëâ No transformation is applied‚Äîwe simply select the most useful features.\n",
        "\n",
        "üîπ Methods of Feature Selection:\n",
        "‚úÖ Filter Methods (Statistical tests) ‚Üí Use correlation, chi-square, or mutual information to rank and select features.\n",
        "‚úÖ Wrapper Methods (Model-based) ‚Üí Train models with different feature subsets and select the best-performing set (e.g., Recursive Feature Elimination).\n",
        "‚úÖ Embedded Methods (Built-in model selection) ‚Üí Models like Lasso Regression automatically remove irrelevant features.\n",
        "\n",
        "üìå Example in Your Spotify Dataset\n",
        "\n",
        "Suppose you have 30 audio features (e.g., Tempo, Danceability, Loudness, Popularity).\n",
        "\n",
        "Using feature selection, you might find that only 10 are relevant for predicting song popularity and drop the rest.\n",
        "\n",
        "2Ô∏è‚É£ Feature Extraction (Creating New Features)\n",
        "Feature extraction transforms the original features into a new set of features that better represent the data.\n",
        "üëâ Instead of selecting from existing features, we create new features that capture the most important information.\n",
        "\n",
        "üîπ Methods of Feature Extraction:\n",
        "‚úÖ Principal Component Analysis (PCA) ‚Üí Creates new components that combine multiple features while preserving variance.\n",
        "‚úÖ t-SNE, UMAP ‚Üí Non-linear techniques for reducing high-dimensional data into 2D/3D.\n",
        "‚úÖ Autoencoders (Deep Learning) ‚Üí Learn compressed feature representations automatically.\n",
        "\n",
        "üìå Example in Your Spotify Dataset\n",
        "\n",
        "Instead of selecting features like Tempo and Danceability, PCA might create a new feature PC1 that represents a mix of both.\n",
        "\n",
        "This new feature captures most of the variance, reducing redundancy.\n",
        "\n",
        "üìå Key Differences:\n",
        "Aspect\tFeature Selection\tFeature Extraction\n",
        "Definition\tSelects important features from the original dataset\tCreates new features from existing ones\n",
        "Approach\tKeeps original features, removes irrelevant ones\tTransforms data into a new feature space\n",
        "Examples\tFilter Methods, Wrapper Methods, Embedded Methods\tPCA, t-SNE, Autoencoders\n",
        "Information Loss?\tMinimal if done well\tPossible, but aims to retain key variance\n",
        "Used When\tSome features are irrelevant or redundant\tFeatures are correlated, and we need a lower-dimensional representation\n",
        "üí° When to Use Each?\n",
        "‚úÖ Use Feature Selection if you have many features but only a few are useful (e.g., some audio features may be irrelevant for popularity prediction).\n",
        "‚úÖ Use Feature Extraction if features are highly correlated or redundant (e.g., PCA can reduce 30 correlated audio features into 5 meaningful ones).\n",
        "\n",
        "14. What are Eigenvalues and Eigenvectors in PCA?\n",
        "\n",
        "\n",
        "Eigenvalues and Eigenvectors in PCA\n",
        "Eigenvalues and eigenvectors are fundamental to Principal Component Analysis (PCA) because they help find the principal components‚Äîthe new feature axes that capture the most variance in the data.\n",
        "\n",
        "üìå What Are Eigenvalues and Eigenvectors?\n",
        "1Ô∏è‚É£ Eigenvectors (Direction of Data Spread)\n",
        "Eigenvectors define the new axes (principal components) where the data varies the most.\n",
        "\n",
        "They are unit vectors (direction-only, no magnitude).\n",
        "\n",
        "Each eigenvector is a linear combination of the original features.\n",
        "\n",
        "2Ô∏è‚É£ Eigenvalues (Importance of Each Eigenvector)\n",
        "Eigenvalues tell us how much variance (or information) each eigenvector captures.\n",
        "\n",
        "Larger eigenvalues = More important principal component.\n",
        "\n",
        "The sum of all eigenvalues gives the total variance in the data.\n",
        "\n",
        "üìå Think of eigenvectors as \"directions\" and eigenvalues as \"importance\" of those directions.\n",
        "\n",
        "üìå How Do Eigenvalues & Eigenvectors Work in PCA?\n",
        "PCA transforms the data by:\n",
        "\n",
        "Computing the Covariance Matrix (\n",
        "Œ£\n",
        "Œ£) ‚Üí Measures relationships between features.\n",
        "\n",
        "Solving for Eigenvalues & Eigenvectors ‚Üí Using the equation:\n",
        "\n",
        "Œ£\n",
        "ùë£\n",
        "=\n",
        "ùúÜ\n",
        "ùë£\n",
        "Œ£v=Œªv\n",
        "where:\n",
        "\n",
        "ùë£\n",
        "v = Eigenvector (new axis direction)\n",
        "\n",
        "ùúÜ\n",
        "Œª = Eigenvalue (variance along that direction)\n",
        "\n",
        "Sorting Eigenvalues in Descending Order ‚Üí The eigenvector with the largest eigenvalue becomes the first principal component (PC1).\n",
        "\n",
        "Selecting the Top K Principal Components ‚Üí Keep the eigenvectors that capture most of the variance.\n",
        "\n",
        "üéµ Example: PCA on a Spotify Dataset\n",
        "Original Features (Before PCA)\n",
        "Song\tDanceability\tTempo\tEnergy\n",
        "A\t0.8\t120\t0.9\n",
        "B\t0.6\t100\t0.7\n",
        "C\t0.9\t130\t1.0\n",
        "Eigenvectors & Eigenvalues from PCA\n",
        "Principal Component\tEigenvector (Direction)\tEigenvalue (Variance Captured)\n",
        "PC1 (Main Trend)\t(0.5, 0.7, 0.5)\t3.2 (Most Important)\n",
        "PC2 (Secondary Trend)\t(-0.6, 0.2, 0.8)\t1.1\n",
        "PC3 (Least Important)\t(0.7, -0.7, 0.2)\t0.3\n",
        "üëâ PC1 captures the most variance, so we might reduce the dataset to just PC1 & PC2, removing PC3.\n",
        "\n",
        "üìå Summary of Eigenvalues & Eigenvectors in PCA\n",
        "Concept\tMeaning in PCA\n",
        "Eigenvectors\tNew axes (principal components) along which data is projected\n",
        "Eigenvalues\tAmount of variance captured by each eigenvector\n",
        "Larger Eigenvalue\tMore important principal component (captures more variance)\n",
        "Sorting Eigenvalues\tHelps select the most informative components\n",
        "Dimensionality Reduction\tKeep top K principal components (largest eigenvalues)\n",
        "\n",
        "\n",
        "15. How do you decide the number of components to keep in PCA?\n",
        "\n",
        "\n",
        "How to Decide the Number of Components to Keep in PCA?\n",
        "When using PCA, we need to decide how many principal components (PCs) to keep while retaining most of the important information. The goal is to reduce dimensionality while minimizing information loss.\n",
        "\n",
        "üìå Methods for Choosing the Optimal Number of Components\n",
        "1Ô∏è‚É£ Explained Variance (Cumulative Variance) ‚Äì The \"95% Rule\"\n",
        "Each principal component captures a certain amount of variance (information).\n",
        "\n",
        "We compute the cumulative variance and keep the top K components that capture at least 95% of the total variance.\n",
        "\n",
        "‚úÖ Steps:\n",
        "\n",
        "Compute the variance explained by each principal component:\n",
        "\n",
        "ùëÉ\n",
        "ùëâ\n",
        "ùê∏\n",
        "=\n",
        "ùúÜ\n",
        "ùëñ\n",
        "‚àë\n",
        "ùúÜ\n",
        "PVE=\n",
        "‚àëŒª\n",
        "Œª\n",
        "i\n",
        "‚Äã\n",
        "\n",
        "‚Äã\n",
        "\n",
        "Sum the top K principal components until reaching ‚â• 95% variance.\n",
        "\n",
        "üìå Example (Variance Explained by Each PC):\n",
        "\n",
        "PC\tEigenvalue\t% Variance Explained\tCumulative Variance\n",
        "PC1\t4.2\t55%\t55%\n",
        "PC2\t2.3\t30%\t85%\n",
        "PC3\t0.9\t10%\t95% ‚úÖ\n",
        "PC4\t0.6\t5%\t100%\n",
        "üëâ Here, PC1 + PC2 + PC3 explain 95% of the variance, so we keep 3 components and drop PC4.\n",
        "\n",
        "2Ô∏è‚É£ Scree Plot (Elbow Method)\n",
        "A Scree Plot shows the variance explained by each principal component.\n",
        "\n",
        "Look for the \"elbow point\", where the variance gain starts flattening.\n",
        "\n",
        "‚úÖ Steps:\n",
        "\n",
        "Plot PC number (X-axis) vs. Explained Variance (%) (Y-axis).\n",
        "\n",
        "Identify the elbow point, where adding more PCs gives diminishing returns.\n",
        "\n",
        "üìå Example Scree Plot:\n",
        "üìâ Sharp drop ‚Üí Elbow ‚Üí Small gains after that\n",
        "\n",
        "lua\n",
        "Copy\n",
        "Edit\n",
        "Variance (%)\n",
        "  |\n",
        "  |  *\n",
        "  |  *  *\n",
        "  |  *  *  *\n",
        "  |  *  *  *  *\n",
        "  |-----------------\n",
        "     PC1 PC2 PC3 PC4 ...\n",
        "üëâ The elbow is at PC3, so we keep 3 components.\n",
        "\n",
        "3Ô∏è‚É£ Cross-Validation (Performance-Based)\n",
        "If PCA is used before a machine learning model (e.g., KNN, regression), test different values of K (number of components) using cross-validation to find the best balance between accuracy and efficiency.\n",
        "\n",
        "‚úÖ Steps:\n",
        "\n",
        "Train a model using different K values.\n",
        "\n",
        "Compare performance (accuracy, RMSE, etc.).\n",
        "\n",
        "Choose the smallest K that gives the best performance.\n",
        "\n",
        "üéµ Example: Choosing K for Your Spotify Dataset\n",
        "Imagine we apply PCA to 30 audio features (Danceability, Tempo, Energy, etc.).\n",
        "\n",
        "The Explained Variance shows 95% variance at K = 5 PCs.\n",
        "\n",
        "The Scree Plot has an elbow at PC5.\n",
        "\n",
        "A KNN model performs best when using 5 principal components.\n",
        "\n",
        "üìå Final Decision: Keep 5 PCs instead of 30 original features! üöÄ\n",
        "\n",
        "üìå Summary: How to Choose the Number of Components?\n",
        "Method\tWhat It Does\tWhen to Use\n",
        "Explained Variance (95% Rule)\tKeep components that explain 95% of variance\tQuick, standard approach\n",
        "Scree Plot (Elbow Method)\tLook for the point where variance gain flattens\tVisual, intuitive\n",
        "Cross-Validation\tTest performance of different K values\tIf using PCA before ML models\n",
        "\n",
        "16. Can PCA be used for classification?\n",
        "\n",
        "\n",
        "Can PCA Be Used for Classification?\n",
        "Yes, PCA can be used in a classification pipeline, but not directly as a classifier. Instead, PCA helps by reducing dimensionality and improving model efficiency before applying classification algorithms.\n",
        "\n",
        "üìå How PCA Helps in Classification?\n",
        "1Ô∏è‚É£ Reduces Dimensionality ‚Üí Helps classifiers perform better by removing redundant features.\n",
        "2Ô∏è‚É£ Removes Noise & Correlation ‚Üí Eliminates irrelevant variations in data.\n",
        "3Ô∏è‚É£ Speeds Up Computation ‚Üí Reducing features makes training classifiers faster.\n",
        "4Ô∏è‚É£ Improves Visualization ‚Üí PCA can project high-dimensional data into 2D or 3D for better insights.\n",
        "\n",
        "üéµ Example: PCA + Classification in Your Spotify Dataset\n",
        "Imagine you want to classify songs as ‚ÄúHit‚Äù or ‚ÄúFlop‚Äù based on audio features (e.g., Tempo, Energy, Loudness).\n",
        "\n",
        "‚úÖ Without PCA ‚Üí 30 raw features ‚Üí High complexity, risk of overfitting\n",
        "‚úÖ With PCA ‚Üí Reduce to 5 PCs ‚Üí Faster and better generalization\n",
        "\n",
        "üìå Steps:\n",
        "\n",
        "Apply PCA to reduce 30 features ‚Üí Keep top K components.\n",
        "\n",
        "Train a classifier (e.g., KNN, SVM, Random Forest) using the reduced dataset.\n",
        "\n",
        "Evaluate model performance ‚Üí Compare accuracy before and after PCA.\n",
        "\n",
        "üìå When Should You Use PCA in Classification?\n",
        "‚úÖ Use PCA if:\n",
        "\n",
        "The dataset has many correlated features.\n",
        "\n",
        "You need to speed up model training.\n",
        "\n",
        "You want to visualize high-dimensional data (e.g., PCA to 2D before classification).\n",
        "\n",
        "‚ùå Avoid PCA if:\n",
        "\n",
        "Features are already optimized and PCA removes useful information.\n",
        "\n",
        "You need interpretability (PCA transforms features, making them hard to interpret).\n",
        "\n",
        "üìå Summary: PCA for Classification\n",
        "Aspect\tDetails\n",
        "Direct Classifier?\t‚ùå No, PCA is not a classification algorithm\n",
        "Used for?\t‚úÖ Feature reduction before classification\n",
        "Helps With?\t‚úÖ Speed, accuracy, visualization\n",
        "Best For?\t‚úÖ High-dimensional, correlated datasets\n",
        "Common Classifiers\t‚úÖ KNN, SVM, Logistic Regression, Random Forest\n",
        "\n",
        "17. What are the limitations of PCA?\n",
        "\n",
        "\n",
        "Limitations of PCA (Principal Component Analysis)\n",
        "While PCA is a powerful technique for dimensionality reduction, it has several limitations that can affect its effectiveness.\n",
        "\n",
        "üìå 1Ô∏è‚É£ PCA Assumes Linearity\n",
        "PCA assumes that the data has linear relationships between features.\n",
        "\n",
        "If the structure in the data is nonlinear, PCA may not be effective.\n",
        "\n",
        "‚ùå Example: If audio features in your Spotify dataset have complex, nonlinear interactions, PCA might not capture them well.\n",
        "\n",
        "‚úÖ Alternative: Use t-SNE, UMAP, or Autoencoders for nonlinear data.\n",
        "\n",
        "üìå 2Ô∏è‚É£ Loss of Interpretability\n",
        "PCA transforms original features into new principal components, making them harder to interpret.\n",
        "\n",
        "‚ùå Example: Instead of \"Danceability\" and \"Tempo,\" you get PC1, PC2, etc., which don‚Äôt have direct meanings.\n",
        "\n",
        "‚úÖ Solution: Analyze feature contributions (e.g., loadings) to understand what each PC represents.\n",
        "\n",
        "üìå 3Ô∏è‚É£ Sensitive to Feature Scaling\n",
        "PCA is affected by differences in scale‚Äîfeatures with larger magnitudes dominate the principal components.\n",
        "\n",
        "‚ùå Example: \"Tempo (100-200 BPM)\" may dominate \"Danceability (0-1)\" unless scaled.\n",
        "\n",
        "‚úÖ Solution: Always apply feature scaling (StandardScaler or MinMaxScaler) before PCA.\n",
        "\n",
        "üìå 4Ô∏è‚É£ Can Remove Important Features\n",
        "PCA removes less important dimensions, but sometimes these \"low variance\" features contain useful classification information.\n",
        "\n",
        "‚ùå Example: A rare but important feature (e.g., a special beat pattern in your Spotify dataset) might be discarded.\n",
        "\n",
        "‚úÖ Solution: Compare PCA performance with and without using classification accuracy as a metric.\n",
        "\n",
        "üìå 5Ô∏è‚É£ Assumes Gaussian Distribution\n",
        "PCA works best when data is normally distributed.\n",
        "\n",
        "‚ùå Example: If your Spotify dataset has skewed distributions (e.g., Loudness or Popularity is highly skewed), PCA might not work well.\n",
        "\n",
        "‚úÖ Solution: Try log transformations or other dimensionality reduction techniques like ICA (Independent Component Analysis).\n",
        "\n",
        "üìå 6Ô∏è‚É£ Computationally Expensive for Large Datasets\n",
        "PCA requires computing the covariance matrix and eigenvalues, which can be slow for high-dimensional data.\n",
        "\n",
        "‚ùå Example: If your dataset has thousands of songs with 50+ audio features, PCA might be computationally expensive.\n",
        "\n",
        "‚úÖ Solution: Use Incremental PCA or Randomized PCA for large datasets.\n",
        "\n",
        "üìå Summary: When to Be Cautious with PCA?\n",
        "Limitation\tWhy It‚Äôs a Problem\tPossible Solution\n",
        "Assumes Linearity\tCan't capture complex patterns\tTry t-SNE, UMAP, Autoencoders\n",
        "Loss of Interpretability\tPCs don‚Äôt have direct meaning\tCheck feature contributions (loadings)\n",
        "Sensitive to Scaling\tLarge-magnitude features dominate\tStandardize data (StandardScaler)\n",
        "Removes Important Features\tLow-variance but useful features get dropped\tCompare performance with & without PCA\n",
        "Assumes Gaussian Data\tWorks best on normally distributed data\tUse log transforms or ICA\n",
        "Computational Cost\tSlow on high-dimensional datasets\tUse Incremental/Randomized PCA\n",
        "\n",
        "18. How do KNN and PCA complement each other?\n",
        "\n",
        "\n",
        "How Do KNN and PCA Complement Each Other?\n",
        "K-Nearest Neighbors (KNN) and Principal Component Analysis (PCA) are often used together in machine learning pipelines to improve classification performance, especially when dealing with high-dimensional data.\n",
        "\n",
        "üìå How PCA Helps KNN?\n",
        "‚úÖ 1Ô∏è‚É£ Reduces Dimensionality ‚Üí Faster KNN Computation\n",
        "\n",
        "KNN is computationally expensive because it calculates distances between all points.\n",
        "\n",
        "PCA reduces the number of features, making distance calculations faster.\n",
        "\n",
        "Example: If your Spotify dataset has 30 audio features, PCA can reduce it to 5 PCs, speeding up KNN.\n",
        "\n",
        "‚úÖ 2Ô∏è‚É£ Removes Noise & Correlation ‚Üí Improves KNN Accuracy\n",
        "\n",
        "PCA removes redundant & correlated features, helping KNN focus on meaningful variations.\n",
        "\n",
        "Example: \"Tempo\" and \"Beat Strength\" might be highly correlated‚ÄîPCA merges them into a single PC.\n",
        "\n",
        "‚úÖ 3Ô∏è‚É£ Avoids Curse of Dimensionality ‚Üí Better Distance Metrics\n",
        "\n",
        "KNN performs poorly when dimensions are too high because distances become meaningless.\n",
        "\n",
        "PCA reduces dimensions, making Euclidean distance (used in KNN) more reliable.\n",
        "\n",
        "‚úÖ 4Ô∏è‚É£ Enables Data Visualization ‚Üí Better Model Understanding\n",
        "\n",
        "PCA allows 2D/3D visualization of high-dimensional data, making it easier to analyze clusters before applying KNN.\n",
        "\n",
        "üìå How KNN Works After PCA?\n",
        "Apply PCA ‚Üí Reduce N features to K principal components.\n",
        "\n",
        "Use KNN on Transformed Data ‚Üí Classify data using the reduced feature set.\n",
        "\n",
        "Evaluate Performance ‚Üí Compare accuracy before & after PCA.\n",
        "\n",
        "üéµ Example: PCA + KNN on Spotify Data (Classifying ‚ÄúHit‚Äù vs. ‚ÄúFlop‚Äù Songs)\n",
        "Without PCA:\n",
        "\n",
        "30 audio features ‚Üí Slow KNN training, risk of overfitting.\n",
        "\n",
        "KNN might struggle due to irrelevant/correlated features.\n",
        "\n",
        "With PCA:\n",
        "\n",
        "Reduce to 5 principal components (95% variance retained).\n",
        "\n",
        "Faster KNN and better classification accuracy.\n",
        "\n",
        "üìå When to Use PCA Before KNN?\n",
        "Scenario\tShould You Use PCA?\n",
        "High-dimensional data (D > 10)\t‚úÖ Yes, PCA reduces dimensions for better KNN\n",
        "Highly correlated features\t‚úÖ Yes, PCA removes redundancy\n",
        "Small dataset with few features\t‚ùå No, PCA may remove useful information\n",
        "KNN performing poorly due to curse of dimensionality\t‚úÖ Yes, PCA can improve distance calculations\n",
        "\n",
        "19. How does KNN handle missing values in a dataset?\n",
        "\n",
        "\n",
        "How Does KNN Handle Missing Values in a Dataset?\n",
        "K-Nearest Neighbors (KNN) does not inherently handle missing values, but there are effective ways to deal with them before or during KNN classification/regression.\n",
        "\n",
        "üìå 1Ô∏è‚É£ Common Methods to Handle Missing Values in KNN\n",
        "‚úÖ 1. Remove Rows with Missing Values\n",
        "\n",
        "If only a few rows have missing data, you can drop them.\n",
        "\n",
        "‚ùå Risk: If too many rows are removed, it may reduce dataset quality.\n",
        "\n",
        "Example: If 5 out of 500 songs in your Spotify dataset have missing values, you can remove them without major impact.\n",
        "\n",
        "‚úÖ 2. Impute Missing Values Using Mean/Median/Mode\n",
        "\n",
        "Replace missing values with the mean (for numerical data) or mode (for categorical data).\n",
        "\n",
        "Example: If \"Tempo\" is missing for a song, replace it with the average tempo of other songs.\n",
        "\n",
        "‚ùå Risk: If data is not normally distributed, mean imputation might not be accurate.\n",
        "\n",
        "‚úÖ 3. Use KNN Imputation (Best for Missing Data in KNN)\n",
        "\n",
        "Instead of using the mean, find K nearest neighbors and use their values to fill in missing data.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Find K most similar rows (excluding the missing feature).\n",
        "\n",
        "Compute the average value of the missing feature from neighbors.\n",
        "\n",
        "Replace the missing value with this computed value.\n",
        "\n",
        "Example: If a song is missing its \"Energy\" value, KNN finds similar songs based on other features and imputes \"Energy\" accordingly.\n",
        "\n",
        "‚úÖ 4. Use a Machine Learning Model for Imputation\n",
        "\n",
        "Train a regression model to predict missing values using other features.\n",
        "\n",
        "Example: Use Linear Regression to predict missing \"Loudness\" values from correlated features like \"Energy\" and \"Danceability.\"\n",
        "\n",
        "üìå 2Ô∏è‚É£ Which Method Should You Use?\n",
        "Scenario\tBest Method\n",
        "Few missing values (<5%)\tDrop rows\n",
        "Continuous numerical data\tMean/Median imputation\n",
        "Categorical data\tMode imputation\n",
        "Large dataset, missing at random\tKNN Imputation ‚úÖ\n",
        "Highly structured relationships\tMachine Learning-based imputation\n",
        "üìå 3Ô∏è‚É£ Summary: Handling Missing Values in KNN\n",
        "Method\tPros\tCons\n",
        "Drop Rows\tSimple, quick\tData loss\n",
        "Mean/Median Imputation\tFast, works for numerical data\tIgnores relationships between features\n",
        "KNN Imputation\tCaptures patterns from similar data\tComputationally expensive\n",
        "ML-based Imputation\tMore accurate for complex data\tRequires extra model training\n",
        "\n",
        "\n",
        "20. What are the key differences between PCA and Linear Discriminant Analysis (LDA)?\n",
        "\n",
        "\n",
        "Key Differences Between PCA and LDA\n",
        "Both PCA (Principal Component Analysis) and LDA (Linear Discriminant Analysis) are dimensionality reduction techniques, but they serve different purposes.\n",
        "\n",
        "üìå 1Ô∏è‚É£ Purpose: Unsupervised vs. Supervised\n",
        "Method\tType\tMain Goal\n",
        "PCA\tUnsupervised\tMaximizes variance in the data\n",
        "LDA\tSupervised\tMaximizes class separability\n",
        "‚úÖ PCA: Reduces dimensions by capturing the directions with the most variance‚Äîignores class labels.\n",
        "‚úÖ LDA: Finds directions that best separate classes‚Äîuses class labels.\n",
        "\n",
        "üìå Example: If you want to reduce features in your Spotify dataset, PCA will preserve overall structure, while LDA will focus on distinguishing \"Hit\" vs. \"Flop\" songs.\n",
        "\n",
        "üìå 2Ô∏è‚É£ How They Work\n",
        "Method\tWhat It Finds?\tTransformation\n",
        "PCA\tEigenvectors of covariance matrix\tProjects data onto new axes with max variance\n",
        "LDA\tEigenvectors of class separation\tProjects data onto axes that maximize class separation\n",
        "‚úÖ PCA Steps:\n",
        "\n",
        "Compute the covariance matrix.\n",
        "\n",
        "Find eigenvectors (principal components).\n",
        "\n",
        "Project data onto the top K components.\n",
        "\n",
        "‚úÖ LDA Steps:\n",
        "\n",
        "Compute between-class and within-class scatter matrices.\n",
        "\n",
        "Solve for eigenvectors that maximize class separation.\n",
        "\n",
        "Project data onto LDA components (‚â§ number of classes - 1).\n",
        "\n",
        "üìå 3Ô∏è‚É£ Number of Components: PCA Can Be Larger Than LDA\n",
        "PCA: Can have as many components as the number of original features.\n",
        "\n",
        "LDA: Maximum C - 1 components (where C = number of classes).\n",
        "\n",
        "üìå Example:\n",
        "If your dataset has 10 features and 3 classes:\n",
        "\n",
        "PCA can have up to 10 components.\n",
        "\n",
        "LDA can have at most 2 components (C-1 = 3-1).\n",
        "\n",
        "üìå 4Ô∏è‚É£ When to Use PCA vs. LDA?\n",
        "Scenario\tUse PCA?\tUse LDA?\n",
        "Unlabeled data\t‚úÖ Yes\t‚ùå No (LDA needs labels)\n",
        "Dimensionality reduction for any purpose\t‚úÖ Yes\t‚ùå No\n",
        "Feature extraction for classification\t‚úÖ Yes\t‚úÖ Yes\n",
        "Maximizing class separation\t‚ùå No\t‚úÖ Yes\n",
        "When features are correlated\t‚úÖ Yes\t‚úÖ Yes\n",
        "üìå 5Ô∏è‚É£ Example: PCA vs. LDA in Spotify Dataset\n",
        "üîπ PCA: If you want to reduce 30 audio features to 5 principal components, PCA will find the most informative features based on variance.\n",
        "üîπ LDA: If you want to classify songs as \"Hit\" or \"Flop\", LDA will find the features that best separate these two classes.\n",
        "\n",
        "\n",
        "Practical\n",
        "\n",
        "21. Train a KNN Classifier on the Iris dataset and print model accuracy?\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features for better performance\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train the KNN Classifier with k=3\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Calculate and print model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "22. Train a KNN Regressor on a synthetic dataset and evaluate using Mean Squared Error (MSE)?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate a synthetic dataset\n",
        "np.random.seed(42)\n",
        "X = np.sort(5 * np.random.rand(100, 1), axis=0)  # Feature: Random values between 0 and 5\n",
        "y = np.sin(X).ravel() + np.random.normal(0, 0.2, X.shape[0])  # Target with noise\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train the KNN Regressor with k=5\n",
        "knn_reg = KNeighborsRegressor(n_neighbors=5)\n",
        "knn_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = knn_reg.predict(X_test)\n",
        "\n",
        "# Calculate and print Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "\n",
        "# Plot the results\n",
        "plt.scatter(X_test, y_test, color='blue', label=\"Actual\")\n",
        "plt.scatter(X_test, y_pred, color='red', label=\"Predicted\")\n",
        "plt.xlabel(\"Feature\")\n",
        "plt.ylabel(\"Target\")\n",
        "plt.legend()\n",
        "plt.title(\"KNN Regression Results\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "23. Train a KNN Classifier using different distance metrics (Euclidean and Manhattan) and compare accuracy?\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features for better performance\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train KNN Classifier with Euclidean distance (default)\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
        "knn_euclidean.fit(X_train, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# Train KNN Classifier with Manhattan distance\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=3, metric='manhattan')\n",
        "knn_manhattan.fit(X_train, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "# Print accuracy comparison\n",
        "print(f\"Accuracy with Euclidean Distance: {accuracy_euclidean:.4f}\")\n",
        "print(f\"Accuracy with Manhattan Distance: {accuracy_manhattan:.4f}\")\n",
        "\n",
        "\n",
        "24. Train a KNN Classifier with different values of K and visualize decision boundaried?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data[:, :2]  # Use only the first two features for 2D visualization\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Function to plot decision boundaries\n",
        "def plot_decision_boundary(knn, X, y, title):\n",
        "    h = 0.02  # Step size in the mesh\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "\n",
        "    # Predict class for each point in the meshgrid\n",
        "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Plot decision boundary\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', cmap=plt.cm.Paired)\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.title(title)\n",
        "\n",
        "# Train KNN with different values of K and plot decision boundaries\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, k in enumerate([1, 5, 10], 1):\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    plt.subplot(1, 3, i)\n",
        "    plot_decision_boundary(knn, X_train, y_train, f'KNN Decision Boundary (k={k})')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "25 .Apply Feature Scaling before training a KNN model and compare results with unscaled data?\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train KNN without feature scaling\n",
        "knn_unscaled = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = knn_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# Apply Standardization (Feature Scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train KNN with feature scaling\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy without Scaling: {accuracy_unscaled:.4f}\")\n",
        "print(f\"Accuracy with Scaling: {accuracy_scaled:.4f}\")\n",
        "\n",
        "26. Train a PCA model on synthetic data and print the explained variance ratio for each component?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate synthetic data (100 samples, 5 features)\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 5) * 10  # Random values between 0 and 10\n",
        "\n",
        "# Standardize the data (PCA works best with scaled data)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train PCA model (retain all components)\n",
        "pca = PCA(n_components=X.shape[1])\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Print explained variance ratio for each component\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "for i, var in enumerate(explained_variance, 1):\n",
        "    print(f\"Principal Component {i}: {var:.4f}\")\n",
        "\n",
        "# Plot explained variance\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.7, color='b', label='Explained Variance')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Variance Ratio')\n",
        "plt.title('Explained Variance Ratio of PCA Components')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "27. Apply PCA before training a KNN Classifier and compare accuracy with and without PCA?\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train KNN without PCA\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "y_pred = knn.predict(X_test_scaled)\n",
        "accuracy_without_pca = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Apply PCA (retain 95% of variance)\n",
        "pca = PCA(n_components=0.95)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# Train KNN with PCA-transformed data\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "accuracy_with_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "# Print comparison results\n",
        "print(f\"Accuracy without PCA: {accuracy_without_pca:.4f}\")\n",
        "print(f\"Accuracy with PCA: {accuracy_with_pca:.4f}\")\n",
        "\n",
        "# Print explained variance\n",
        "print(f\"Explained Variance Ratio: {pca.explained_variance_ratio_}\")\n",
        "print(f\"Number of Principal Components Retained: {pca.n_components_}\")\n",
        "\n",
        "28. Perform Hyperparameter Tuning on a KNN Classifier using GridSearchCV?\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define KNN model\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Define hyperparameters to tune\n",
        "param_grid = {\n",
        "    'n_neighbors': [1, 3, 5, 7, 9, 11],  # Different values of K\n",
        "    'metric': ['euclidean', 'manhattan']  # Distance metrics\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get best model\n",
        "best_knn = grid_search.best_estimator_\n",
        "\n",
        "# Predict using the best model\n",
        "y_pred_best = best_knn.predict(X_test_scaled)\n",
        "\n",
        "# Print results\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n",
        "print(f\"Test Set Accuracy with Best Model: {accuracy_score(y_test, y_pred_best):.4f}\")\n",
        "\n",
        "\n",
        "29. Train a KNN Classifier and check the number of misclassified samples?\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train KNN Classifier with k=3\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = knn.predict(X_test_scaled)\n",
        "\n",
        "# Calculate misclassified samples\n",
        "misclassified_samples = (y_test != y_pred).sum()\n",
        "\n",
        "# Print results\n",
        "print(f\"Total Test Samples: {len(y_test)}\")\n",
        "print(f\"Misclassified Samples: {misclassified_samples}\")\n",
        "print(f\"Test Set Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "\n",
        "\n",
        "30. Train a PCA model and visualize the cumulative explained variance.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Compute cumulative explained variance\n",
        "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# Plot cumulative explained variance\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--', color='b')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('PCA Cumulative Explained Variance')\n",
        "plt.axhline(y=0.95, color='r', linestyle='--', label='95% Variance')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "31. Train a KNN Classifier using different values of the weights parameter (uniform vs. distance) and compare  accuracy?\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train KNN with uniform weights (equal weight to all neighbors)\n",
        "knn_uniform = KNeighborsClassifier(n_neighbors=5, weights='uniform')\n",
        "knn_uniform.fit(X_train_scaled, y_train)\n",
        "y_pred_uniform = knn_uniform.predict(X_test_scaled)\n",
        "accuracy_uniform = accuracy_score(y_test, y_pred_uniform)\n",
        "\n",
        "# Train KNN with distance-based weights (closer neighbors contribute more)\n",
        "knn_distance = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
        "knn_distance.fit(X_train_scaled, y_train)\n",
        "y_pred_distance = knn_distance.predict(X_test_scaled)\n",
        "accuracy_distance = accuracy_score(y_test, y_pred_distance)\n",
        "\n",
        "# Print comparison results\n",
        "print(f\"Accuracy with 'uniform' weights: {accuracy_uniform:.4f}\")\n",
        "print(f\"Accuracy with 'distance' weights: {accuracy_distance:.4f}\")\n",
        "\n",
        "\n",
        "32. Train a KNN Regressor and analyze the effect of different K values on performance?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate synthetic dataset\n",
        "np.random.seed(42)\n",
        "X = np.linspace(0, 10, 100).reshape(-1, 1)  # Features\n",
        "y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])  # Target with noise\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Try different K values and evaluate performance\n",
        "k_values = [1, 3, 5, 7, 10, 15]\n",
        "mse_values = []\n",
        "\n",
        "for k in k_values:\n",
        "    knn_regressor = KNeighborsRegressor(n_neighbors=k)\n",
        "    knn_regressor.fit(X_train_scaled, y_train)\n",
        "    y_pred = knn_regressor.predict(X_test_scaled)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_values.append(mse)\n",
        "    print(f\"K={k}, MSE={mse:.4f}\")\n",
        "\n",
        "# Plot MSE vs K values\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(k_values, mse_values, marker='o', linestyle='--', color='b')\n",
        "plt.xlabel('Number of Neighbors (K)')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "plt.title('Effect of K on KNN Regression Performance')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "33. Implement KNN Imputation for handling missing values in a dataset?\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "# Create a synthetic dataset with missing values\n",
        "data = {\n",
        "    'Feature1': [5, 2, np.nan, 8, 4, 7, np.nan, 6, 3, 9],\n",
        "    'Feature2': [1, np.nan, 5, 7, np.nan, 6, 8, 3, 4, np.nan],\n",
        "    'Feature3': [np.nan, 3, 6, 2, 9, 5, 7, np.nan, 8, 4]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(\"Original Dataset with Missing Values:\")\n",
        "print(df)\n",
        "\n",
        "# Apply KNN Imputation (using k=3 neighbors)\n",
        "imputer = KNNImputer(n_neighbors=3)\n",
        "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
        "\n",
        "print(\"\\nDataset After KNN Imputation:\")\n",
        "print(df_imputed)\n",
        "\n",
        "34. Train a PCA model and visualize the data projection onto the first two principal components?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "target_names = iris.target_names\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA and keep the first 2 principal components\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Create a scatter plot of the first two principal components\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y, palette='viridis', style=y, legend=True)\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('PCA Projection of the Iris Dataset')\n",
        "plt.legend(labels=target_names)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "35. Train a KNN Classifier using the KD Tree and Ball Tree algorithms and compare performance?\n",
        "import time\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define KNN models using 'kd_tree' and 'ball_tree'\n",
        "algorithms = ['kd_tree', 'ball_tree']\n",
        "results = {}\n",
        "\n",
        "for algo in algorithms:\n",
        "    start_time = time.time()\n",
        "\n",
        "    knn = KNeighborsClassifier(n_neighbors=5, algorithm=algo)\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "    y_pred = knn.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    results[algo] = {'accuracy': accuracy, 'time': elapsed_time}\n",
        "\n",
        "# Print comparison results\n",
        "for algo, metrics in results.items():\n",
        "    print(f\"Algorithm: {algo}\")\n",
        "    print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
        "    print(f\"  Training Time: {metrics['time']:.6f} seconds\\n\")\n",
        "\n",
        "\n",
        "36. Train a PCA model on a high-dimensional dataset and visualize the Scree plot?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "# Load a high-dimensional dataset (Digits dataset with 64 features)\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Compute explained variance ratio\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# Plot the Scree plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--', color='b')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.title('Scree Plot of PCA on High-Dimensional Data')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "37. Train a KNN Classifier and evaluate performance using Precision, Recall, and F1-Score?\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train KNN Classifier with k=5\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = knn.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate using Precision, Recall, and F1-Score\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
        "\n",
        "\n",
        "38. Train a PCA model and analyze the effect of different numbers of components on accuracy?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Try different numbers of PCA components and evaluate accuracy\n",
        "component_range = range(1, X.shape[1] + 1)\n",
        "accuracy_scores = []\n",
        "\n",
        "for n_components in component_range:\n",
        "    # Apply PCA\n",
        "    pca = PCA(n_components=n_components)\n",
        "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "    X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "    # Train KNN classifier\n",
        "    knn = KNeighborsClassifier(n_neighbors=5)\n",
        "    knn.fit(X_train_pca, y_train)\n",
        "\n",
        "    # Evaluate accuracy\n",
        "    y_pred = knn.predict(X_test_pca)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_scores.append(accuracy)\n",
        "    print(f\"PCA Components: {n_components}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Plot accuracy vs. number of PCA components\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(component_range, accuracy_scores, marker='o', linestyle='--', color='b')\n",
        "plt.xlabel('Number of PCA Components')\n",
        "plt.ylabel('KNN Accuracy')\n",
        "plt.title('Effect of PCA Components on Classification Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "39. Train a KNN Classifier with different leaf_size values and compare accuracy?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Try different leaf_size values and evaluate accuracy\n",
        "leaf_sizes = [5, 10, 20, 30, 50]\n",
        "accuracy_scores = []\n",
        "\n",
        "for leaf_size in leaf_sizes:\n",
        "    knn = KNeighborsClassifier(n_neighbors=5, leaf_size=leaf_size)\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Predict and calculate accuracy\n",
        "    y_pred = knn.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_scores.append(accuracy)\n",
        "    print(f\"Leaf Size: {leaf_size}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Plot accuracy vs. leaf_size values\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(leaf_sizes, accuracy_scores, marker='o', linestyle='--', color='b')\n",
        "plt.xlabel('Leaf Size')\n",
        "plt.ylabel('KNN Accuracy')\n",
        "plt.title('Effect of Leaf Size on KNN Classification Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        " 40. Train a PCA model and visualize how data points are transformed before and after PCA?\n",
        " import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "target_names = iris.target_names\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA and reduce to 2 components\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Plot original data (first two features)\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, palette='viridis', style=y, legend=True)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('Original Data (First Two Features)')\n",
        "plt.legend(labels=target_names)\n",
        "\n",
        "# Plot transformed data (First two PCA components)\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y, palette='viridis', style=y, legend=True)\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('Data Transformed by PCA')\n",
        "plt.legend(labels=target_names)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        " 41. Train a KNN Classifier on a real-world dataset (Wine dataset) and print classification report?\n",
        " from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train a KNN Classifier with k=5\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = knn.predict(X_test_scaled)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=wine.target_names))\n",
        "\n",
        "\n",
        " 42. Train a KNN Regressor and analyze the effect of different distance metrics on prediction error?\n",
        " import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import fetch_california_housing  # Alternative for Boston dataset\n",
        "\n",
        "# Load the dataset (using California Housing as a substitute for Boston)\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split the dataset (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define distance metrics to test\n",
        "metrics = ['euclidean', 'manhattan']\n",
        "errors = {}\n",
        "\n",
        "for metric in metrics:\n",
        "    # Train KNN Regressor\n",
        "    knn = KNeighborsRegressor(n_neighbors=5, metric=metric)\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Predict and calculate Mean Squared Error (MSE)\n",
        "    y_pred = knn.predict(X_test_scaled)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    errors[metric] = mse\n",
        "    print(f\"Distance Metric: {metric}, MSE: {mse:.4f}\")\n",
        "\n",
        "# Plot the comparison\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(errors.keys(), errors.values(), color=['blue', 'green'])\n",
        "plt.xlabel('Distance Metric')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "plt.title('Effect of Distance Metrics on KNN Regression Error')\n",
        "plt.show()\n",
        "\n",
        "\n",
        " 43. Train a KNN Classifier and evaluate using ROC-AUC score?\n",
        " import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target  # Binary classification (0 = malignant, 1 = benign)\n",
        "\n",
        "# Split the dataset (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train a KNN Classifier (k=5)\n",
        "knn = KNeighborsClassifier(n_neighbors=5, probability=True)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get predicted probabilities for the positive class\n",
        "y_scores = knn.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Compute ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_scores)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Compute ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
        "\n",
        "# Plot ROC Curve\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'KNN (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # Random guess line\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve for KNN Classifier')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "  44. Train a PCA model and visualize the variance captured by each principal component?\n",
        "  import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA (keep all components)\n",
        "pca = PCA(n_components=X.shape[1])\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Get explained variance ratio\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "cumulative_variance = np.cumsum(explained_variance)\n",
        "\n",
        "# Plot Explained Variance Ratio\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.7, label='Explained Variance')\n",
        "plt.plot(range(1, len(explained_variance) + 1), cumulative_variance, marker='o', linestyle='--', color='r', label='Cumulative Variance')\n",
        "\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Variance Explained')\n",
        "plt.title('Explained Variance by Principal Components')\n",
        "plt.xticks(range(1, len(explained_variance) + 1))\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        " 45. Train a KNN Classifier and perform feature selection before training?\n",
        " import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split dataset (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Perform Feature Selection (Select Top 10 Features)\n",
        "selector = SelectKBest(score_func=f_classif, k=10)\n",
        "X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
        "X_test_selected = selector.transform(X_test_scaled)\n",
        "\n",
        "# Get selected feature names\n",
        "selected_features = feature_names[selector.get_support()]\n",
        "print(f\"Selected Features: {selected_features}\")\n",
        "\n",
        "# Train KNN Classifier (Before Feature Selection)\n",
        "knn_all = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_all.fit(X_train_scaled, y_train)\n",
        "y_pred_all = knn_all.predict(X_test_scaled)\n",
        "accuracy_all = accuracy_score(y_test, y_pred_all)\n",
        "\n",
        "# Train KNN Classifier (After Feature Selection)\n",
        "knn_selected = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_selected.fit(X_train_selected, y_train)\n",
        "y_pred_selected = knn_selected.predict(X_test_selected)\n",
        "accuracy_selected = accuracy_score(y_test, y_pred_selected)\n",
        "\n",
        "print(f\"Accuracy Before Feature Selection: {accuracy_all:.4f}\")\n",
        "print(f\"Accuracy After Feature Selection: {accuracy_selected:.4f}\")\n",
        "\n",
        "# Bar plot comparison\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(['All Features', 'Selected Features'], [accuracy_all, accuracy_selected], color=['blue', 'green'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('KNN Accuracy Before and After Feature Selection')\n",
        "plt.ylim(0.9, 1)  # Set y-axis range for better visualization\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        " 46. Train a PCA model and visualize the data reconstruction error after reducing dimensions?\n",
        " import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the Digits dataset\n",
        "digits = load_digits()\n",
        "X, y = digits.data, digits.target  # X: pixel data, y: digit labels\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA with 10 components\n",
        "n_components = 10\n",
        "pca = PCA(n_components=n_components)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Reconstruct the data from PCA components\n",
        "X_reconstructed = pca.inverse_transform(X_pca)\n",
        "\n",
        "# Compute Reconstruction Error (Mean Squared Error)\n",
        "reconstruction_error = mean_squared_error(X_scaled, X_reconstructed)\n",
        "print(f\"Reconstruction Error (MSE): {reconstruction_error:.4f}\")\n",
        "\n",
        "# Visualizing Original vs Reconstructed Images\n",
        "fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
        "\n",
        "for i in range(5):\n",
        "    # Original Image\n",
        "    axes[0, i].imshow(X[i].reshape(8, 8), cmap='gray')\n",
        "    axes[0, i].axis('off')\n",
        "    axes[0, i].set_title(\"Original\")\n",
        "\n",
        "    # Reconstructed Image\n",
        "    axes[1, i].imshow(X_reconstructed[i].reshape(8, 8), cmap='gray')\n",
        "    axes[1, i].axis('off')\n",
        "    axes[1, i].set_title(\"Reconstructed\")\n",
        "\n",
        "plt.suptitle(f\"PCA Reconstruction with {n_components} Components\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        " 47. Train a KNN Classifier and visualize the decision boundary?\n",
        " import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data[:, :2]  # Select first two features for 2D visualization\n",
        "\n",
        "# Split dataset (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train KNN Classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Create a mesh grid for plotting decision boundary\n",
        "x_min, x_max = X_train_scaled[:, 0].min() - 1, X_train_scaled[:, 0].max() + 1\n",
        "y_min, y_max = X_train_scaled[:, 1].min() - 1, X_train_scaled[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
        "\n",
        "# Predict labels for each point in the mesh grid\n",
        "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plot the decision boundary\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.Paired)\n",
        "plt.scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], c=y_train, edgecolor='k', cmap=plt.cm.Paired, label=\"Train\")\n",
        "plt.scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], c=y_test, edgecolor='k', cmap=plt.cm.Paired, marker='x', label=\"Test\")\n",
        "plt.xlabel('Feature 1 (Standardized)')\n",
        "plt.ylabel('Feature 2 (Standardized)')\n",
        "plt.title('KNN Decision Boundary (k=5)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        " 48. Train a PCA model and analyze the effect of different numbers of components on data variance.\n",
        " import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA (keep all components)\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Compute explained variance ratio\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "cumulative_variance = np.cumsum(explained_variance)\n",
        "\n",
        "# Plot Explained Variance vs. Number of Components\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, len(explained_variance) + 1), cumulative_variance, marker='o', linestyle='--', color='b')\n",
        "plt.axhline(y=0.95, color='r', linestyle='--', label='95% Variance Threshold')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('Effect of PCA Components on Data Variance')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Print the number of components required for 95% variance\n",
        "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
        "print(f\"Number of components required to retain 95% variance: {n_components_95}\")\n"
      ]
    }
  ]
}