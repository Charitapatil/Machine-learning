{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D21F1qrckac6"
      },
      "outputs": [],
      "source": [
        "Theoretical\n",
        "\n",
        "\n",
        "\n",
        "1. What is Boosting in Machine Learning?\n",
        "\n",
        "\n",
        "Boosting is an ensemble learning technique in machine learning that combines multiple weak learners (usually decision trees) to create a strong predictive model. It works by sequentially training models, where each new model corrects the errors made by the previous ones.\n",
        "\n",
        "How Boosting Works\n",
        "Initialize: Start with a weak learner (often a small decision tree).\n",
        "Iterative Training: Each subsequent model focuses more on the samples that were misclassified by the previous models.\n",
        "Weight Adjustment: Misclassified instances get higher weights, so the next model pays more attention to them.\n",
        "Final Prediction: The models are combined, typically through weighted voting or summation, to make a final strong prediction.\n",
        "Popular Boosting Algorithms\n",
        "AdaBoost (Adaptive Boosting): Assigns higher weights to misclassified instances.\n",
        "Gradient Boosting (GBM): Uses gradient descent to minimize errors iteratively.\n",
        "XGBoost (Extreme Gradient Boosting): An optimized version of GBM with speed and performance improvements.\n",
        "LightGBM (Light Gradient Boosting Machine): A faster and memory-efficient version of GBM.\n",
        "CatBoost: Designed for categorical data handling efficiently.\n",
        "Key Advantages\n",
        "âœ” Improves accuracy significantly\n",
        "âœ” Reduces bias and variance\n",
        "âœ” Works well with structured/tabular data\n",
        "\n",
        "Key Disadvantages\n",
        "âœ˜ Computationally expensive\n",
        "âœ˜ Prone to overfitting if not tuned properly\n",
        "\n",
        "2. How does Boosting differ from Bagging?\n",
        "Boosting vs. Bagging: Key Differences\n",
        "Feature\tBoosting\tBagging\n",
        "Concept\tSequentially improves weak learners by focusing on misclassified samples\tIndependently trains multiple models in parallel and averages their results\n",
        "Model Dependency\tEach model is dependent on the previous one\tModels are trained independently\n",
        "Error Handling\tReduces bias by learning from mistakes iteratively\tReduces variance by averaging multiple predictions\n",
        "Weight Adjustment\tAssigns higher weights to misclassified samples\tAll samples are treated equally (random sampling with replacement)\n",
        "Common Algorithms\tAdaBoost, Gradient Boosting (GBM), XGBoost, LightGBM, CatBoost\tRandom Forest, Bagging Classifier, Extra Trees\n",
        "Overfitting Tendency\tMore prone to overfitting if not tuned properly\tLess prone to overfitting due to variance reduction\n",
        "Computation Cost\tSlower, as models are trained sequentially\tFaster, as models are trained in parallel\n",
        "Key Takeaways\n",
        "Boosting is better for reducing bias and improving accuracy but can overfit if not tuned properly.\n",
        "Bagging is better for reducing variance and improving model stability.\n",
        "Random Forest (a bagging technique) is great for high variance models, while Boosting is ideal when you need to improve weak models.\n",
        "\n",
        "3. What is the key idea behind AdaBoost?\n",
        "Key Idea Behind AdaBoost (Adaptive Boosting)\n",
        "AdaBoost is an ensemble learning algorithm that improves weak classifiers by focusing on misclassified samples. The key idea is to assign higher weights to incorrectly classified instances, ensuring the next model focuses more on them.\n",
        "\n",
        "How AdaBoost Works\n",
        "Initialize Weights:\n",
        "\n",
        "All training samples start with equal weights.\n",
        "Train a Weak Learner (e.g., Decision Stump):\n",
        "\n",
        "A simple classifier (weak learner) is trained on the weighted dataset.\n",
        "Compute Model Error:\n",
        "\n",
        "The error is calculated based on misclassified samples.\n",
        "Update Sample Weights:\n",
        "\n",
        "Misclassified points get higher weights, making them more important in the next iteration.\n",
        "Repeat Steps 2-4:\n",
        "\n",
        "Train new weak learners sequentially, each improving the previous ones.\n",
        "Final Prediction:\n",
        "\n",
        "All weak learners are combined using a weighted vote to make the final decision.\n",
        "Mathematical Intuition\n",
        "The weight update formula ensures harder-to-classify samples get more attention.\n",
        "The final model is a weighted sum of weak models.\n",
        "Advantages of AdaBoost\n",
        "âœ” Improves accuracy by reducing bias\n",
        "âœ” Works well with weak learners\n",
        "âœ” Less prone to overfitting than many boosting algorithms\n",
        "\n",
        "Disadvantages of AdaBoost\n",
        "âœ˜ Sensitive to noisy data and outliers\n",
        "âœ˜ Requires careful tuning of the number of weak learners\n",
        "\n",
        "\n",
        "4. Explain the working of AdaBoost with an example?\n",
        "Step-by-Step Explanation\n",
        "1. Initialize Weights\n",
        "Suppose we have a dataset with 4 points and two classes (ğŸ”´, ğŸ”µ):\n",
        "Sample\tFeature\tClass\tInitial Weight\n",
        "A\t2.5\tğŸ”´\t1/4 (0.25)\n",
        "B\t4.1\tğŸ”µ\t1/4 (0.25)\n",
        "C\t5.2\tğŸ”´\t1/4 (0.25)\n",
        "D\t6.8\tğŸ”µ\t1/4 (0.25)\n",
        "Initially, all points have equal weights (1/n).\n",
        "2. Train Weak Classifier (First Iteration)\n",
        "We train a weak classifier (e.g., a Decision Stump).\n",
        "Suppose it misclassifies C.\n",
        "Sample\tPrediction\tCorrect?\n",
        "A\tğŸ”´\tâœ…\n",
        "B\tğŸ”µ\tâœ…\n",
        "C\tğŸ”µ (wrong)\tâŒ\n",
        "D\tğŸ”µ\tâœ…\n",
        "Error = Sum of misclassified sample weights â†’ Error = 0.25\n",
        "Classifier weight (Î±) is computed as:\n",
        "ğ›¼\n",
        "=\n",
        "1\n",
        "2\n",
        "ln\n",
        "â¡\n",
        "(\n",
        "1\n",
        "âˆ’\n",
        "Error\n",
        "Error\n",
        ")\n",
        "=\n",
        "1\n",
        "2\n",
        "ln\n",
        "â¡\n",
        "(\n",
        "0.75\n",
        "0.25\n",
        ")\n",
        "=\n",
        "0.55\n",
        "Î±=\n",
        "2\n",
        "1\n",
        "â€‹\n",
        " ln(\n",
        "Error\n",
        "1âˆ’Error\n",
        "â€‹\n",
        " )=\n",
        "2\n",
        "1\n",
        "â€‹\n",
        " ln(\n",
        "0.25\n",
        "0.75\n",
        "â€‹\n",
        " )=0.55\n",
        "3. Update Weights\n",
        "Misclassified samples (C) get a higher weight in the next iteration.\n",
        "\n",
        "Updated weight formula:\n",
        "\n",
        "ğ‘¤\n",
        "new\n",
        "=\n",
        "ğ‘¤\n",
        "old\n",
        "Ã—\n",
        "ğ‘’\n",
        "Â±\n",
        "ğ›¼\n",
        "w\n",
        "new\n",
        "â€‹\n",
        " =w\n",
        "old\n",
        "â€‹\n",
        " Ã—e\n",
        "Â±Î±\n",
        "\n",
        "If correct, decrease weight\n",
        "If wrong, increase weight\n",
        "After updating weights:\n",
        "\n",
        "Sample\tNew Weight\n",
        "A\t0.22\n",
        "B\t0.22\n",
        "C\t0.34\n",
        "D\t0.22\n",
        "4. Train Second Weak Classifier\n",
        "New model focuses more on C (higher weight).\n",
        "A new weak classifier is trained and makes fewer errors.\n",
        "Process repeats for T iterations, combining weak learners.\n",
        "5. Final Prediction\n",
        "Combine all weak classifiers using a weighted vote.\n",
        "Strong classifier = Weighted sum of weak models.\n",
        "Key Takeaways\n",
        "âœ… AdaBoost assigns more weight to misclassified samples.\n",
        "âœ… Weak models work sequentially, improving each time.\n",
        "âœ… Final decision is based on a weighted majority vote.\n",
        "\n",
        "5. What is Gradient Boosting, and how is it different from AdaBoost?\n",
        "Gradient Boosting is a boosting algorithm that builds an ensemble of weak models (typically decision trees) by minimizing a loss function using gradient descent. Instead of adjusting sample weights like AdaBoost, it corrects the previous model's errors by adding new models that predict the residual errors.\n",
        "\n",
        "How Gradient Boosting Works\n",
        "Start with a weak model (e.g., a small decision tree).\n",
        "Calculate Residual Errors: Compute the difference between actual and predicted values (residuals).\n",
        "Train a New Model to predict these residual errors.\n",
        "Update Predictions: Add this new model to the ensemble, improving the overall accuracy.\n",
        "Repeat steps 2â€“4 for T iterations, each time reducing the error.\n",
        "Difference Between Gradient Boosting and AdaBoost\n",
        "Feature\tGradient Boosting\tAdaBoost\n",
        "Error Handling\tMinimizes residual errors using gradient descent\tUpdates weights of misclassified samples\n",
        "How Models Improve\tEach new model predicts residual errors\tEach new model focuses more on hard-to-classify samples\n",
        "Loss Function\tUses a differentiable loss function (e.g., MSE, log loss)\tUses exponential loss to adjust sample weights\n",
        "Weight Adjustment\tDoesn't reweight samples; adds weak learners to reduce errors\tReweights samples to focus on misclassified ones\n",
        "Performance\tUsually more accurate with proper tuning\tSimpler but may not perform as well on complex tasks\n",
        "Computational Cost\tSlower due to gradient calculations\tFaster, but less flexible\n",
        "Key Takeaways\n",
        "âœ” Gradient Boosting is better for regression and complex tasks.\n",
        "âœ” AdaBoost works well for classification with simple models.\n",
        "âœ” XGBoost, LightGBM, and CatBoost are optimized versions of Gradient Boosting.\n",
        "\n",
        "6. What is the loss function in Gradient Boosting?\n",
        "Loss Function in Gradient Boosting\n",
        "The loss function in Gradient Boosting is a key component that measures how far predictions are from the actual values. The algorithm minimizes this loss function using gradient descent, improving model performance iteratively.\n",
        "\n",
        "Common Loss Functions in Gradient Boosting\n",
        "Gradient Boosting is flexible and supports different loss functions for regression and classification tasks.\n",
        "\n",
        "1. Regression Loss Functions\n",
        "Mean Squared Error (MSE)\n",
        "\n",
        "ğ¿\n",
        "(\n",
        "ğ‘¦\n",
        ",\n",
        "ğ‘¦\n",
        "^\n",
        ")\n",
        "=\n",
        "1\n",
        "ğ‘›\n",
        "âˆ‘\n",
        "(\n",
        "ğ‘¦\n",
        "ğ‘–\n",
        "âˆ’\n",
        "ğ‘¦\n",
        "^\n",
        "ğ‘–\n",
        ")\n",
        "2\n",
        "L(y,\n",
        "y\n",
        "^\n",
        "â€‹\n",
        " )=\n",
        "n\n",
        "1\n",
        "â€‹\n",
        " âˆ‘(y\n",
        "i\n",
        "â€‹\n",
        " âˆ’\n",
        "y\n",
        "^\n",
        "â€‹\n",
        "\n",
        "i\n",
        "â€‹\n",
        " )\n",
        "2\n",
        "\n",
        "Penalizes larger errors more heavily.\n",
        "Suitable for continuous target values.\n",
        "Mean Absolute Error (MAE)\n",
        "\n",
        "ğ¿\n",
        "(\n",
        "ğ‘¦\n",
        ",\n",
        "ğ‘¦\n",
        "^\n",
        ")\n",
        "=\n",
        "1\n",
        "ğ‘›\n",
        "âˆ‘\n",
        "âˆ£\n",
        "ğ‘¦\n",
        "ğ‘–\n",
        "âˆ’\n",
        "ğ‘¦\n",
        "^\n",
        "ğ‘–\n",
        "âˆ£\n",
        "L(y,\n",
        "y\n",
        "^\n",
        "â€‹\n",
        " )=\n",
        "n\n",
        "1\n",
        "â€‹\n",
        " âˆ‘âˆ£y\n",
        "i\n",
        "â€‹\n",
        " âˆ’\n",
        "y\n",
        "^\n",
        "â€‹\n",
        "\n",
        "i\n",
        "â€‹\n",
        " âˆ£\n",
        "Less sensitive to outliers than MSE.\n",
        "Huber Loss (Robust to Outliers)\n",
        "\n",
        "ğ¿\n",
        "(\n",
        "ğ‘¦\n",
        ",\n",
        "ğ‘¦\n",
        "^\n",
        ")\n",
        "=\n",
        "âˆ‘\n",
        "ğ‘–\n",
        "{\n",
        "1\n",
        "2\n",
        "(\n",
        "ğ‘¦\n",
        "ğ‘–\n",
        "âˆ’\n",
        "ğ‘¦\n",
        "^\n",
        "ğ‘–\n",
        ")\n",
        "2\n",
        "forÂ smallÂ errors\n",
        "ğ›¿\n",
        "(\n",
        "âˆ£\n",
        "ğ‘¦\n",
        "ğ‘–\n",
        "âˆ’\n",
        "ğ‘¦\n",
        "^\n",
        "ğ‘–\n",
        "âˆ£\n",
        "âˆ’\n",
        "1\n",
        "2\n",
        "ğ›¿\n",
        ")\n",
        "forÂ largeÂ errors\n",
        "L(y,\n",
        "y\n",
        "^\n",
        "â€‹\n",
        " )=\n",
        "i\n",
        "âˆ‘\n",
        "â€‹\n",
        " {\n",
        "2\n",
        "1\n",
        "â€‹\n",
        " (y\n",
        "i\n",
        "â€‹\n",
        " âˆ’\n",
        "y\n",
        "^\n",
        "â€‹\n",
        "\n",
        "i\n",
        "â€‹\n",
        " )\n",
        "2\n",
        "\n",
        "Î´(âˆ£y\n",
        "i\n",
        "â€‹\n",
        " âˆ’\n",
        "y\n",
        "^\n",
        "â€‹\n",
        "\n",
        "i\n",
        "â€‹\n",
        " âˆ£âˆ’\n",
        "2\n",
        "1\n",
        "â€‹\n",
        " Î´)\n",
        "â€‹\n",
        "\n",
        "forÂ smallÂ errors\n",
        "forÂ largeÂ errors\n",
        "â€‹\n",
        "\n",
        "Switches between MSE and MAE dynamically.\n",
        "2. Classification Loss Functions\n",
        "Log Loss (Binary Classification)\n",
        "\n",
        "ğ¿\n",
        "(\n",
        "ğ‘¦\n",
        ",\n",
        "ğ‘¦\n",
        "^\n",
        ")\n",
        "=\n",
        "âˆ’\n",
        "âˆ‘\n",
        "ğ‘–\n",
        "ğ‘¦\n",
        "ğ‘–\n",
        "log\n",
        "â¡\n",
        "(\n",
        "ğ‘¦\n",
        "^\n",
        "ğ‘–\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "âˆ’\n",
        "ğ‘¦\n",
        "ğ‘–\n",
        ")\n",
        "log\n",
        "â¡\n",
        "(\n",
        "1\n",
        "âˆ’\n",
        "ğ‘¦\n",
        "^\n",
        "ğ‘–\n",
        ")\n",
        "L(y,\n",
        "y\n",
        "^\n",
        "â€‹\n",
        " )=âˆ’\n",
        "i\n",
        "âˆ‘\n",
        "â€‹\n",
        " y\n",
        "i\n",
        "â€‹\n",
        " log(\n",
        "y\n",
        "^\n",
        "â€‹\n",
        "\n",
        "i\n",
        "â€‹\n",
        " )+(1âˆ’y\n",
        "i\n",
        "â€‹\n",
        " )log(1âˆ’\n",
        "y\n",
        "^\n",
        "â€‹\n",
        "\n",
        "i\n",
        "â€‹\n",
        " )\n",
        "Measures how well probabilities match actual labels.\n",
        "Multinomial Log Loss (Multiclass Classification)\n",
        "\n",
        "ğ¿\n",
        "(\n",
        "ğ‘¦\n",
        ",\n",
        "ğ‘¦\n",
        "^\n",
        ")\n",
        "=\n",
        "âˆ’\n",
        "âˆ‘\n",
        "ğ‘–\n",
        "âˆ‘\n",
        "ğ‘˜\n",
        "ğ‘¦\n",
        "ğ‘–\n",
        ",\n",
        "ğ‘˜\n",
        "log\n",
        "â¡\n",
        "(\n",
        "ğ‘¦\n",
        "^\n",
        "ğ‘–\n",
        ",\n",
        "ğ‘˜\n",
        ")\n",
        "L(y,\n",
        "y\n",
        "^\n",
        "â€‹\n",
        " )=âˆ’\n",
        "i\n",
        "âˆ‘\n",
        "â€‹\n",
        "\n",
        "k\n",
        "âˆ‘\n",
        "â€‹\n",
        " y\n",
        "i,k\n",
        "â€‹\n",
        " log(\n",
        "y\n",
        "^\n",
        "â€‹\n",
        "\n",
        "i,k\n",
        "â€‹\n",
        " )\n",
        "Extends Log Loss to multiple classes.\n",
        "Exponential Loss (Used in AdaBoost)\n",
        "\n",
        "ğ¿\n",
        "(\n",
        "ğ‘¦\n",
        ",\n",
        "ğ‘¦\n",
        "^\n",
        ")\n",
        "=\n",
        "ğ‘’\n",
        "âˆ’\n",
        "ğ‘¦\n",
        "ğ‘¦\n",
        "^\n",
        "L(y,\n",
        "y\n",
        "^\n",
        "â€‹\n",
        " )=e\n",
        "âˆ’y\n",
        "y\n",
        "^\n",
        "â€‹\n",
        "\n",
        "\n",
        "Increases emphasis on misclassified points.\n",
        "How Gradient Boosting Uses the Loss Function\n",
        "Compute gradient (derivative of loss function).\n",
        "Fit a weak learner (tree) to the gradient.\n",
        "Update predictions to reduce loss.\n",
        "Repeat until convergence.\n",
        "Key Takeaways\n",
        "âœ” MSE for regression, Log Loss for classification.\n",
        "âœ” The choice of loss function affects model performance and robustness.\n",
        "âœ” Gradient Boosting learns by reducing the loss function step by step.\n",
        "\n",
        "7. How does XGBoost improve over traditional Gradient Boosting?\n",
        "XGBoost (Extreme Gradient Boosting) is an optimized version of traditional Gradient Boosting that enhances speed, performance, and efficiency. Hereâ€™s how it improves:\n",
        "\n",
        "1. Regularization to Prevent Overfitting\n",
        "âœ… L1 (Lasso) & L2 (Ridge) Regularization:\n",
        "\n",
        "Unlike traditional Gradient Boosting, XGBoost adds L1 (Lasso) and L2 (Ridge) regularization to the loss function:\n",
        "ğ¿\n",
        "=\n",
        "âˆ‘\n",
        "Loss\n",
        "(\n",
        "ğ‘¦\n",
        ",\n",
        "ğ‘¦\n",
        "^\n",
        ")\n",
        "+\n",
        "ğœ†\n",
        "âˆ£\n",
        "âˆ£\n",
        "ğ‘¤\n",
        "âˆ£\n",
        "âˆ£\n",
        "2\n",
        "+\n",
        "ğ›¼\n",
        "âˆ£\n",
        "ğ‘¤\n",
        "âˆ£\n",
        "L=âˆ‘Loss(y,\n",
        "y\n",
        "^\n",
        "â€‹\n",
        " )+Î»âˆ£âˆ£wâˆ£âˆ£\n",
        "2\n",
        " +Î±âˆ£wâˆ£\n",
        "L1 reduces the number of features (feature selection).\n",
        "L2 prevents large weight values, making models more generalizable.\n",
        "ğŸ“Œ Benefit: Reduces overfitting and improves generalization.\n",
        "\n",
        "2. Second-Order Approximation (Taylor Expansion)\n",
        "âœ… Uses Second-Order Derivatives for Optimization\n",
        "\n",
        "Traditional Gradient Boosting only uses gradients (first derivative) to minimize loss.\n",
        "XGBoost also uses second derivatives (Hessian matrix) for better optimization.\n",
        "ğŸ“Œ Benefit: Faster convergence and better accuracy.\n",
        "\n",
        "3. Column Subsampling (Feature Selection)\n",
        "âœ… Random Feature Sampling Like Random Forest\n",
        "\n",
        "Instead of using all features at every split, XGBoost selects a random subset of features per tree.\n",
        "ğŸ“Œ Benefit:\n",
        "\n",
        "Reduces correlation between trees.\n",
        "Prevents overfitting.\n",
        "Speeds up training.\n",
        "4. Handling Missing Values Automatically\n",
        "âœ… Finds Optimal Splits for Missing Data\n",
        "\n",
        "XGBoost can learn the best direction for missing values rather than imputing them.\n",
        "ğŸ“Œ Benefit: Handles datasets with missing values efficiently.\n",
        "\n",
        "5. Weighted Quantile Sketch Algorithm\n",
        "âœ… Better Handling of Large Datasets\n",
        "\n",
        "XGBoost uses an efficient split-finding algorithm that scales well to large datasets.\n",
        "Traditional Gradient Boosting struggles with large-scale data.\n",
        "ğŸ“Œ Benefit: XGBoost is 10x faster than traditional Gradient Boosting.\n",
        "\n",
        "6. Parallel and Distributed Computing\n",
        "âœ… Multithreading for Faster Training\n",
        "\n",
        "Traditional Gradient Boosting trains trees sequentially, making it slower.\n",
        "XGBoost parallelizes tree construction across multiple cores.\n",
        "Supports GPU acceleration and distributed training (Hadoop, Spark, etc.)\n",
        "ğŸ“Œ Benefit: Massively faster training (can handle millions of data points).\n",
        "\n",
        "7. Shrinkage and Learning Rate\n",
        "âœ… Reduces Overfitting with Step-by-Step Learning\n",
        "\n",
        "XGBoost scales the contribution of each tree using a learning rate (Î·).\n",
        "ğ¹\n",
        "ğ‘¡\n",
        "(\n",
        "ğ‘¥\n",
        ")\n",
        "=\n",
        "ğ¹\n",
        "ğ‘¡\n",
        "âˆ’\n",
        "1\n",
        "(\n",
        "ğ‘¥\n",
        ")\n",
        "+\n",
        "ğœ‚\n",
        "â‹…\n",
        "â„\n",
        "ğ‘¡\n",
        "(\n",
        "ğ‘¥\n",
        ")\n",
        "F\n",
        "t\n",
        "â€‹\n",
        " (x)=F\n",
        "tâˆ’1\n",
        "â€‹\n",
        " (x)+Î·â‹…h\n",
        "t\n",
        "â€‹\n",
        " (x)\n",
        "This prevents large updates that could cause overfitting.\n",
        "ğŸ“Œ Benefit: More stable training and better accuracy.\n",
        "\n",
        "8. Tree Pruning (\"Max Depth\" Instead of \"Depth-First\")\n",
        "âœ… Prevents Overfitting by Stopping Early\n",
        "\n",
        "Traditional Gradient Boosting grows trees until no improvement is found.\n",
        "XGBoost prunes trees in advance by setting a maximum depth.\n",
        "ğŸ“Œ Benefit: Faster training and prevents deep, overfit trees.\n",
        "\n",
        "XGBoost vs. Traditional Gradient Boosting: Summary\n",
        "Feature\tXGBoost\tTraditional GBM\n",
        "Regularization\tL1 & L2 regularization (prevents overfitting)\tNo built-in regularization\n",
        "Optimization\tUses second-order derivatives (faster convergence)\tUses only first-order gradients\n",
        "Feature Sampling\tRandom feature selection like Random Forest\tUses all features per tree\n",
        "Handling Missing Values\tAuto-learns best splits\tNeeds manual imputation\n",
        "Parallelization\tMultithreaded, supports GPUs, distributed\tSequential training (slower)\n",
        "Tree Pruning\tPre-pruning with max depth\tGrows deep trees (overfitting risk)\n",
        "Computational Efficiency\tMuch faster (10x-100x speedup)\tSlower, especially on large data\n",
        "Key Takeaways\n",
        "âœ” XGBoost is faster, more accurate, and better at preventing overfitting.\n",
        "âœ” It handles large datasets efficiently (parallelization, GPU support).\n",
        "âœ” Regularization and pruning make it more robust.\n",
        "âœ” Ideal for Kaggle competitions and large-scale ML problems.\n",
        "\n",
        "8. What is the difference between XGBoost and CatBoost?\n",
        "XGBoost vs. CatBoost: Key Differences\n",
        "Both XGBoost and CatBoost are powerful Gradient Boosting algorithms, but they differ in how they handle data, categorical features, speed, and accuracy.\n",
        "\n",
        "Feature\tXGBoost\tCatBoost\n",
        "Developed By\tTianqi Chen\tYandex\n",
        "Handling Categorical Data\tRequires one-hot encoding or label encoding\tUses native categorical encoding (no preprocessing needed)\n",
        "Training Speed\tFaster than traditional GBM, but slower than CatBoost on categorical data\tFaster on datasets with many categorical features\n",
        "Overfitting Prevention\tL1 & L2 regularization, column sampling\tBuilt-in ordered boosting prevents overfitting\n",
        "Parallel Processing\tCPU & GPU support, optimized for multi-threading\tOptimized GPU acceleration, efficient CPU usage\n",
        "Hyperparameter Tuning\tMore hyperparameters to tune\tRequires fewer hyperparameter adjustments\n",
        "Best Use Cases\tNumeric-heavy datasets, structured data, tabular data\tDatasets with many categorical variables (e.g., NLP, finance, e-commerce)\n",
        "Interpretability\tFeature importance, SHAP values supported\tFeature importance, visualization tools available\n",
        "\n",
        "\n",
        "9. What are some real-world applications of Boosting techniques?\n",
        "Real-World Applications of Boosting Techniques ğŸš€\n",
        "Boosting algorithms (like AdaBoost, Gradient Boosting, XGBoost, LightGBM, and CatBoost) are widely used across industries for high-performance machine learning tasks. Here are some key applications:\n",
        "\n",
        "1. Fraud Detection (Banking & Finance) ğŸ’³\n",
        "âœ… Boosting models help detect fraudulent transactions by identifying patterns in transaction data.\n",
        "âœ… Used by banks, credit card companies, and fintech firms (e.g., PayPal, Visa, MasterCard).\n",
        "ğŸ“Œ Example: XGBoost is used for credit card fraud detection by analyzing user behavior and transaction history.\n",
        "\n",
        "2. Customer Churn Prediction (Telecom & SaaS) ğŸ“‰\n",
        "âœ… Boosting models predict customer churn by analyzing usage patterns and service complaints.\n",
        "âœ… Used by telecom companies (AT&T, Verizon, T-Mobile) and SaaS platforms.\n",
        "ğŸ“Œ Example: Gradient Boosting helps companies target at-risk customers with retention offers.\n",
        "\n",
        "3. Recommendation Systems (E-Commerce & Streaming) ğŸ›ï¸ğŸµ\n",
        "âœ… Used for personalized recommendations on platforms like Amazon, Netflix, Spotify.\n",
        "âœ… Boosting helps predict user preferences based on past interactions.\n",
        "ğŸ“Œ Example: CatBoost is used to improve product recommendations on e-commerce platforms.\n",
        "\n",
        "4. Credit Scoring & Loan Default Prediction (Banking) ğŸ¦\n",
        "âœ… Boosting improves credit risk assessment for banks and lenders (JPMorgan, Wells Fargo, LendingClub).\n",
        "âœ… Used to predict loan repayment likelihood based on historical data.\n",
        "ğŸ“Œ Example: XGBoost is commonly used in credit scoring models to determine loan eligibility.\n",
        "\n",
        "5. Medical Diagnosis & Disease Prediction (Healthcare) ğŸ¥\n",
        "âœ… Used for cancer detection, heart disease prediction, and diabetes risk assessment.\n",
        "âœ… Boosting models analyze medical history, genetic data, and lab results.\n",
        "ğŸ“Œ Example: Gradient Boosting is used for early cancer detection using radiology images.\n",
        "\n",
        "6. Stock Market & Cryptocurrency Prediction ğŸ“ˆ\n",
        "âœ… Hedge funds and traders use Boosting for predicting stock prices and market trends.\n",
        "âœ… Models analyze historical data, sentiment analysis, and financial indicators.\n",
        "ğŸ“Œ Example: XGBoost helps in stock price forecasting for algorithmic trading strategies.\n",
        "\n",
        "7. NLP: Sentiment Analysis & Chatbots ğŸ“ğŸ¤–\n",
        "âœ… Boosting models power chatbots, virtual assistants, and sentiment analysis tools.\n",
        "âœ… Used by companies like Google, Microsoft, and OpenAI for text classification and spam detection.\n",
        "ğŸ“Œ Example: Boosting helps classify positive vs. negative customer reviews on platforms like Yelp.\n",
        "\n",
        "8. Autonomous Vehicles & Image Recognition ğŸš—ğŸ“·\n",
        "âœ… Boosting models improve object detection for self-driving cars and security cameras.\n",
        "âœ… Used in Tesla Autopilot, Waymo, and Amazon Go.\n",
        "ğŸ“Œ Example: AdaBoost is used for face recognition in security systems.\n",
        "\n",
        "9. Cybersecurity: Malware Detection & Intrusion Prevention ğŸ”\n",
        "âœ… Boosting algorithms detect cyberattacks, phishing, and malware in real time.\n",
        "âœ… Used by antivirus software (McAfee, Symantec) and cloud security firms (CrowdStrike, Palo Alto Networks).\n",
        "ğŸ“Œ Example: XGBoost is used for anomaly detection in network security.\n",
        "\n",
        "10. Insurance Claims & Risk Assessment ğŸ ğŸš‘\n",
        "âœ… Used in insurance underwriting, claim fraud detection, and risk modeling.\n",
        "âœ… Helps insurers predict which claims are likely to be fraudulent.\n",
        "ğŸ“Œ Example: Gradient Boosting is used to predict car accident risks in auto insurance policies.\n",
        "\n",
        "10. How does regularization help in XGBoost?\n",
        "How Regularization Helps in XGBoost ğŸš€\n",
        "Regularization in XGBoost helps control model complexity, prevents overfitting, and improves generalization by penalizing large weights. XGBoost includes both L1 (Lasso) and L2 (Ridge) regularization, which are not present in traditional Gradient Boosting.\n",
        "\n",
        "Types of Regularization in XGBoost\n",
        "1ï¸âƒ£ L1 Regularization (Lasso) â€“ alpha (Î»1)\n",
        "âœ… Encourages sparsity by forcing some weights to be exactly zero.\n",
        "âœ… Reduces feature redundancy and performs feature selection.\n",
        "âœ… Formula:\n",
        "\n",
        "Penalty\n",
        "=\n",
        "ğœ†\n",
        "1\n",
        "âˆ‘\n",
        "âˆ£\n",
        "ğ‘¤\n",
        "ğ‘—\n",
        "âˆ£\n",
        "Penalty=Î»\n",
        "1\n",
        "â€‹\n",
        " âˆ‘âˆ£w\n",
        "j\n",
        "â€‹\n",
        " âˆ£\n",
        "âœ… Useful when dealing with high-dimensional data.\n",
        "\n",
        "ğŸ“Œ Effect: Eliminates irrelevant features, making the model simpler and faster.\n",
        "\n",
        "2ï¸âƒ£ L2 Regularization (Ridge) â€“ lambda (Î»2)\n",
        "âœ… Shrinks weights towards zero but does not remove them completely.\n",
        "âœ… Helps prevent large fluctuations in feature importance.\n",
        "âœ… Formula:\n",
        "\n",
        "Penalty\n",
        "=\n",
        "ğœ†\n",
        "2\n",
        "âˆ‘\n",
        "ğ‘¤\n",
        "ğ‘—\n",
        "2\n",
        "Penalty=Î»\n",
        "2\n",
        "â€‹\n",
        " âˆ‘w\n",
        "j\n",
        "2\n",
        "â€‹\n",
        "\n",
        "âœ… Used to control model complexity and avoid overfitting.\n",
        "\n",
        "ğŸ“Œ Effect: Helps reduce variance and improves generalization.\n",
        "\n",
        "3ï¸âƒ£ Tree-Specific Regularization (gamma)\n",
        "âœ… gamma (Î³): Minimum loss reduction required to make a split in a tree.\n",
        "âœ… Higher gamma values make trees conservative, preventing overfitting.\n",
        "âœ… Formula:\n",
        "\n",
        "LossÂ Reduction\n",
        ">\n",
        "ğ›¾\n",
        "LossÂ Reduction>Î³\n",
        "ğŸ“Œ Effect: Encourages simpler trees, reducing overfitting.\n",
        "\n",
        "How Regularization Helps in XGBoost\n",
        "âœ… Prevents Overfitting â†’ Reduces tree complexity by penalizing large weights.\n",
        "âœ… Improves Generalization â†’ The model performs well on unseen data.\n",
        "âœ… Feature Selection â†’ L1 Regularization removes irrelevant features automatically.\n",
        "âœ… Controls Tree Growth â†’ gamma prevents unnecessary splits.\n",
        "\n",
        "Tuning Regularization Parameters in XGBoost\n",
        "Parameter\tEffect\tRecommended Value\n",
        "lambda (Î»2)\tShrinks large weights (L2 Regularization)\t1 to 10\n",
        "alpha (Î»1)\tInduces sparsity (L1 Regularization)\t0 to 1\n",
        "gamma (Î³)\tControls tree split complexity\t0 to 5\n",
        "Example of using regularization in XGBoost (Python):\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "import xgboost as xgb\n",
        "\n",
        "# Define XGBoost parameters with regularization\n",
        "params = {\n",
        "    'objective': 'binary:logistic',  # Classification task\n",
        "    'lambda': 10,    # L2 Regularization\n",
        "    'alpha': 1,      # L1 Regularization\n",
        "    'gamma': 5,      # Pruning threshold\n",
        "    'max_depth': 6,  # Limits tree depth\n",
        "    'learning_rate': 0.1\n",
        "}\n",
        "\n",
        "# Train XGBoost model\n",
        "model = xgb.XGBClassifier(**params)\n",
        "model.fit(X_train, y_train)\n",
        "Key Takeaways\n",
        "âœ” Regularization (L1 & L2) prevents overfitting and improves generalization.\n",
        "âœ” L1 (alpha) removes irrelevant features (sparsity).\n",
        "âœ” L2 (lambda) smooths weights and prevents large fluctuations.\n",
        "âœ” gamma controls tree splits, reducing unnecessary complexity.\n",
        "\n",
        "11. What are some hyperparaHyperparameters to Tune in Gradient Boosting Models ğŸš€\n",
        "Tuning hyperparameters in Gradient Boosting Models (GBMs) like XGBoost, LightGBM, and CatBoost is crucial for optimizing performance. Here are the key hyperparameters categorized by their impact:\n",
        "\n",
        "1ï¸âƒ£ Tree-Based Parameters ğŸŒ³\n",
        "These control the structure of individual trees in the ensemble.\n",
        "\n",
        "Hyperparameter\tDescription\tRecommended Range\n",
        "n_estimators\tNumber of trees (boosting rounds)\t100 â€“ 1000 (higher for complex data)\n",
        "max_depth\tMaximum depth of each tree\t3 â€“ 10 (lower prevents overfitting)\n",
        "min_child_weight\tMinimum sum of instance weights per leaf\t1 â€“ 10 (higher reduces overfitting)\n",
        "gamma\tMinimum loss reduction required for a split (pruning)\t0 â€“ 5 (higher â†’ simpler model)\n",
        "ğŸ“Œ Effect:\n",
        "\n",
        "Increasing n_estimators improves accuracy but may overfit.\n",
        "Higher max_depth captures more complexity but risks overfitting.\n",
        "min_child_weight & gamma control tree growth.\n",
        "2ï¸âƒ£ Regularization Parameters ğŸ”’\n",
        "These prevent overfitting by penalizing complex models.\n",
        "\n",
        "Hyperparameter\tDescription\tRecommended Range\n",
        "lambda (L2)\tL2 regularization (Ridge)\t0 â€“ 10\n",
        "alpha (L1)\tL1 regularization (Lasso, sparsity)\t0 â€“ 1\n",
        "subsample\tFraction of samples used per tree\t0.5 â€“ 1.0\n",
        "ğŸ“Œ Effect:\n",
        "\n",
        "Higher lambda smooths weights (avoids large fluctuations).\n",
        "Higher alpha makes the model sparser (removes irrelevant features).\n",
        "Lower subsample (e.g., 0.7) adds randomness, reducing overfitting.\n",
        "3ï¸âƒ£ Boosting-Specific Parameters ğŸš€\n",
        "These control how boosting works.\n",
        "\n",
        "Hyperparameter\tDescription\tRecommended Range\n",
        "learning_rate\tStep size shrinkage (trade-off with n_estimators)\t0.01 â€“ 0.3\n",
        "colsample_bytree\tFraction of features used per tree\t0.5 â€“ 1.0\n",
        "colsample_bylevel\tFraction of features used per split\t0.5 â€“ 1.0\n",
        "ğŸ“Œ Effect:\n",
        "\n",
        "Lower learning_rate (e.g., 0.05) improves stability but requires higher n_estimators.\n",
        "Setting colsample_bytree < 1.0 reduces feature correlation, improving generalization.\n",
        "4ï¸âƒ£ Loss Function & Objective Parameters ğŸ¯\n",
        "These depend on the task (classification, regression, ranking, etc.)\n",
        "\n",
        "Hyperparameter\tDescription\tCommon Choices\n",
        "objective\tDefines the loss function\t'reg:squarederror', 'binary:logistic', 'multi:softmax'\n",
        "eval_metric\tEvaluation metric\t'rmse', 'logloss', 'auc'\n",
        "ğŸ“Œ Effect:\n",
        "\n",
        "For classification â†’ 'binary:logistic' (binary), 'multi:softmax' (multi-class).\n",
        "For regression â†’ 'reg:squarederror'.\n",
        "For ranking â†’ 'rank:pairwise'.\n",
        "5ï¸âƒ£ Speed Optimization Parameters âš¡\n",
        "These speed up training, especially for large datasets.\n",
        "\n",
        "Hyperparameter\tDescription\tRecommended Range\n",
        "tree_method\tMethod for building trees\t'auto', 'hist', 'gpu_hist'\n",
        "max_bin\tNumber of bins for continuous features\t255 â€“ 512 (higher for accuracy)\n",
        "grow_policy\tTree growth strategy\t'depthwise' (balanced), 'lossguide' (better for big data)\n",
        "ğŸ“Œ Effect:\n",
        "\n",
        "Use 'gpu_hist' for massive speedup if you have a GPU.\n",
        "Increasing max_bin improves precision but slows training.\n",
        "Tuning Hyperparameters (Python Example)\n",
        "Hereâ€™s how to tune XGBoost using GridSearchCV:\n",
        "\n",
        "python\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 300, 500],\n",
        "    'max_depth': [3, 6, 9],\n",
        "    'learning_rate': [0.01, 0.1, 0.3],\n",
        "    'subsample': [0.7, 1.0],\n",
        "    'colsample_bytree': [0.7, 1.0]\n",
        "}\n",
        "\n",
        "# Initialize model\n",
        "xgb = XGBClassifier()\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = GridSearchCV(xgb, param_grid, scoring='accuracy', cv=3, verbose=1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "Key Takeaways ğŸ†\n",
        "âœ” Tree Parameters (max_depth, min_child_weight, gamma) control complexity.\n",
        "âœ” Regularization (lambda, alpha) prevents overfitting.\n",
        "âœ” Boosting-Specific (learning_rate, subsample) balances speed vs. accuracy.\n",
        "\n",
        "12. What is the concept of Feature Importance in Boosting?\n",
        "Types of Feature Importance in Boosting\n",
        "Boosting algorithms offer multiple ways to measure feature importance:\n",
        "\n",
        "1ï¸âƒ£ Gain (Information Gain Importance) ğŸ”\n",
        "âœ… Measures how much a feature reduces impurity (entropy/Gini) in splits.\n",
        "âœ… Features with higher gain contribute more to model accuracy.\n",
        "âœ… Used in XGBoost, LightGBM.\n",
        "\n",
        "ğŸ“Œ Effect:\n",
        "\n",
        "High Gain â†’ Feature is crucial for decision-making.\n",
        "Low Gain â†’ Feature contributes little.\n",
        "2ï¸âƒ£ Split Count (Frequency Importance) ğŸ“Š\n",
        "âœ… Measures how many times a feature was used to split the data across all trees.\n",
        "âœ… Features that appear in more splits are considered important.\n",
        "âœ… Available in XGBoost, LightGBM.\n",
        "\n",
        "ğŸ“Œ Effect:\n",
        "\n",
        "High Split Count â†’ Feature is frequently used in tree construction.\n",
        "Low Split Count â†’ Feature is rarely selected.\n",
        "ğŸ”¹ Limitation: Might overestimate importance of features used at higher depths.\n",
        "\n",
        "3ï¸âƒ£ Shapley Values (SHAP) âš–ï¸\n",
        "âœ… Based on game theory, SHAP values show how much each feature contributes to each prediction.\n",
        "âœ… Works well with non-linear models like Gradient Boosting.\n",
        "âœ… Used in XGBoost, CatBoost, LightGBM.\n",
        "\n",
        "ğŸ“Œ Effect:\n",
        "\n",
        "More reliable than Gain & Split Count because it captures feature interactions.\n",
        "Computationally expensive but best for interpretability.\n",
        "How to Get Feature Importance in XGBoost (Python Example)\n",
        "python\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Train an XGBoost model\n",
        "model = xgb.XGBClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Plot feature importance\n",
        "xgb.plot_importance(model, importance_type=\"gain\")  # Options: 'weight', 'gain', 'cover'\n",
        "plt.show()\n",
        "Feature Importance in LightGBM\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Train LightGBM model\n",
        "model = lgb.LGBMClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Plot feature importance\n",
        "lgb.plot_importance(model, importance_type=\"gain\")\n",
        "plt.show()\n",
        "Feature Importance in CatBoost\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "import catboost as cb\n",
        "\n",
        "# Train CatBoost model\n",
        "model = cb.CatBoostClassifier(verbose=0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance\n",
        "feature_importance = model.get_feature_importance()\n",
        "print(feature_importance)\n",
        "Key Takeaways ğŸ¯\n",
        "âœ” Gain Importance â†’ Measures how much a feature improves splits (most common).\n",
        "âœ” Split Count Importance â†’ Counts how often a feature is used (biased towards deep trees).\n",
        "âœ” SHAP Values â†’ Best for explainability but computationally expensive.\n",
        "âœ” Use feature importance to remove irrelevant features and improve efficiency.\n",
        "\n",
        " 13.Why is CatBoost efficient for categorical data?\n",
        "Why is CatBoost Efficient for Categorical Data? ğŸ±ğŸš€\n",
        "CatBoost (Categorical Boosting) is a gradient boosting algorithm optimized for categorical data. Unlike XGBoost or LightGBM, which require manual encoding (e.g., one-hot encoding, label encoding), CatBoost automatically handles categorical features efficiently.\n",
        "\n",
        "1ï¸âƒ£ CatBoost Uses Ordered Target Encoding (OHE) ğŸ†\n",
        "âœ… Problem with Traditional Encoding (Label Encoding, One-Hot Encoding):\n",
        "\n",
        "One-hot encoding increases dimensionality, making models slow.\n",
        "Label encoding introduces unintended ordinal relationships.\n",
        "âœ… Solution: Ordered Target Encoding:\n",
        "\n",
        "Instead of replacing categories with fixed numbers, CatBoost dynamically encodes categories based on their target values.\n",
        "It avoids target leakage by ensuring that each row is encoded using only past observations.\n",
        "ğŸ“Œ Effect:\n",
        "\n",
        "More accurate encoding, especially for high-cardinality categorical features (e.g., Zip Codes, Product IDs).\n",
        "Prevents data leakage, unlike regular target encoding.\n",
        "2ï¸âƒ£ CatBoost Uses Greedy Permutation for Encoding ğŸ”€\n",
        "âœ… When encoding categorical features, CatBoost applies multiple permutations to shuffle data.\n",
        "âœ… This randomized approach prevents overfitting and bias from certain categories.\n",
        "âœ… Improves generalization in unseen data.\n",
        "\n",
        "ğŸ“Œ Effect:\n",
        "\n",
        "Ensures that encoding doesnâ€™t leak future information into training.\n",
        "Works well even with small datasets with high-cardinality categorical variables.\n",
        "3ï¸âƒ£ No Need for Manual Preprocessing ğŸš€\n",
        "âœ… XGBoost & LightGBM require:\n",
        "\n",
        "One-hot encoding or label encoding.\n",
        "Manual feature engineering for categorical data.\n",
        "âœ… CatBoost automatically detects categorical columns and applies target encoding.\n",
        "ğŸ“Œ Effect: Reduces preprocessing time and avoids human errors.\n",
        "\n",
        "4ï¸âƒ£ Supports Categorical Features Natively (No Need for One-Hot Encoding) ğŸ¯\n",
        "âœ… Unlike XGBoost & LightGBM, CatBoost natively supports categorical variables.\n",
        "âœ… Instead of expanding categorical features into multiple binary columns, CatBoost keeps them as a single column.\n",
        "\n",
        "ğŸ“Œ Effect:\n",
        "\n",
        "Lower memory usage ğŸ§ \n",
        "Faster training & inference â©\n",
        "5ï¸âƒ£ Works Well with Small Data & High-Cardinality Features ğŸ“Š\n",
        "âœ… Many ML models struggle when categorical variables have thousands of unique values (e.g., User IDs, Product Categories).\n",
        "âœ… CatBoost efficiently handles large categories, unlike one-hot encoding, which increases memory usage.\n",
        "\n",
        "ğŸ“Œ Effect: Best choice for real-world datasets with many categorical variables.\n",
        "\n",
        "How to Use CatBoost with Categorical Data (Python Example)\n",
        "python\n",
        "import catboost as cb\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sample Data\n",
        "data = {\n",
        "    'Gender': ['Male', 'Female', 'Female', 'Male', 'Male'],\n",
        "    'City': ['NY', 'LA', 'SF', 'NY', 'SF'],\n",
        "    'Purchase': [100, 200, 150, 130, 170]\n",
        "}\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Convert categorical columns to category dtype\n",
        "df['Gender'] = df['Gender'].astype('category')\n",
        "df['City'] = df['City'].astype('category')\n",
        "\n",
        "# Define categorical feature indices\n",
        "cat_features = ['Gender', 'City']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=['Purchase']), df['Purchase'])\n",
        "\n",
        "# Initialize and train CatBoost model\n",
        "model = cb.CatBoostRegressor(cat_features=cat_features, verbose=0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_test)\n",
        "print(predictions)\n",
        "Key Takeaways ğŸ¯\n",
        "âœ” CatBoost is optimized for categorical features without preprocessing.\n",
        "âœ” Uses Ordered Target Encoding to handle categories dynamically.\n",
        "âœ” Prevents target leakage using greedy permutations.\n",
        "âœ” Avoids One-Hot Encoding, reducing memory and improving speed.\n",
        "âœ” Best for datasets with high-cardinality categorical variables.\n",
        "\n",
        "\n",
        "    Practical\n",
        "\n",
        "\n",
        "14.Train an AdaBoost Classifier on a sample dataset and print model accuracy?\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train & test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize AdaBoost with a Decision Tree as the base model\n",
        "adaboost = AdaBoostClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(max_depth=1),  # Weak learner (stump)\n",
        "    n_estimators=50,  # Number of boosting rounds\n",
        "    learning_rate=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train AdaBoost model\n",
        "adaboost.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = adaboost.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"âœ… Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "15. Train an AdaBoost Regressor and evaluate performance using Mean Absolute Error (MAE)?\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Load dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split into train & test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize AdaBoost Regressor with a Decision Tree as the base model\n",
        "adaboost_reg = AdaBoostRegressor(\n",
        "    base_estimator=DecisionTreeRegressor(max_depth=4),  # Weak learner\n",
        "    n_estimators=50,  # Number of boosting rounds\n",
        "    learning_rate=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train AdaBoost model\n",
        "adaboost_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = adaboost_reg.predict(X_test)\n",
        "\n",
        "# Evaluate performance using Mean Absolute Error (MAE)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"âœ… Model MAE: {mae:.4f}\")\n",
        "\n",
        "\n",
        "16. Train a Gradient Boosting Classifier on the Breast Cancer dataset and print feature importance?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "feature_names = cancer.feature_names  # Get feature names\n",
        "\n",
        "# Split into train & test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train Gradient Boosting Classifier\n",
        "gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gb_clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"âœ… Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Get feature importance\n",
        "feature_importance = gb_clf.feature_importances_\n",
        "\n",
        "# Sort and plot feature importance\n",
        "sorted_idx = np.argsort(feature_importance)[::-1]  # Sort descending\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(X.shape[1]), feature_importance[sorted_idx], align=\"center\")\n",
        "plt.xticks(range(X.shape[1]), feature_names[sorted_idx], rotation=90)\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Feature Importance in Gradient Boosting Classifier\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "17. Train a Gradient Boosting Regressor and evaluate using R-Squared Score?\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split into train & test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train Gradient Boosting Regressor\n",
        "gb_reg = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gb_reg.predict(X_test)\n",
        "\n",
        "# Evaluate performance using R-Squared (RÂ²) score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"âœ… Model RÂ² Score: {r2:.4f}\")\n",
        "\n",
        "18. Train an XGBoost Classifier on a dataset and compare accuracy with Gradient Boosting?\n",
        "import numpy as np\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# Split into train & test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Gradient Boosting Classifier\n",
        "gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb_clf.fit(X_train, y_train)\n",
        "gb_pred = gb_clf.predict(X_test)\n",
        "gb_acc = accuracy_score(y_test, gb_pred)\n",
        "\n",
        "# Train XGBoost Classifier\n",
        "xgb_clf = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "xgb_pred = xgb_clf.predict(X_test)\n",
        "xgb_acc = accuracy_score(y_test, xgb_pred)\n",
        "\n",
        "# Print accuracy comparison\n",
        "print(f\"âœ… Gradient Boosting Accuracy: {gb_acc:.4f}\")\n",
        "print(f\"âœ… XGBoost Accuracy: {xgb_acc:.4f}\")\n",
        "\n",
        "# Compare results\n",
        "better_model = \"XGBoost\" if xgb_acc > gb_acc else \"Gradient Boosting\"\n",
        "print(f\"ğŸ† Best Model: {better_model}\")\n",
        "\n",
        "\n",
        "19. Train a CatBoost Classifier and evaluate using F1-Score?\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Load dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# Split into train & test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train CatBoost Classifier\n",
        "cat_clf = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, verbose=0, random_state=42)\n",
        "cat_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = cat_clf.predict(X_test)\n",
        "\n",
        "# Evaluate performance using F1-Score\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f\"âœ… Model F1-Score: {f1:.4f}\")\n",
        "\n",
        "\n",
        "20. Train an XGBoost Regressor and evaluate using Mean Squared Error (MSE)?\n",
        "    from xgboost import XGBRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split into train & test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train XGBoost Regressor\n",
        "xgb_reg = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "xgb_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = xgb_reg.predict(X_test)\n",
        "\n",
        "# Evaluate performance using Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"âœ… Model MSE: {mse:.4f}\")\n",
        "\n",
        "\n",
        "21. Train an AdaBoost Classifier and visualize feature importance?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "feature_names = cancer.feature_names  # Get feature names\n",
        "\n",
        "# Split into train & test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train AdaBoost Classifier\n",
        "adaboost_clf = AdaBoostClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(max_depth=2),  # Weak learner\n",
        "    n_estimators=50,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "adaboost_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = adaboost_clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"âœ… Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Get feature importance\n",
        "feature_importance = adaboost_clf.feature_importances_\n",
        "\n",
        "# Sort and plot feature importance\n",
        "sorted_idx = np.argsort(feature_importance)[::-1]  # Sort descending\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(X.shape[1]), feature_importance[sorted_idx], align=\"center\")\n",
        "plt.xticks(range(X.shape[1]), feature_names[sorted_idx], rotation=90)\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Feature Importance in AdaBoost Classifier\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "22. Train a Gradient Boosting Regressor and plot learning curves?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split into train & test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train Gradient Boosting Regressor\n",
        "gb_reg = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb_reg.fit(X_train, y_train)\n",
        "\n",
        "# Compute learning curve values\n",
        "train_errors = []\n",
        "test_errors = []\n",
        "\n",
        "for i, y_pred in enumerate(gb_reg.staged_predict(X_train)):  # Predictions on training set\n",
        "    train_errors.append(mean_squared_error(y_train, y_pred))\n",
        "\n",
        "for i, y_pred in enumerate(gb_reg.staged_predict(X_test)):  # Predictions on test set\n",
        "    test_errors.append(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "# Plot learning curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(train_errors) + 1), np.sqrt(train_errors), \"r-\", label=\"Training Error\")\n",
        "plt.plot(range(1, len(test_errors) + 1), np.sqrt(test_errors), \"b-\", label=\"Validation Error\")\n",
        "plt.xlabel(\"Number of Trees\")\n",
        "plt.ylabel(\"Root Mean Squared Error (RMSE)\")\n",
        "plt.title(\"Learning Curve of Gradient Boosting Regressor\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "23. Train an XGBoost Classifier and visualize feature importance?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from xgboost import XGBClassifier, plot_importance\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "feature_names = cancer.feature_names  # Get feature names\n",
        "\n",
        "# Split into train & test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train XGBoost Classifier\n",
        "xgb_clf = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = xgb_clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"âœ… Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_importance(xgb_clf, importance_type=\"gain\", max_num_features=10)  # Top 10 features\n",
        "plt.title(\"Feature Importance in XGBoost Classifier\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "24. Train a CatBoost Classifier and plot the confusion matrix?\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# Split into train & test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train CatBoost Classifier\n",
        "cat_clf = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, verbose=0, random_state=42)\n",
        "cat_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = cat_clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"âœ… Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix as heatmap\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Benign\", \"Malignant\"], yticklabels=[\"Benign\", \"Malignant\"])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix - CatBoost Classifier\")\n",
        "plt.show()\n",
        "\n",
        "25. Train an AdaBoost Classifier with different numbers of estimators and compare accuracy?\n",
        "   import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# Split into train & test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define different numbers of estimators to test\n",
        "n_estimators_list = [10, 50, 100, 200, 500]\n",
        "accuracy_scores = []\n",
        "\n",
        "# Train and evaluate AdaBoost Classifier with different n_estimators\n",
        "for n in n_estimators_list:\n",
        "    ada_clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2),\n",
        "                                 n_estimators=n,\n",
        "                                 learning_rate=0.1,\n",
        "                                 random_state=42)\n",
        "    ada_clf.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = ada_clf.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_scores.append(accuracy)\n",
        "    print(f\"âœ… n_estimators={n}, Accuracy={accuracy:.4f}\")\n",
        "\n",
        "# Plot accuracy vs. number of estimators\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(n_estimators_list, accuracy_scores, marker='o', linestyle='-')\n",
        "plt.xlabel(\"Number of Estimators\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"AdaBoost Classifier: Accuracy vs. Number of Estimators\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "26. Train a Gradient Boosting Classifier and visualize the ROC curve?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Load dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# Split into train & test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train Gradient Boosting Classifier\n",
        "gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_scores = gb_clf.predict_proba(X_test)[:, 1]  # Get probability for positive class\n",
        "\n",
        "# Compute ROC curve and AUC score\n",
        "fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color=\"blue\", lw=2, label=f\"ROC curve (AUC = {roc_auc:.4f})\")\n",
        "plt.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")  # Random classifier line\n",
        "plt.xlabel(\"False Positive Rate (FPR)\")\n",
        "plt.ylabel(\"True Positive Rate (TPR)\")\n",
        "plt.title(\"ROC Curve - Gradient Boosting Classifier\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "27. Train an XGBoost Regressor and tune the learning rate using GridSearchCV?\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split into train & test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define base XGBoost Regressor\n",
        "xgb_reg = xgb.XGBRegressor(n_estimators=100, max_depth=3, random_state=42)\n",
        "\n",
        "# Define hyperparameter grid for learning rate tuning\n",
        "param_grid = {'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]}\n",
        "\n",
        "# Perform Grid Search with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(xgb_reg, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best learning rate\n",
        "best_lr = grid_search.best_params_['learning_rate']\n",
        "print(f\"âœ… Best Learning Rate: {best_lr}\")\n",
        "\n",
        "# Train final model with best learning rate\n",
        "best_xgb = xgb.XGBRegressor(n_estimators=100, max_depth=3, learning_rate=best_lr, random_state=42)\n",
        "best_xgb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = best_xgb.predict(X_test)\n",
        "\n",
        "# Evaluate model using RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(f\"âœ… Model RMSE with best learning rate ({best_lr}): {rmse:.4f}\")\n",
        "\n",
        "\n",
        "28. Train a CatBoost Classifier on an imbalanced dataset and compare performance with class weighting?\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from collections import Counter\n",
        "\n",
        "# Generate an imbalanced dataset\n",
        "X, y = make_classification(n_samples=5000, n_features=20, weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "# Split into train & test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Check class distribution\n",
        "print(f\"Class distribution in training set: {Counter(y_train)}\")\n",
        "\n",
        "# Train CatBoost without class weighting\n",
        "cat_clf_no_weight = CatBoostClassifier(iterations=500, learning_rate=0.1, depth=6, verbose=0, random_state=42)\n",
        "cat_clf_no_weight.fit(X_train, y_train)\n",
        "\n",
        "# Predict & evaluate\n",
        "y_pred_no_weight = cat_clf_no_weight.predict(X_test)\n",
        "f1_no_weight = f1_score(y_test, y_pred_no_weight)\n",
        "print(f\"âœ… F1-Score (No Class Weighting): {f1_no_weight:.4f}\")\n",
        "\n",
        "# Train CatBoost with class weighting (scale_pos_weight)\n",
        "class_weights = {0: 1, 1: 9}  # Adjusting for class imbalance\n",
        "cat_clf_weighted = CatBoostClassifier(iterations=500, learning_rate=0.1, depth=6, verbose=0, scale_pos_weight=9, random_state=42)\n",
        "cat_clf_weighted.fit(X_train, y_train)\n",
        "\n",
        "# Predict & evaluate\n",
        "y_pred_weighted = cat_clf_weighted.predict(X_test)\n",
        "f1_weighted = f1_score(y_test, y_pred_weighted)\n",
        "print(f\"âœ… F1-Score (With Class Weighting): {f1_weighted:.4f}\")\n",
        "\n",
        "# Print classification reports\n",
        "print(\"\\nClassification Report (No Class Weighting):\")\n",
        "print(classification_report(y_test, y_pred_no_weight))\n",
        "print(\"\\nClassification Report (With Class Weighting):\")\n",
        "print(classification_report(y_test, y_pred_weighted))\n",
        "\n",
        "\n",
        "29. Train an AdaBoost Classifier and analyze the effect of different learning rates?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# Split into train & test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define different learning rates to test\n",
        "learning_rates = [0.001, 0.01, 0.1, 0.5, 1.0, 2.0]\n",
        "accuracy_scores = []\n",
        "\n",
        "# Train and evaluate AdaBoost Classifier with different learning rates\n",
        "for lr in learning_rates:\n",
        "    ada_clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2),\n",
        "                                 n_estimators=100,\n",
        "                                 learning_rate=lr,\n",
        "                                 random_state=42)\n",
        "    ada_clf.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = ada_clf.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_scores.append(accuracy)\n",
        "    print(f\"âœ… Learning Rate={lr}, Accuracy={accuracy:.4f}\")\n",
        "\n",
        "# Plot accuracy vs. learning rate\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(learning_rates, accuracy_scores, marker='o', linestyle='-')\n",
        "plt.xscale(\"log\")  # Log scale for better visualization\n",
        "plt.xlabel(\"Learning Rate (log scale)\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"AdaBoost Classifier: Accuracy vs. Learning Rate\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "30. Train an XGBoost Classifier for multi-class classification and evaluate using log-loss.\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss, accuracy_score\n",
        "\n",
        "# Load dataset (Digits dataset with 10 classes: 0-9)\n",
        "digits = load_digits()\n",
        "X, y = digits.data, digits.target\n",
        "\n",
        "# Split into train & test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Initialize and train XGBoost Classifier\n",
        "xgb_clf = xgb.XGBClassifier(objective=\"multi:softprob\", num_class=10, n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for log-loss calculation\n",
        "y_pred_probs = xgb_clf.predict_proba(X_test)\n",
        "\n",
        "# Predict classes for accuracy calculation\n",
        "y_pred = xgb_clf.predict(X_test)\n",
        "\n",
        "# Compute log-loss\n",
        "logloss = log_loss(y_test, y_pred_probs)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f\"âœ… Log-Loss: {logloss:.4f}\")\n",
        "print(f\"âœ… Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n"
      ]
    }
  ]
}