{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmMxuunaUKz6"
      },
      "outputs": [],
      "source": [
        "Theoretical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1. Can we use Bagging for regression problems?\n",
        "Yes, Bagging (Bootstrap Aggregating) can be used for regression problems! In fact, one of the most common examples of Bagging applied to regression is the Bagging Regressor, which is implemented in libraries like sklearn.\n",
        "\n",
        "How Bagging Works in Regression\n",
        "Bootstrapping: Multiple subsets of the training data are randomly sampled with replacement.\n",
        "Base Learners: A regression model (like Decision Trees, SVR, or k-NN) is trained on each subset.\n",
        "Aggregation: The final prediction is the average of all the individual models' predictions (instead of voting, which is used in classification).\n",
        "Advantages of Bagging in Regression\n",
        "Reduces variance: Helps smooth out fluctuations in predictions.\n",
        "Improves stability: Works well with high-variance models like Decision Trees.\n",
        "Handles overfitting: By averaging predictions, it avoids fitting too closely to any single dataset sample.\n",
        "Common Bagging Regressor Example\n",
        "In Python using sklearn:\n",
        "\n",
        "python\n",
        "\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Sample Data\n",
        "X = np.random.rand(100, 5)\n",
        "y = np.random.rand(100)\n",
        "\n",
        "# Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Bagging with Decision Tree Regressor\n",
        "bagging_reg = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=10, random_state=42)\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = bagging_reg.predict(X_test)\n",
        "\n",
        "# Evaluate Model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "Alternative Ensemble Methods for Regression\n",
        "Random Forest Regressor (uses Bagging with Decision Trees)\n",
        "Boosting Methods like Gradient Boosting, AdaBoost, and XGBoost\n",
        "Stacking & Blending for combining multiple models\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2.What is the difference between multiple model training and single model training?\n",
        "\n",
        "\n",
        "Difference Between Multiple Model Training and Single Model Training\n",
        "Machine learning models can be trained in two main ways: Single Model Training and Multiple Model Training (Ensemble Learning). Each approach has its own advantages and use cases.\n",
        "\n",
        "1. Single Model Training\n",
        "This involves training one machine learning model on a dataset and using it to make predictions.\n",
        "\n",
        "Characteristics:\n",
        "A single algorithm (e.g., Decision Tree, Neural Network, SVM) is used to learn from the data.\n",
        "The model optimizes its parameters to minimize error or maximize accuracy.\n",
        "Computationally less expensive compared to multiple model training.\n",
        "Simpler to interpret and deploy.\n",
        "Advantages:\n",
        "✅ Easy to implement and interpret\n",
        "✅ Requires fewer computational resources\n",
        "✅ Works well when the dataset is simple and well-structured\n",
        "\n",
        "Disadvantages:\n",
        "❌ Prone to overfitting (if the model is too complex) or underfitting (if the model is too simple)\n",
        "❌ Less robust to noisy data\n",
        "❌ Limited generalization power compared to ensemble models\n",
        "\n",
        "2. Multiple Model Training (Ensemble Learning)\n",
        "This involves training multiple models and combining their predictions to improve performance.\n",
        "\n",
        "Types of Multiple Model Training:\n",
        "Bagging (Bootstrap Aggregating)\n",
        "\n",
        "Trains multiple models on different random subsets of the data.\n",
        "Aggregates results (e.g., majority voting for classification, averaging for regression).\n",
        "Example: Random Forest (bagging applied to Decision Trees).\n",
        "Boosting\n",
        "\n",
        "Models are trained sequentially, where each model learns from the errors of the previous one.\n",
        "Focuses on hard-to-predict samples.\n",
        "Examples: Gradient Boosting, AdaBoost, XGBoost, LightGBM.\n",
        "Stacking\n",
        "\n",
        "Combines predictions of multiple models using a meta-model.\n",
        "Different base models are trained, and their predictions are used as inputs for another model.\n",
        "Advantages of Multiple Model Training:\n",
        "✅ Better Accuracy & Generalization: Reduces bias and variance\n",
        "✅ More Robust to Overfitting: Since multiple models are used\n",
        "✅ Handles Complex Patterns Better\n",
        "\n",
        "Disadvantages:\n",
        "❌ Computationally Expensive: Requires more processing power and memory\n",
        "❌ Harder to Interpret: Since multiple models contribute to predictions\n",
        "❌ More Complex to Tune and Deploy\n",
        "\n",
        "Key Differences at a Glance\n",
        "Feature\tSingle Model Training\tMultiple Model Training (Ensemble)\n",
        "Number of Models\tOne\tMultiple\n",
        "Performance\tCan be good but limited\tGenerally higher accuracy\n",
        "Risk of Overfitting\tHigher for complex models\tLower due to averaging\n",
        "Computational Cost\tLower\tHigher\n",
        "Interpretability\tEasier\tHarder\n",
        "Examples\tDecision Tree, SVM, Neural Networks\tRandom Forest, XGBoost, Stacking\n",
        "When to Use Which Approach?\n",
        "Use Single Model Training when:\n",
        "\n",
        "The dataset is simple and small.\n",
        "You need a fast, interpretable model.\n",
        "You don’t have the computational resources for ensembles.\n",
        "Use Multiple Model Training (Ensemble Learning) when:\n",
        "\n",
        "The dataset is complex with high variance.\n",
        "You need high accuracy and robustness.\n",
        "Overfitting is a concern.\n",
        "Computational resources are available.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "3. Explain the concept of feature randomness in Random Forest?\n",
        "Feature Randomness in Random Forest\n",
        "Random Forest is an ensemble learning method that builds multiple decision trees and combines their predictions to improve accuracy and reduce overfitting. One of its key strengths is feature randomness, which enhances model diversity and robustness.\n",
        "\n",
        "1. What is Feature Randomness?\n",
        "Feature randomness means that instead of using all the features for splitting at each node, Random Forest selects a random subset of features at each split.\n",
        "\n",
        "In a single Decision Tree, the best split is chosen based on all available features.\n",
        "In Random Forest, each tree randomly selects a subset of features at every split to reduce correlation between trees.\n",
        "How It Works:\n",
        "Bootstrap Sampling: A random subset of data is drawn (with replacement) for each tree.\n",
        "Random Feature Selection: At each split in a tree, only a random subset of features (not all features) is considered for finding the best split.\n",
        "Independent Tree Growth: Each tree grows independently with different splits, leading to diverse decision boundaries.\n",
        "Aggregation of Predictions: The final output is obtained by majority voting (classification) or averaging (regression).\n",
        "2. Why is Feature Randomness Important?\n",
        "✅ Reduces Overfitting\n",
        "Without randomness, deep trees can memorize patterns, leading to overfitting.\n",
        "Feature randomness ensures that no single feature dominates across all trees.\n",
        "✅ Improves Generalization\n",
        "Different trees learn different patterns, making the model more robust to unseen data.\n",
        "✅ Increases Diversity Among Trees\n",
        "If the same features are always chosen, trees become highly correlated.\n",
        "Randomizing feature selection ensures that trees make different decisions, leading to better ensemble performance.\n",
        "3. Feature Randomness Parameter in Random Forest\n",
        "The number of features considered at each split is controlled by the max_features parameter in sklearn.\n",
        "\n",
        "Common Settings for max_features:\n",
        "max_features Value\tEffect\n",
        "\"auto\" (default) or \"sqrt\"\tUses √(total features) (default for classification).\n",
        "\"log2\"\tUses log₂(total features).\n",
        "None\tUses all features (no randomness, behaves like Bagging).\n",
        "Integer value\tUses a fixed number of features per split.\n",
        "4. Example in Python\n",
        "python\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Random Forest with feature randomness\n",
        "rf = RandomForestClassifier(n_estimators=10, max_features='sqrt', random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Check feature importance\n",
        "print(rf.feature_importances_)\n",
        "5. Key Takeaways\n",
        "Feature Randomness in Random Forest prevents trees from always selecting the most dominant features.\n",
        "It decorrelates trees, reducing variance and improving generalization.\n",
        "The max_features parameter controls how many features are considered at each split.\n",
        "The default (sqrt for classification, log2 for regression) provides a good balance between accuracy and diversity.\n",
        "\n",
        "4. What is OOB (Out-of-Bag) Score?\n",
        "The Out-of-Bag (OOB) Score is a performance metric used in Random Forest models to estimate the generalization accuracy without needing a separate validation set. It is based on the idea of bootstrap aggregating (bagging).\n",
        "\n",
        "How It Works:\n",
        "In Random Forest, each tree is trained on a bootstrap sample (a random subset of the training data with replacement).\n",
        "Some data points are left out (typically around 37% of the samples are not included in any given tree).\n",
        "These out-of-bag (OOB) samples are then used to evaluate the model’s performance.\n",
        "The OOB score is computed as the mean accuracy (or another relevant metric) of these OOB predictions across all trees.\n",
        "Advantages of OOB Score:\n",
        "No need for a separate validation set → Saves data for training.\n",
        "Provides an unbiased estimate of the model’s performance.\n",
        "Reduces overfitting risks, as it gives insight into how well the model generalizes.\n",
        "When to Use:\n",
        "OOB score is useful in classification and regression tasks when using Random Forest.\n",
        "It works well when data is limited, since it acts as an internal cross-validation method.\n",
        "Would you like an example of how to compute the OOB score in Python using sklearn?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\t5. How can you measure the importance of features in a Random Forest model?\n",
        "In a Random Forest model, you can measure feature importance using two main techniques:\n",
        "\n",
        "1. Mean Decrease in Impurity (MDI) – Gini Importance\n",
        "Also called \"Feature Importance\" in sklearn.\n",
        "It measures how much a feature reduces impurity (e.g., Gini impurity or entropy in classification, variance in regression) across all trees.\n",
        "Features that cause more splits and reduce impurity more are considered important.\n",
        "Limitations: Can be biased toward features with many categories or high cardinality.\n",
        "Implementation in Python:\n",
        "\n",
        "python\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Train a Random Forest model\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance\n",
        "feature_importances = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
        "print(feature_importances)\n",
        "2. Mean Decrease in Accuracy (MDA) – Permutation Importance\n",
        "Also known as \"Permutation Feature Importance\".\n",
        "Measures how much shuffling a feature’s values affects model performance (e.g., accuracy for classification, RMSE for regression).\n",
        "More decrease in performance = more important the feature.\n",
        "Advantage: More robust and works well with correlated features.\n",
        "Implementation in Python:\n",
        "\n",
        "python\n",
        "\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# Compute permutation importance\n",
        "perm_importance = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=42)\n",
        "\n",
        "# Convert to a Pandas Series\n",
        "perm_importances = pd.Series(perm_importance.importances_mean, index=X_test.columns).sort_values(ascending=False)\n",
        "print(perm_importances)\n",
        "Comparison of Methods\n",
        "Method\tPros\tCons\n",
        "MDI (Gini Importance)\tFast, easy to compute\tBiased towards features with more categories\n",
        "MDA (Permutation Importance)\tMore reliable, works well with correlated features\tComputationally expensive\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. Explain the working principle of a Bagging Classifier?\n",
        "Bagging Classifier: Working Principle\n",
        "A Bagging Classifier (short for Bootstrap Aggregating) is an ensemble learning method that improves the stability and accuracy of machine learning models, especially for high-variance models like decision trees. It reduces overfitting by averaging multiple predictions.\n",
        "\n",
        "How Bagging Works\n",
        "Bootstrap Sampling\n",
        "\n",
        "The algorithm randomly selects multiple subsets of the training data with replacement.\n",
        "Each subset is used to train a separate model (typically, weak learners like Decision Trees).\n",
        "Model Training\n",
        "\n",
        "Each base model (also called a weak learner) is trained on a different subset of data.\n",
        "These models are independent of each other.\n",
        "Aggregation (Averaging/Voting)\n",
        "\n",
        "For classification, the final prediction is done via majority voting (the most common predicted class).\n",
        "For regression, the final output is the average of all model predictions.\n",
        "Advantages of Bagging\n",
        "✅ Reduces Overfitting – Works well with high-variance models like Decision Trees.\n",
        "✅ Improves Accuracy – By averaging multiple models, it creates a more robust prediction.\n",
        "✅ Handles Noisy Data – Bootstrapping ensures each model sees different variations of the data.\n",
        "✅ Parallelizable – Each model is independent, making bagging easy to parallelize.\n",
        "\n",
        "Bagging Classifier in Python (sklearn)\n",
        "python\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Sample Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define Base Model (Weak Learner)\n",
        "base_model = DecisionTreeClassifier()\n",
        "\n",
        "# Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(base_estimator=base_model, n_estimators=100, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"Bagging Classifier Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "When to Use Bagging?\n",
        "When you have a high-variance model (like Decision Trees).\n",
        "When you want to reduce overfitting without increasing bias.\n",
        "When you have enough computational resources to train multiple models in parallel.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "7. How do you evaluate a Bagging Classifier’s performance?\n",
        "Evaluating a Bagging Classifier’s Performance\n",
        "To assess how well a Bagging Classifier performs, you can use various evaluation metrics and techniques, depending on whether you're solving a classification or regression problem.\n",
        "\n",
        "1. Standard Evaluation Metrics\n",
        "✅ For Classification:\n",
        "Accuracy (if the classes are balanced)\n",
        "Precision, Recall, F1-score (for imbalanced data)\n",
        "ROC-AUC Score (for probabilistic outputs)\n",
        "Example in Python:\n",
        "\n",
        "python\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
        "\n",
        "# Predictions\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "y_pred_proba = bagging_clf.predict_proba(X_test)[:, 1]  # For ROC-AUC\n",
        "\n",
        "# Accuracy\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Classification Report (Precision, Recall, F1-score)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# ROC-AUC Score\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_pred_proba))\n",
        "✅ For Regression:\n",
        "Mean Squared Error (MSE)\n",
        "Mean Absolute Error (MAE)\n",
        "R² Score (Coefficient of Determination)\n",
        "Example in Python:\n",
        "\n",
        "python\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Predictions\n",
        "y_pred = bagging_regressor.predict(X_test)\n",
        "\n",
        "# Regression Metrics\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
        "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
        "print(\"R² Score:\", r2_score(y_test, y_pred))\n",
        "2. Cross-Validation\n",
        "To get a more reliable performance estimate, use k-fold cross-validation:\n",
        "\n",
        "python\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Perform 5-fold cross-validation\n",
        "cv_scores = cross_val_score(bagging_clf, X, y, cv=5, scoring=\"accuracy\")\n",
        "print(\"Cross-Validation Accuracy:\", cv_scores.mean())\n",
        "3. Out-of-Bag (OOB) Score\n",
        "Since Bagging uses bootstrap sampling, it leaves some data points out-of-bag (OOB), which can be used to evaluate performance without needing a separate validation set.\n",
        "\n",
        "python\n",
        "\n",
        "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, oob_score=True, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# OOB Score\n",
        "print(\"OOB Score:\", bagging_clf.oob_score_)\n",
        "✅ Advantage: Saves data for training instead of using a separate validation set.\n",
        "\n",
        "4. Bias-Variance Analysis\n",
        "If the model has low training and test accuracy → High bias (underfitting).\n",
        "If the model has high training accuracy but low test accuracy → High variance (overfitting).\n",
        "Bagging usually reduces variance while maintaining low bias.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " 8. How does a Bagging Regressor work?\n",
        "Bagging Regressor: How It Works\n",
        "A Bagging Regressor is an ensemble learning technique that improves regression model performance by reducing variance through bootstrap aggregating (bagging). It is particularly useful when using high-variance models like Decision Trees.\n",
        "\n",
        "Working Principle\n",
        "Bootstrap Sampling\n",
        "\n",
        "The training dataset is randomly sampled with replacement to create multiple subsets.\n",
        "Each subset has the same size as the original dataset but may contain duplicate instances.\n",
        "Train Multiple Weak Learners\n",
        "\n",
        "Each subset is used to train an independent base model (e.g., Decision Trees, SVMs, or other regressors).\n",
        "The models are trained in parallel.\n",
        "Aggregate Predictions (Averaging)\n",
        "\n",
        "Each trained model makes a prediction on a test input.\n",
        "The final prediction is obtained by averaging the outputs of all models:\n",
        "𝑦\n",
        "^\n",
        "=\n",
        "1\n",
        "𝑁\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑁\n",
        "𝑦\n",
        "^\n",
        "𝑖\n",
        "y\n",
        "^\n",
        "​\n",
        " =\n",
        "N\n",
        "1\n",
        "​\n",
        "\n",
        "i=1\n",
        "∑\n",
        "N\n",
        "​\n",
        "\n",
        "y\n",
        "^\n",
        "​\n",
        "\n",
        "i\n",
        "​\n",
        "\n",
        "This reduces variance and improves stability compared to a single model.\n",
        "Advantages of Bagging Regressor\n",
        "✅ Reduces Overfitting – Especially useful for high-variance models like Decision Trees.\n",
        "✅ More Stable Predictions – Averaging reduces fluctuations from individual models.\n",
        "✅ Parallelizable – Each model is trained independently, making it efficient for parallel computing.\n",
        "\n",
        "Bagging Regressor in Python (sklearn)\n",
        "python\n",
        "\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load Data (Assuming X, y are defined)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define Base Regressor (Weak Learner)\n",
        "base_regressor = DecisionTreeRegressor()\n",
        "\n",
        "# Create Bagging Regressor\n",
        "bagging_reg = BaggingRegressor(base_estimator=base_regressor, n_estimators=100, random_state=42)\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = bagging_reg.predict(X_test)\n",
        "\n",
        "# Evaluate Performance\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "Key Hyperparameters of BaggingRegressor\n",
        "base_estimator: The weak learner (default is DecisionTreeRegressor()).\n",
        "n_estimators: Number of models in the ensemble (higher = better generalization but more computation).\n",
        "oob_score: Whether to use out-of-bag (OOB) samples for performance evaluation.\n",
        "max_samples: Fraction of training samples used per model (default = 1.0).\n",
        "max_features: Fraction of features used per model (default = 1.0).\n",
        "When to Use Bagging Regressor?\n",
        "✅ When you have high-variance models (e.g., Decision Trees).\n",
        "✅ When you want better generalization compared to a single model.\n",
        "✅ When you need a model that is robust to noise and outliers.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "9. What is the main advantage of ensemble techniques?\n",
        "Main Advantage of Ensemble Techniques\n",
        "The primary advantage of ensemble techniques is that they combine multiple models to improve overall performance, making predictions more accurate, stable, and generalizable than individual models.\n",
        "\n",
        "Key Benefits of Ensemble Techniques\n",
        "✅ Higher Accuracy – Combining multiple models reduces individual weaknesses and leads to better performance.\n",
        "✅ Reduced Overfitting – Methods like Bagging (e.g., Random Forest) reduce variance, while Boosting (e.g., XGBoost) reduces bias.\n",
        "✅ Better Generalization – Ensembles perform well on unseen data, preventing over-reliance on training data patterns.\n",
        "✅ Handles Noisy Data – Averaging or voting across models minimizes the impact of outliers and noise.\n",
        "✅ Robustness – If one model fails or is weak, the ensemble can still perform well.\n",
        "✅ Versatility – Can be applied to classification, regression, anomaly detection, and ranking tasks.\n",
        "\n",
        "Comparison of Ensemble Methods\n",
        "Ensemble Method\tReduces Variance?\tReduces Bias?\tExample Algorithm\n",
        "Bagging\t✅ Yes\t❌ No\tRandom Forest\n",
        "Boosting\t❌ No\t✅ Yes\tXGBoost, AdaBoost\n",
        "Stacking\t✅ Yes\t✅ Yes\tStacked Generalization\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "10. What is the main challenge of ensemble methods?\n",
        "Main Challenge of Ensemble Methods\n",
        "While ensemble methods improve model performance, they also come with challenges. The biggest challenge is the increased computational complexity and interpretability compared to single models.\n",
        "\n",
        "Key Challenges of Ensemble Methods\n",
        "🚀 1. Computational Cost\n",
        "\n",
        "Training multiple models (e.g., in Random Forest or Boosting) requires more processing power and memory.\n",
        "Methods like Boosting (e.g., XGBoost, AdaBoost) are particularly slow for large datasets.\n",
        "⚡ 2. Complexity & Interpretability\n",
        "\n",
        "Ensembles (e.g., Stacking, Boosting) are harder to interpret than a single Decision Tree or Logistic Regression model.\n",
        "Extracting feature importance and understanding decision-making is more complex.\n",
        "🔄 3. Risk of Overfitting (Boosting)\n",
        "\n",
        "Some ensemble techniques (like Boosting) can overfit if not properly regularized (e.g., too many weak learners).\n",
        "Hyperparameter tuning is often required to prevent overfitting.\n",
        "📊 4. Large Model Size\n",
        "\n",
        "Models like Random Forest or Gradient Boosting can grow very large, making deployment and inference slower.\n",
        "Not ideal for real-time applications where fast predictions are required.\n",
        "🔢 5. Hyperparameter Tuning Complexity\n",
        "\n",
        "Many hyperparameters (e.g., n_estimators, max_depth, learning_rate) must be tuned for optimal performance.\n",
        "Techniques like Grid Search or Bayesian Optimization may be needed.\n",
        "How to Mitigate These Challenges?\n",
        "✅ Use feature selection to reduce dimensionality before applying ensembles.\n",
        "✅ Optimize hyperparameters (e.g., tuning n_estimators to prevent overfitting).\n",
        "✅ Use simpler base models if computational cost is a concern.\n",
        "✅ Apply model compression techniques (e.g., pruning, quantization) for deployment.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "11. Explain the key idea behind ensemble techniques?\n",
        "Key Idea Behind Ensemble Techniques\n",
        "The core idea behind ensemble techniques is to combine multiple models to create a stronger and more accurate predictive model than any single model alone.\n",
        "\n",
        "Instead of relying on one weak learner, ensemble methods aggregate predictions from multiple models to reduce errors, improve generalization, and increase stability.\n",
        "\n",
        "Why Does Ensemble Learning Work?\n",
        "✅ Diversity → Different models make different mistakes, and combining them balances out individual errors.\n",
        "✅ Reduction in Overfitting → Averaging predictions (Bagging) or sequential learning (Boosting) helps models generalize better.\n",
        "✅ Higher Accuracy → Aggregating results reduces noise and improves overall performance.\n",
        "\n",
        "Types of Ensemble Techniques & Their Key Ideas\n",
        "Ensemble Method\tKey Idea\tExample Algorithms\n",
        "Bagging (Bootstrap Aggregating)\tTrain multiple models on random subsets of data and combine predictions (reduces variance).\tRandom Forest, BaggingClassifier\n",
        "Boosting\tTrain models sequentially, where each new model corrects errors of the previous one (reduces bias).\tAdaBoost, XGBoost, Gradient Boosting\n",
        "Stacking\tTrain multiple models and use a meta-model to learn from their predictions.\tStacked Generalization\n",
        "Voting (Hard/Soft Voting)\tCombine predictions from different models using majority voting (classification) or averaging (regression).\tVotingClassifier\n",
        "Analogy: \"Wisdom of the Crowd\"\n",
        "Think of a quiz show where you can \"Ask the Audience.\"\n",
        "\n",
        "Instead of relying on one person’s answer, the audience collectively provides the best response.\n",
        "Bagging is like averaging everyone's answers.\n",
        "Boosting is like refining answers based on past mistakes.\n",
        "\n",
        "\n",
        "\n",
        "12. What is a Random Forest Classifier?\n",
        "Random Forest Classifier: Overview\n",
        "A Random Forest Classifier is an ensemble learning method that builds multiple Decision Trees and combines their predictions to improve accuracy, reduce overfitting, and enhance generalization.\n",
        "\n",
        "It is based on Bagging (Bootstrap Aggregating) and introduces additional randomness by selecting a random subset of features for each tree.\n",
        "\n",
        "How Does a Random Forest Classifier Work?\n",
        "1️⃣ Bootstrap Sampling (Bagging)\n",
        "\n",
        "The training data is randomly sampled with replacement to create multiple subsets.\n",
        "Each subset is used to train an independent Decision Tree.\n",
        "2️⃣ Feature Randomness (Feature Subset Selection)\n",
        "\n",
        "At each split, a random subset of features is selected instead of using all features.\n",
        "This reduces correlation between trees and improves diversity.\n",
        "3️⃣ Majority Voting (Aggregation)\n",
        "\n",
        "Each tree makes a prediction for a test sample.\n",
        "The final prediction is based on majority voting (classification) or averaging (regression).\n",
        "Advantages of Random Forest Classifier\n",
        "✅ Higher Accuracy – Reduces overfitting compared to a single Decision Tree.\n",
        "✅ Handles Missing Data – Can handle incomplete datasets effectively.\n",
        "✅ Works Well with High-Dimensional Data – Performs well even with many features.\n",
        "✅ Resistant to Overfitting – Due to feature randomness and bagging.\n",
        "✅ Robust to Noise and Outliers – Reduces variance in predictions.\n",
        "\n",
        "Random Forest Classifier in Python (sklearn)\n",
        "python\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Data (Assuming X, y are defined)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "\n",
        "# Evaluate Accuracy\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "Key Hyperparameters in Random Forest\n",
        "Hyperparameter\tDescription\n",
        "n_estimators\tNumber of Decision Trees (default: 100)\n",
        "max_depth\tMaximum depth of trees (prevents overfitting)\n",
        "max_features\tNumber of features considered at each split\n",
        "oob_score\tUses Out-of-Bag samples for evaluation\n",
        "n_jobs\tNumber of CPU cores used for parallel processing\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "13. What are the main types of ensemble techniques?\n",
        "Main Types of Ensemble Techniques\n",
        "Ensemble learning methods combine multiple models to improve accuracy, reduce overfitting, and enhance generalization. The main types of ensemble techniques are:\n",
        "\n",
        "1️⃣ Bagging (Bootstrap Aggregating)\n",
        "📌 Key Idea: Train multiple independent models on random subsets of data and combine their predictions (reduces variance).\n",
        "📌 Best for: High-variance models (e.g., Decision Trees).\n",
        "📌 Example Algorithms:\n",
        "\n",
        "Random Forest 🌳 (Multiple Decision Trees)\n",
        "Bagging Classifier (Any base model)\n",
        "🔹 How It Works:\n",
        "\n",
        "Bootstraps (random subsets) are created from the dataset.\n",
        "Each subset is used to train a weak learner (e.g., Decision Trees).\n",
        "The final prediction is obtained via majority voting (classification) or averaging (regression).\n",
        "python\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
        "2️⃣ Boosting\n",
        "📌 Key Idea: Train models sequentially, where each new model corrects the errors of the previous ones (reduces bias).\n",
        "📌 Best for: High-bias models (e.g., weak learners like shallow Decision Trees).\n",
        "📌 Example Algorithms:\n",
        "\n",
        "AdaBoost (Adaptive Boosting) 🔥\n",
        "Gradient Boosting (GBM, XGBoost, LightGBM, CatBoost)\n",
        "🔹 How It Works:\n",
        "\n",
        "The first model is trained on the original dataset.\n",
        "The next model focuses on misclassified samples by assigning higher weights to them.\n",
        "The process continues iteratively, improving performance.\n",
        "python\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "adaboost_clf = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "3️⃣ Stacking (Stacked Generalization)\n",
        "📌 Key Idea: Train multiple different models and use a meta-model to combine their outputs.\n",
        "📌 Best for: When diverse models (e.g., Logistic Regression + Decision Trees + SVM) perform well in different areas.\n",
        "📌 Example Algorithm:\n",
        "\n",
        "StackingClassifier (combines different models into one)\n",
        "🔹 How It Works:\n",
        "\n",
        "Multiple base models (e.g., Decision Trees, SVM, Neural Networks) are trained.\n",
        "Their predictions become the input for a meta-model.\n",
        "The meta-model learns how to best combine the base models’ outputs.\n",
        "python\n",
        "\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('dt', DecisionTreeClassifier()),\n",
        "        ('svm', SVC(probability=True))\n",
        "    ],\n",
        "    final_estimator=LogisticRegression()\n",
        ")\n",
        "4️⃣ Voting (Majority Voting)\n",
        "📌 Key Idea: Combine predictions from multiple models using majority voting (classification) or averaging (regression).\n",
        "📌 Best for: When different models perform well on different aspects of the problem.\n",
        "📌 Example Algorithm:\n",
        "\n",
        "VotingClassifier (for classification)\n",
        "🔹 Types of Voting:\n",
        "\n",
        "Hard Voting → Chooses the class with the most votes.\n",
        "Soft Voting → Averages predicted probabilities (better when models provide probability outputs).\n",
        "python\n",
        "\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('lr', LogisticRegression()),\n",
        "        ('dt', DecisionTreeClassifier()),\n",
        "        ('svm', SVC(probability=True))\n",
        "    ],\n",
        "    voting='soft'  # Use 'hard' for majority voting\n",
        ")\n",
        "Comparison of Ensemble Techniques\n",
        "Method\tReduces Variance?\tReduces Bias?\tExamples\n",
        "Bagging\t✅ Yes\t❌ No\tRandom Forest, BaggingClassifier\n",
        "Boosting\t❌ No\t✅ Yes\tAdaBoost, XGBoost, LightGBM\n",
        "Stacking\t✅ Yes\t✅ Yes\tStackingClassifier\n",
        "Voting\t✅ Yes\t❌ No\tVotingClassifier\n",
        "Which Ensemble Method Should You Use?\n",
        "✅ If your model overfits (high variance) → Use Bagging (e.g., Random Forest).\n",
        "✅ If your model underfits (high bias) → Use Boosting (e.g., XGBoost).\n",
        "✅ If you have diverse models → Use Stacking or Voting.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "14. What is ensemble learning in machine learning?\n",
        "Ensemble Learning in Machine Learning\n",
        "📌 Definition:\n",
        "Ensemble learning is a technique that combines multiple models (weak or strong learners) to produce a better predictive model than any individual model alone. The goal is to improve accuracy, stability, and generalization by reducing variance, bias, or both.\n",
        "\n",
        "Why Use Ensemble Learning?\n",
        "✅ Higher Accuracy → Combining models reduces individual weaknesses.\n",
        "✅ Reduces Overfitting → Techniques like Bagging stabilize predictions.\n",
        "✅ Reduces Underfitting → Techniques like Boosting refine weak models.\n",
        "✅ Handles Noisy Data → Aggregating predictions minimizes the effect of outliers.\n",
        "✅ Works with Any Model → Can combine Decision Trees, Neural Networks, SVMs, etc.\n",
        "\n",
        "Types of Ensemble Learning\n",
        "1️⃣ Bagging (Bootstrap Aggregating)\n",
        "\n",
        "Goal: Reduces variance by training multiple models on random subsets of data.\n",
        "Example: Random Forest (combining Decision Trees).\n",
        "2️⃣ Boosting\n",
        "\n",
        "Goal: Reduces bias by sequentially training models where each focuses on correcting previous mistakes.\n",
        "Example: XGBoost, AdaBoost, Gradient Boosting.\n",
        "3️⃣ Stacking (Stacked Generalization)\n",
        "\n",
        "Goal: Combines multiple diverse models and trains a meta-model to learn from their outputs.\n",
        "Example: StackingClassifier (combining SVMs, Decision Trees, etc.).\n",
        "4️⃣ Voting\n",
        "\n",
        "Goal: Aggregates predictions from multiple models using majority voting (classification) or averaging (regression).\n",
        "Example: VotingClassifier.\n",
        "Analogy: \"Wisdom of the Crowd\" 🤝\n",
        "Think of ensemble learning like a panel of experts making a decision together:\n",
        "\n",
        "Bagging → Ask multiple experts independently and take the average opinion.\n",
        "Boosting → Ask one expert, let another improve their response, and repeat.\n",
        "Stacking → Combine different expert opinions and let a final judge decide.\n",
        "Python Example: Random Forest (Bagging)\n",
        "python\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Data (Assuming X, y are defined)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions & Evaluation\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "When Should You Use Ensemble Learning?\n",
        "✅ If a single model is not performing well.\n",
        "✅ If you want to reduce overfitting (high variance models) → Use Bagging.\n",
        "✅ If you want to reduce bias (weak models) → Use Boosting.\n",
        "✅ If you have diverse models that perform well in different areas → Use Stacking or Voting.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "15. When should we avoid using ensemble methods?\n",
        "When to Avoid Using Ensemble Methods?\n",
        "Although ensemble methods improve accuracy and generalization, they are not always the best choice. You should avoid using them in the following scenarios:\n",
        "\n",
        "🚀 1. When Interpretability is Important\n",
        "📌 Why?\n",
        "\n",
        "Ensembles like Random Forest, XGBoost, and Stacking create black-box models, making it hard to explain predictions.\n",
        "If model explainability is required (e.g., in healthcare, finance, legal applications), a simpler model like Logistic Regression or Decision Trees is better.\n",
        "✅ Use Instead: Decision Trees, Logistic Regression, Rule-Based Models.\n",
        "\n",
        "⏳ 2. When Computational Cost is Too High\n",
        "📌 Why?\n",
        "\n",
        "Training hundreds or thousands of models (e.g., in Random Forest or Gradient Boosting) requires significant computing power and memory.\n",
        "Boosting models like XGBoost or LightGBM can be slow for large datasets.\n",
        "✅ Use Instead: A single Decision Tree, SVM, or simpler models.\n",
        "\n",
        "⚡ 3. When Real-Time Predictions are Needed\n",
        "📌 Why?\n",
        "\n",
        "Ensemble models are slower during inference, especially in real-time applications like fraud detection, recommendation systems, or self-driving cars.\n",
        "A complex ensemble (like Stacking) might cause latency issues in production.\n",
        "✅ Use Instead:\n",
        "\n",
        "Lightweight models like Logistic Regression or Naïve Bayes.\n",
        "If using ensembles, prune the model or use model distillation.\n",
        "📊 4. When Data is Small or Clean\n",
        "📌 Why?\n",
        "\n",
        "If the dataset is small and noise-free, a single, well-tuned model may perform just as well.\n",
        "Bagging-based ensembles (like Random Forest) work best with high variance models; on small datasets, they may overfit.\n",
        "✅ Use Instead: A single Decision Tree, SVM, or even k-NN.\n",
        "\n",
        "🔄 5. When a Single Strong Model is Sufficient\n",
        "📌 Why?\n",
        "\n",
        "If a single model performs well (e.g., a well-regularized Logistic Regression or a fine-tuned Neural Network), adding ensembles may not bring significant improvements.\n",
        "Ensembles shine when individual models struggle.\n",
        "✅ Use Instead: Fine-tune hyperparameters of a single model.\n",
        "\n",
        "🚫 6. When Overfitting is a Concern (Boosting Models)\n",
        "📌 Why?\n",
        "\n",
        "Boosting methods (e.g., AdaBoost, XGBoost) focus on hard-to-classify samples, which can lead to overfitting, especially on noisy datasets.\n",
        "✅ Use Instead: Bagging-based methods (Random Forest) or simpler models.\n",
        "\n",
        "💡 Conclusion: When NOT to Use Ensembles\n",
        "Scenario\tWhy Avoid Ensembles?\tBetter Alternative\n",
        "Need interpretability\tEnsembles are black-box models\tDecision Trees, Logistic Regression\n",
        "Limited computation\tTraining takes too long\tSingle Decision Tree, SVM\n",
        "Real-time predictions\tInference is slow\tLogistic Regression, LightGBM (optimized)\n",
        "Small dataset\tMay not generalize well\tSingle model, k-NN\n",
        "A single model performs well\tEnsembles add unnecessary complexity\tFine-tune the single model\n",
        "Noisy data\tBoosting may overfit\tBagging or Regularization\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "16. How does Bagging help in reducing overfitting?\n",
        "How Does Bagging Reduce Overfitting?\n",
        "📌 Bagging (Bootstrap Aggregating) reduces overfitting by combining multiple independent models trained on random subsets of data. By averaging predictions (or majority voting), it smooths out noise, reducing variance while maintaining accuracy.\n",
        "\n",
        "🔹 How Bagging Works (Step-by-Step)\n",
        "1️⃣ Bootstrap Sampling (Data Randomization)\n",
        "\n",
        "Bagging creates multiple random subsets of the original dataset (with replacement).\n",
        "Each subset is used to train a separate model (e.g., Decision Trees in Random Forest).\n",
        "2️⃣ Independent Model Training\n",
        "\n",
        "Each model is trained separately on its subset, leading to diversity in learning.\n",
        "Since models see different data samples, they learn different patterns, preventing overfitting to a specific subset.\n",
        "3️⃣ Aggregation of Predictions\n",
        "\n",
        "For classification → Uses majority voting (the most common class wins).\n",
        "For regression → Uses averaging (reduces extreme predictions).\n",
        "This aggregation reduces model variance, preventing overfitting to noisy patterns in the data.\n",
        "🔹 Why Does Bagging Reduce Overfitting?\n",
        "✅ Reduces Variance → By averaging over multiple models, extreme predictions are canceled out, leading to a more stable model.\n",
        "✅ Prevents Learning Noise → Individual models may overfit noisy samples, but the combined model smooths out these errors.\n",
        "✅ Creates Diversity → Different models learn different aspects of the data, making the ensemble more robust.\n",
        "✅ Resistant to Outliers → Outliers affect individual models, but their impact is minimized in aggregation.\n",
        "\n",
        "🔹 Example: Bagging Classifier in Python\n",
        "python\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Data (Assuming X, y are defined)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Classifier (with Decision Tree as base model)\n",
        "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions & Evaluation\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "print(\"Bagging Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "🔹 Bagging vs. Boosting: When to Use?\n",
        "Method\tOverfitting Risk?\tBest When...\n",
        "Bagging\tLow (reduces variance) ✅\tThe model is overfitting (high variance).\n",
        "Boosting\tHigh (can overfit) ⚠️\tThe model is underfitting (high bias).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "17. Why is Random Forest better than a single Decision Tree?\n",
        "Why is Random Forest Better Than a Single Decision Tree?\n",
        "📌 Random Forest 🌳 is an ensemble method that combines multiple Decision Trees to produce a more accurate, stable, and generalized model. It improves upon a single Decision Tree by reducing overfitting, increasing robustness, and handling noise better.\n",
        "\n",
        "🔹 Key Advantages of Random Forest Over a Decision Tree\n",
        "Feature\tRandom Forest 🌳\tSingle Decision Tree 🌲\n",
        "Overfitting\t🚀 Reduces overfitting by averaging multiple trees ✅\t⚠️ Prone to overfitting (deep trees memorize data) ❌\n",
        "Accuracy\t🔥 Higher accuracy (reduces variance)\tLower accuracy, especially on unseen data\n",
        "Stability\t✅ Stable (small changes in data don’t affect much)\t❌ Unstable (small data changes → completely different tree)\n",
        "Bias-Variance Tradeoff\t✅ Lower variance (ensemble effect)\t❌ High variance (sensitive to noise)\n",
        "Feature Importance\t✅ Provides feature importance insights\tLimited interpretability\n",
        "Resistant to Outliers\t✅ Less sensitive to outliers\t❌ Highly sensitive to outliers\n",
        "Handles Large Datasets\t✅ Scales well for high-dimensional data\t❌ Can become slow for large datasets\n",
        "🔹 Why Does Random Forest Perform Better?\n",
        "✅ 1. Uses Bagging to Reduce Overfitting\n",
        "\n",
        "A single Decision Tree memorizes data (high variance).\n",
        "Random Forest averages multiple trees, making it more generalized.\n",
        "✅ 2. Random Feature Selection Improves Diversity\n",
        "\n",
        "Each Decision Tree in Random Forest uses a random subset of features, leading to decorrelated trees that don’t make the same mistakes.\n",
        "This prevents a dominant feature from overpowering the learning process.\n",
        "✅ 3. More Robust to Noise and Outliers\n",
        "\n",
        "Decision Trees split based on training data, making them sensitive to noise.\n",
        "Random Forest averages predictions, dampening the effect of outliers.\n",
        "✅ 4. Better Generalization on Unseen Data\n",
        "\n",
        "A single Decision Tree can perfectly fit training data but fails on test data.\n",
        "Random Forest maintains high accuracy on test data by reducing variance.\n",
        "🔹 Example: Random Forest vs. Decision Tree in Python\n",
        "python\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Data (Assuming X, y are defined)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Single Decision Tree\n",
        "tree_clf = DecisionTreeClassifier(random_state=42)\n",
        "tree_clf.fit(X_train, y_train)\n",
        "tree_pred = tree_clf.predict(X_test)\n",
        "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, tree_pred))\n",
        "\n",
        "# Random Forest\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "rf_pred = rf_clf.predict(X_test)\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf_pred))\n",
        "💡 Expected Result:\n",
        "🚀 Random Forest Accuracy > Decision Tree Accuracy, proving its ability to generalize better.\n",
        "\n",
        "🔹 When Should You Use a Decision Tree Instead?\n",
        "❌ If interpretability is required → Decision Trees are easier to explain.\n",
        "❌ If computation time is limited → Training Random Forest is slower.\n",
        "\n",
        "However, in most cases, Random Forest is preferred for higher accuracy and stability.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "18. What is the role of bootstrap sampling in Bagging?\n",
        "Role of Bootstrap Sampling in Bagging\n",
        "📌 Bootstrap sampling is a key component of Bagging (Bootstrap Aggregating). It helps create diverse training sets by randomly sampling data with replacement, making the ensemble more robust and reducing overfitting.\n",
        "\n",
        "🔹 How Does Bootstrap Sampling Work?\n",
        "1️⃣ Random Sampling with Replacement\n",
        "\n",
        "Given a dataset of N samples, Bagging randomly selects N samples with replacement to train each base model.\n",
        "Some samples may be repeated, while others may be left out.\n",
        "2️⃣ Train Multiple Base Models on Different Subsets\n",
        "\n",
        "Each model (e.g., Decision Trees in a Random Forest) is trained on a different bootstrapped dataset.\n",
        "Since models see different data distributions, they learn different patterns.\n",
        "3️⃣ Aggregate Predictions\n",
        "\n",
        "For classification → Uses majority voting (most common prediction wins).\n",
        "For regression → Uses averaging (reduces extreme predictions).\n",
        "🔹 Why is Bootstrap Sampling Important?\n",
        "✅ 1. Introduces Diversity → Different models see different data, preventing overfitting to a single pattern.\n",
        "✅ 2. Reduces Overfitting → Individual models overfit less because their predictions are averaged.\n",
        "✅ 3. Stabilizes Predictions → Reduces variance by ensuring no single model dominates the ensemble.\n",
        "✅ 4. Improves Generalization → Helps the model perform better on unseen data.\n",
        "✅ 5. Creates Out-of-Bag (OOB) Samples → The left-out samples can be used for internal validation (like a built-in cross-validation).\n",
        "\n",
        "🔹 Example: Bootstrap Sampling in Python\n",
        "python\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Original dataset (10 samples)\n",
        "data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "\n",
        "# Bootstrap sampling (random selection with replacement)\n",
        "np.random.seed(42)\n",
        "bootstrap_sample = np.random.choice(data, size=len(data), replace=True)\n",
        "\n",
        "print(\"Original Data:\", data)\n",
        "print(\"Bootstrap Sample:\", bootstrap_sample)\n",
        "Output Example:\n",
        "\n",
        "\n",
        "Original Data: [1 2 3 4 5 6 7 8 9 10]\n",
        "Bootstrap Sample: [7 4 8 5 7 9 1 8 9 9]\n",
        "👉 Notice that some values repeat, while others are missing.\n",
        "\n",
        "🔹 Bootstrap Sampling in Random Forest\n",
        "python\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Train a Random Forest Classifier (automatically uses Bootstrap Sampling)\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, bootstrap=True, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "🔹 Summary: Why Bootstrap Sampling?\n",
        "Advantage\tWhy It Matters?\n",
        "Diversity\tEach model sees a different subset of data.\n",
        "Overfitting Reduction\tModels don’t memorize data; they generalize better.\n",
        "Bias-Variance Tradeoff\tReduces variance, making predictions more stable.\n",
        "Internal Validation\tOOB score estimates performance without extra validation data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "19. What are some real-world applications of ensemble techniques?\n",
        "Real-World Applications of Ensemble Techniques\n",
        "Ensemble techniques are widely used across various industries because they boost accuracy, reduce overfitting, and enhance model stability. Below are some key applications:\n",
        "\n",
        "1️⃣ Fraud Detection (Finance & Banking) 💰\n",
        "Problem: Identifying fraudulent transactions in real-time.\n",
        "Ensemble Solution: Random Forest + Gradient Boosting (XGBoost, LightGBM)\n",
        "Why?\n",
        "✅ Detects fraud patterns in noisy data.\n",
        "✅ Combines weak models to improve anomaly detection.\n",
        "Example: Credit card fraud detection in banks like Visa, MasterCard, and PayPal.\n",
        "2️⃣ Recommendation Systems (E-commerce & Entertainment) 🎥🛒\n",
        "Problem: Suggesting relevant products or content to users.\n",
        "Ensemble Solution: Hybrid models (Collaborative Filtering + XGBoost)\n",
        "Why?\n",
        "✅ Improves personalization by blending multiple models.\n",
        "✅ Reduces bias from a single recommendation model.\n",
        "Example:\n",
        "Netflix, Spotify → Movie & music recommendations.\n",
        "Amazon, eBay → Personalized shopping recommendations.\n",
        "3️⃣ Medical Diagnosis & Disease Prediction 🏥\n",
        "Problem: Diagnosing diseases using medical records & imaging.\n",
        "Ensemble Solution: Random Forest + Neural Networks + Boosting\n",
        "Why?\n",
        "✅ Higher accuracy than single models.\n",
        "✅ Reduces false positives in medical tests.\n",
        "Example:\n",
        "Cancer detection (using MRI scans).\n",
        "COVID-19 detection (X-ray + CT scans).\n",
        "Diabetes risk prediction (based on patient history).\n",
        "4️⃣ Stock Market Prediction 📈\n",
        "Problem: Forecasting stock prices & financial trends.\n",
        "Ensemble Solution: Stacking (LSTMs + Random Forest + XGBoost)\n",
        "Why?\n",
        "✅ Reduces the risk of overfitting to volatile markets.\n",
        "✅ Improves accuracy by combining multiple models.\n",
        "Example: Hedge funds, financial institutions, and algorithmic trading firms.\n",
        "5️⃣ Sentiment Analysis & NLP (Social Media & Customer Feedback) 📝\n",
        "Problem: Understanding emotions & opinions from text (tweets, reviews, etc.).\n",
        "Ensemble Solution: Stacking (BERT + XGBoost + Random Forest)\n",
        "Why?\n",
        "✅ Captures complex patterns in text data.\n",
        "✅ Enhances accuracy in spam detection & hate speech filtering.\n",
        "Example:\n",
        "Twitter & Facebook → Hate speech & misinformation detection.\n",
        "Amazon & Yelp → Product review sentiment analysis.\n",
        "6️⃣ Autonomous Vehicles (Self-Driving Cars) 🚗\n",
        "Problem: Real-time object detection & decision-making in autonomous driving.\n",
        "Ensemble Solution: Boosting (XGBoost) + Deep Learning (CNNs)\n",
        "Why?\n",
        "✅ Improves image recognition for pedestrians, signs, and vehicles.\n",
        "✅ Reduces false positives in obstacle detection.\n",
        "Example: Tesla Autopilot, Waymo (Google’s self-driving project).\n",
        "7️⃣ Cybersecurity & Intrusion Detection 🔒\n",
        "Problem: Detecting and preventing cyber-attacks & malware threats.\n",
        "Ensemble Solution: Bagging (Random Forest) + Boosting (XGBoost, AdaBoost)\n",
        "Why?\n",
        "✅ Detects anomalies in network traffic & system logs.\n",
        "✅ Reduces false alarms in intrusion detection systems.\n",
        "Example: SIEM systems in large corporations (e.g., IBM, Splunk).\n",
        "🔹 Summary: Why Use Ensemble Learning?\n",
        "Industry\tApplication\tEnsemble Methods Used\n",
        "Finance & Banking\tFraud detection\tRandom Forest, XGBoost\n",
        "E-commerce\tRecommendation Systems\tCollaborative Filtering + Boosting\n",
        "Healthcare\tDisease diagnosis\tRandom Forest + Neural Networks\n",
        "Stock Market\tPrice prediction\tStacking (LSTMs + XGBoost)\n",
        "Social Media\tSentiment analysis\tNLP Models + Boosting\n",
        "Autonomous Vehicles\tSelf-driving car vision\tCNNs + Boosting\n",
        "Cybersecurity\tIntrusion detection\tRandom Forest + Boosting\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "20. What is the difference between Bagging and Boosting?\n",
        "\n",
        "Bagging vs. Boosting: Key Differences\n",
        "Both Bagging and Boosting are ensemble learning techniques that combine multiple models to improve performance, but they work in different ways.\n",
        "\n",
        "Feature\tBagging 🌳\tBoosting 🔥\n",
        "Objective\tReduce variance (overfitting) ✅\tReduce bias (underfitting) ✅\n",
        "How It Works?\tTrains models independently on random subsets (bootstrap sampling)\tTrains models sequentially, correcting previous mistakes\n",
        "Model Training\tParallel (all models train at the same time) ⚡\tSequential (each model learns from previous errors) 🔄\n",
        "Data Sampling\tBootstrap Sampling (with replacement)\tUses the entire dataset, but reweights misclassified samples\n",
        "Base Model Type\tHigh-variance models (e.g., Decision Trees) 🌲\tWeak learners (e.g., Shallow Trees) 🌱\n",
        "Final Prediction\tMajority Voting (classification) or Averaging (regression)\tWeighted combination of all models\n",
        "Reduces Overfitting?\t✅ Yes, by averaging multiple models\t❌ Can overfit if too many models are used\n",
        "Computation Time\tFaster (models train in parallel) ⚡\tSlower (models train sequentially) 🕒\n",
        "Best For\tHigh variance models (overfitting)\tHigh bias models (underfitting)\n",
        "Example Algorithms\tRandom Forest, Bagging Classifier\tAdaBoost, Gradient Boosting, XGBoost\n",
        "🔹 How They Work: Example\n",
        "1️⃣ Bagging (Bootstrap Aggregating)\n",
        "Trains multiple models (e.g., Decision Trees) in parallel.\n",
        "Each model sees a random subset of data (bootstrap sampling).\n",
        "Final prediction:\n",
        "Classification → Majority voting 🗳️\n",
        "Regression → Averaging 📊\n",
        "Example Algorithm: Random Forest\n",
        "python\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "2️⃣ Boosting (Sequential Learning)\n",
        "Models train sequentially, learning from previous mistakes.\n",
        "Misclassified samples get higher weights in the next iteration.\n",
        "Final prediction: Weighted sum of weak learners.\n",
        "Example Algorithm: AdaBoost, Gradient Boosting, XGBoost\n",
        "python\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "boosting = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
        "boosting.fit(X_train, y_train)\n",
        "🔹 When to Use Bagging vs. Boosting?\n",
        "Scenario\tUse Bagging 🌳\tUse Boosting 🔥\n",
        "Overfitting (High Variance)?\t✅ Reduces overfitting\t❌ Can overfit if not tuned properly\n",
        "Underfitting (High Bias)?\t❌ Not ideal\t✅ Improves weak models\n",
        "Fast Training Needed?\t✅ Parallel processing\t❌ Slower (sequential training)\n",
        "Small Data Size?\t❌ Needs large data\t✅ Works well with small datasets\n",
        "Interpretability?\t✅ Easy to understand\t❌ Harder to interpret\n",
        "🔹 Summary: Key Takeaways\n",
        "Bagging → Reduces variance, best for unstable models (e.g., Decision Trees).\n",
        "Boosting → Reduces bias, best for weak models (e.g., shallow trees, logistic regression).\n",
        "Bagging = Parallel, Boosting = Sequential.\n",
        "Bagging is safer (less overfitting), while Boosting is more powerful but needs tuning.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Practical\n",
        "21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy?\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with Decision Trees\n",
        "bagging_clf = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,  # Number of trees in the ensemble\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)?\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "diabetes = load_diabetes()\n",
        "X, y = diabetes.data, diabetes.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Bagging Regressor with Decision Trees\n",
        "bagging_reg = BaggingRegressor(\n",
        "    base_estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,  # Number of trees in the ensemble\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = bagging_reg.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "\n",
        "23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores?\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importance scores\n",
        "feature_importances = rf_clf.feature_importances_\n",
        "feature_names = breast_cancer.feature_names\n",
        "\n",
        "# Sort feature importances in descending order\n",
        "sorted_indices = np.argsort(feature_importances)[::-1]\n",
        "\n",
        "print(\"Feature Importances:\")\n",
        "for idx in sorted_indices:\n",
        "    print(f\"{feature_names[idx]}: {feature_importances[idx]:.4f}\")\n",
        "\n",
        "23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores?\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importance scores\n",
        "feature_importances = rf_clf.feature_importances_\n",
        "feature_names = breast_cancer.feature_names\n",
        "\n",
        "# Sort feature importances in descending order\n",
        "sorted_indices = np.argsort(feature_importances)[::-1]\n",
        "\n",
        "print(\"Feature Importances:\")\n",
        "for idx in sorted_indices:\n",
        "    print(f\"{feature_names[idx]}: {feature_importances[idx]:.4f}\")\n",
        "\n",
        "24. Train a Random Forest Regressor and compare its performance with a single Decision Tree?\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"spotify_data.csv\")  # Update with actual file path\n",
        "\n",
        "# Select relevant features and target variable\n",
        "features = ['Duration (ms)', 'Popularity']  # Update with more relevant features\n",
        "target = 'Popularity'  # Assuming we predict track popularity\n",
        "\n",
        "# Drop rows with missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Define X (features) and y (target)\n",
        "X = df[features].drop(columns=['Popularity'])  # Features\n",
        "y = df[target]  # Target variable\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "dt_model = DecisionTreeRegressor(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "dt_preds = dt_model.predict(X_test)\n",
        "rf_preds = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate models\n",
        "def evaluate(y_true, y_pred, model_name):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    print(f\"{model_name} Performance:\")\n",
        "    print(f\"Mean Absolute Error: {mae:.2f}\")\n",
        "    print(f\"R² Score: {r2:.4f}\\n\")\n",
        "\n",
        "evaluate(y_test, dt_preds, \"Decision Tree Regressor\")\n",
        "evaluate(y_test, rf_preds, \"Random Forest Regressor\")\n",
        "\n",
        "25. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier?\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"spotify_data.csv\")  # Update with actual file path\n",
        "\n",
        "# Select relevant features and target variable\n",
        "features = ['Duration (ms)', 'Popularity']  # Update with more relevant features\n",
        "target = 'Popularity'  # Assuming we classify tracks into popular (1) or not (0)\n",
        "\n",
        "# Convert popularity into a binary classification (Example: threshold = 50)\n",
        "df['Popularity_Label'] = (df['Popularity'] >= 50).astype(int)\n",
        "\n",
        "# Drop rows with missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Define X (features) and y (target)\n",
        "X = df[features]\n",
        "y = df['Popularity_Label']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier with OOB enabled\n",
        "rf_model = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Compute OOB score\n",
        "oob_score = rf_model.oob_score_\n",
        "print(f\"Out-of-Bag (OOB) Score: {oob_score:.4f}\")\n",
        "\n",
        "# Evaluate on test data\n",
        "y_pred = rf_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "26. Train a Bagging Classifier using SVM as a base estimator and print accuracy?\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"spotify_data.csv\")  # Update with actual file path\n",
        "\n",
        "# Select relevant features and target variable\n",
        "features = ['Duration (ms)', 'Popularity']  # Update with more relevant features\n",
        "target = 'Popularity'  # Assuming we classify tracks into popular (1) or not (0)\n",
        "\n",
        "# Convert popularity into a binary classification (Example: threshold = 50)\n",
        "df['Popularity_Label'] = (df['Popularity'] >= 50).astype(int)\n",
        "\n",
        "# Drop rows with missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Define X (features) and y (target)\n",
        "X = df[features]\n",
        "y = df['Popularity_Label']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Bagging Classifier with SVM as base estimator\n",
        "base_svm = SVC(kernel='rbf', probability=True)  # Using RBF kernel with probability=True for Bagging\n",
        "bagging_model = BaggingClassifier(base_estimator=base_svm, n_estimators=10, random_state=42)\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = bagging_model.predict(X_test)\n",
        "\n",
        "# Compute Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bagging Classifier with SVM Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "27. Train a Random Forest Classifier with different numbers of trees and compare accuracy?\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"spotify_data.csv\")  # Update with actual file path\n",
        "\n",
        "# Select relevant features and target variable\n",
        "features = ['Duration (ms)', 'Popularity']  # Update with more relevant features\n",
        "target = 'Popularity'  # Assuming we classify tracks into popular (1) or not (0)\n",
        "\n",
        "# Convert popularity into a binary classification (Example: threshold = 50)\n",
        "df['Popularity_Label'] = (df['Popularity'] >= 50).astype(int)\n",
        "\n",
        "# Drop rows with missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Define X (features) and y (target)\n",
        "X = df[features]\n",
        "y = df['Popularity_Label']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# List of different numbers of trees to test\n",
        "n_trees = [10, 50, 100, 200, 500]\n",
        "accuracy_scores = []\n",
        "\n",
        "# Train and evaluate Random Forest with different numbers of trees\n",
        "for n in n_trees:\n",
        "    rf_model = RandomForestClassifier(n_estimators=n, random_state=42)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    y_pred = rf_model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_scores.append(accuracy)\n",
        "    print(f\"Random Forest with {n} trees - Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Plot accuracy vs number of trees\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(n_trees, accuracy_scores, marker='o', linestyle='-', color='b')\n",
        "plt.xlabel(\"Number of Trees (n_estimators)\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Random Forest Accuracy vs. Number of Trees\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        " 28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score?\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"spotify_data.csv\")  # Update with actual file path\n",
        "\n",
        "# Select relevant features and target variable\n",
        "features = ['Duration (ms)', 'Popularity']  # Update with more relevant features\n",
        "target = 'Popularity'  # Assuming we classify tracks into popular (1) or not (0)\n",
        "\n",
        "# Convert popularity into a binary classification (Example: threshold = 50)\n",
        "df['Popularity_Label'] = (df['Popularity'] >= 50).astype(int)\n",
        "\n",
        "# Drop rows with missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Define X (features) and y (target)\n",
        "X = df[features]\n",
        "y = df['Popularity_Label']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Bagging Classifier with Logistic Regression as base estimator\n",
        "base_lr = LogisticRegression(max_iter=1000)  # Ensuring convergence\n",
        "bagging_model = BaggingClassifier(base_estimator=base_lr, n_estimators=10, random_state=42)\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for AUC calculation\n",
        "y_prob = bagging_model.predict_proba(X_test)[:, 1]  # Get probability of class 1\n",
        "\n",
        "# Compute AUC score\n",
        "auc_score = roc_auc_score(y_test, y_prob)\n",
        "print(f\"Bagging Classifier with Logistic Regression - AUC Score: {auc_score:.4f}\")\n",
        "\n",
        "29. Train a Random Forest Regressor and analyze feature importance scores?\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"spotify_data.csv\")  # Update with actual file path\n",
        "\n",
        "# Select relevant features and target variable\n",
        "features = ['Duration (ms)', 'Popularity']  # Update with more relevant features\n",
        "target = 'Popularity'  # Predicting track popularity\n",
        "\n",
        "# Drop rows with missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Define X (features) and y (target)\n",
        "X = df[features].drop(columns=['Popularity'])  # Features\n",
        "y = df[target]  # Target variable\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Extract feature importance\n",
        "feature_importance = rf_model.feature_importances_\n",
        "feature_names = X.columns\n",
        "\n",
        "# Create a DataFrame for visualization\n",
        "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print feature importance scores\n",
        "print(\"Feature Importance Scores:\")\n",
        "print(importance_df)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.barh(importance_df['Feature'], importance_df['Importance'], color='royalblue')\n",
        "plt.xlabel(\"Importance Score\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(\"Feature Importance in Random Forest Regressor\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n",
        "\n",
        "30. Train an ensemble model using both Bagging and Random Forest and compare accuracy.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"spotify_data.csv\")  # Update with actual file path\n",
        "\n",
        "# Select relevant features and target variable\n",
        "features = ['Duration (ms)', 'Popularity']  # Update with more relevant features\n",
        "target = 'Popularity'  # Assuming we classify tracks into popular (1) or not (0)\n",
        "\n",
        "# Convert popularity into a binary classification (Example: threshold = 50)\n",
        "df['Popularity_Label'] = (df['Popularity'] >= 50).astype(int)\n",
        "\n",
        "# Drop rows with missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Define X (features) and y (target)\n",
        "X = df[features]\n",
        "y = df['Popularity_Label']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Classifier (Base Estimator: Decision Tree)\n",
        "bagging_model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_bagging = bagging_model.predict(X_test)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Compute Accuracy\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "\n",
        "# Print results\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy_bagging:.4f}\")\n",
        "print(f\"Random Forest Classifier Accuracy: {accuracy_rf:.4f}\")\n",
        "\n",
        "31.Train a Random Forest Classifier and tune hyperparameters using GridSearchCV.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"spotify_data.csv\")  # Update with actual file path\n",
        "\n",
        "# Select relevant features and target variable\n",
        "features = ['Duration (ms)', 'Popularity']  # Update with more relevant features\n",
        "target = 'Popularity'  # Assuming we classify tracks into popular (1) or not (0)\n",
        "\n",
        "# Convert popularity into a binary classification (Example: threshold = 50)\n",
        "df['Popularity_Label'] = (df['Popularity'] >= 50).astype(int)\n",
        "\n",
        "# Drop rows with missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Define X (features) and y (target)\n",
        "X = df[features]\n",
        "y = df['Popularity_Label']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define Random Forest model\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],         # Number of trees\n",
        "    'max_depth': [None, 10, 20],            # Maximum depth of trees\n",
        "    'min_samples_split': [2, 5, 10],        # Minimum samples to split a node\n",
        "    'min_samples_leaf': [1, 2, 4],          # Minimum samples in a leaf node\n",
        "    'bootstrap': [True, False]              # Whether to use bootstrap sampling\n",
        "}\n",
        "\n",
        "# Use GridSearchCV to find the best hyperparameters\n",
        "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid,\n",
        "                           cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model and hyperparameters\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = best_rf_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Best Random Forest Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "32. Train a Bagging Regressor with different numbers of base estimators and compare performance?\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"spotify_data.csv\")  # Update with actual file path\n",
        "\n",
        "# Select relevant features and target variable\n",
        "features = ['Duration (ms)', 'Popularity']  # Update with more relevant features\n",
        "target = 'Popularity'  # Predicting track popularity\n",
        "\n",
        "# Drop rows with missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Define X (features) and y (target)\n",
        "X = df[features].drop(columns=['Popularity'])  # Features\n",
        "y = df[target]  # Target variable\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# List of different numbers of base estimators to test\n",
        "n_estimators_list = [10, 50, 100, 200]\n",
        "mae_scores = []\n",
        "r2_scores = []\n",
        "\n",
        "# Train and evaluate Bagging Regressor with different estimators\n",
        "for n in n_estimators_list:\n",
        "    bagging_model = BaggingRegressor(base_estimator=DecisionTreeRegressor(),\n",
        "                                     n_estimators=n, random_state=42)\n",
        "    bagging_model.fit(X_train, y_train)\n",
        "    y_pred = bagging_model.predict(X_test)\n",
        "\n",
        "    # Compute metrics\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    mae_scores.append(mae)\n",
        "    r2_scores.append(r2)\n",
        "\n",
        "    print(f\"Bagging Regressor with {n} estimators - MAE: {mae:.4f}, R² Score: {r2:.4f}\")\n",
        "\n",
        "# Plot performance comparison\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(n_estimators_list, mae_scores, marker='o', linestyle='-', color='b', label=\"MAE\")\n",
        "plt.plot(n_estimators_list, r2_scores, marker='s', linestyle='-', color='r', label=\"R² Score\")\n",
        "plt.xlabel(\"Number of Base Estimators\")\n",
        "plt.ylabel(\"Performance Metric\")\n",
        "plt.title(\"Bagging Regressor Performance vs. Number of Base Estimators\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "33. Train a Random Forest Classifier and analyze misclassified samples?\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"spotify_data.csv\")  # Update with actual file path\n",
        "\n",
        "# Select relevant features and target variable\n",
        "features = ['Duration (ms)', 'Popularity']  # Update with more relevant features\n",
        "target = 'Popularity'  # Assuming we classify tracks into popular (1) or not (0)\n",
        "\n",
        "# Convert popularity into a binary classification (Example: threshold = 50)\n",
        "df['Popularity_Label'] = (df['Popularity'] >= 50).astype(int)\n",
        "\n",
        "# Drop rows with missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Define X (features) and y (target)\n",
        "X = df[features]\n",
        "y = df['Popularity_Label']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Compute Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Random Forest Classifier Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Identify misclassified samples\n",
        "misclassified_indices = np.where(y_pred != y_test)[0]\n",
        "misclassified_samples = X_test.iloc[misclassified_indices].copy()\n",
        "misclassified_samples['Actual'] = y_test.iloc[misclassified_indices].values\n",
        "misclassified_samples['Predicted'] = y_pred[misclassified_indices]\n",
        "\n",
        "# Print first few misclassified samples\n",
        "print(\"\\nMisclassified Samples:\")\n",
        "print(misclassified_samples.head())\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier?\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"spotify_data.csv\")  # Update with actual file path\n",
        "\n",
        "# Select relevant features and target variable\n",
        "features = ['Duration (ms)', 'Popularity']  # Update with more relevant features\n",
        "target = 'Popularity'  # Assuming we classify tracks into popular (1) or not (0)\n",
        "\n",
        "# Convert popularity into a binary classification (Example: threshold = 50)\n",
        "df['Popularity_Label'] = (df['Popularity'] >= 50).astype(int)\n",
        "\n",
        "# Drop rows with missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Define X (features) and y (target)\n",
        "X = df[features]\n",
        "y = df['Popularity_Label']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Classifier\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "y_pred_dt = dt_model.predict(X_test)\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "# Train Bagging Classifier (Base Estimator: Decision Tree)\n",
        "bagging_model = BaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
        "                                  n_estimators=100, random_state=42)\n",
        "bagging_model.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_model.predict(X_test)\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "\n",
        "# Print accuracy comparison\n",
        "print(f\"Decision Tree Accuracy: {accuracy_dt:.4f}\")\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy_bagging:.4f}\")\n",
        "\n",
        "35. Train a Random Forest Classifier and visualize the confusion matrix?\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"spotify_data.csv\")  # Update with actual file path\n",
        "\n",
        "# Select relevant features and target variable\n",
        "features = ['Duration (ms)', 'Popularity']  # Update with more relevant features\n",
        "target = 'Popularity'  # Assuming we classify tracks into popular (1) or not (0)\n",
        "\n",
        "# Convert popularity into a binary classification (Example: threshold = 50)\n",
        "df['Popularity_Label'] = (df['Popularity'] >= 50).astype(int)\n",
        "\n",
        "# Drop rows with missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Define X (features) and y (target)\n",
        "X = df[features]\n",
        "y = df['Popularity_Label']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Compute Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Random Forest Classifier Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Compute Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Not Popular\", \"Popular\"], yticklabels=[\"Not Popular\", \"Popular\"])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix - Random Forest Classifier\")\n",
        "plt.show()\n",
        "\n",
        "36. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy?\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"spotify_data.csv\")  # Update with actual file path\n",
        "\n",
        "# Select relevant features and target variable\n",
        "features = ['Duration (ms)', 'Popularity']  # Update with more relevant features\n",
        "target = 'Popularity'  # Assuming we classify tracks into popular (1) or not (0)\n",
        "\n",
        "# Convert popularity into a binary classification (Example: threshold = 50)\n",
        "df['Popularity_Label'] = (df['Popularity'] >= 50).astype(int)\n",
        "\n",
        "# Drop rows with missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Define X (features) and y (target)\n",
        "X = df[features]\n",
        "y = df['Popularity_Label']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define base learners\n",
        "base_learners = [\n",
        "    ('decision_tree', DecisionTreeClassifier(random_state=42)),\n",
        "    ('svm', SVC(probability=True, random_state=42)),\n",
        "    ('log_reg', LogisticRegression(max_iter=1000, random_state=42))\n",
        "]\n",
        "\n",
        "# Define Stacking Classifier (Meta-learner: Logistic Regression)\n",
        "stacking_model = StackingClassifier(estimators=base_learners, final_estimator=LogisticRegression())\n",
        "\n",
        "# Train Stacking Classifier\n",
        "stacking_model.fit(X_train, y_train)\n",
        "y_pred_stack = stacking_model.predict(X_test)\n",
        "accuracy_stack = accuracy_score(y_test, y_pred_stack)\n",
        "\n",
        "# Train individual models for comparison\n",
        "models = {\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "    \"SVM\": SVC(probability=True, random_state=42),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42)\n",
        "}\n",
        "\n",
        "accuracy_scores = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy_scores[name] = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy comparison\n",
        "print(\"\\nAccuracy Scores:\")\n",
        "for model_name, acc in accuracy_scores.items():\n",
        "    print(f\"{model_name}: {acc:.4f}\")\n",
        "\n",
        "print(f\"Stacking Classifier Accuracy: {accuracy_stack:.4f}\")\n",
        "\n",
        "37. Train a Random Forest Classifier and print the top 5 most important features?\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"spotify_data.csv\")  # Update with actual file path\n",
        "\n",
        "# Select relevant features and target variable\n",
        "features = ['Duration (ms)', 'Danceability', 'Energy', 'Loudness', 'Tempo', 'Popularity']  # Example features\n",
        "target = 'Popularity'  # Assuming we classify tracks into popular (1) or not (0)\n",
        "\n",
        "# Convert popularity into a binary classification (Example: threshold = 50)\n",
        "df['Popularity_Label'] = (df['Popularity'] >= 50).astype(int)\n",
        "\n",
        "# Drop rows with missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Define X (features) and y (target)\n",
        "X = df[features].drop(columns=['Popularity'])  # Exclude the original popularity column\n",
        "y = df['Popularity_Label']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Extract feature importance scores\n",
        "feature_importances = rf_model.feature_importances_\n",
        "\n",
        "# Create a DataFrame to store feature importance\n",
        "feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
        "\n",
        "# Sort features by importance (descending order)\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print top 5 most important features\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(feature_importance_df.head())\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.barh(feature_importance_df['Feature'].head(5), feature_importance_df['Importance'].head(5), color='b')\n",
        "plt.xlabel(\"Feature Importance Score\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(\"Top 5 Most Important Features - Random Forest\")\n",
        "plt.gca().invert_yaxis()  # Invert y-axis for better visualization\n",
        "plt.show()\n",
        "\n",
        "38. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score?\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"spotify_data.csv\")  # Update with actual file path\n",
        "\n",
        "# Select relevant features and target variable\n",
        "features = ['Duration (ms)', 'Danceability', 'Energy', 'Loudness', 'Tempo', 'Popularity']  # Example features\n",
        "target = 'Popularity'  # Assuming we classify tracks into popular (1) or not (0)\n",
        "\n",
        "# Convert popularity into a binary classification (Example: threshold = 50)\n",
        "df['Popularity_Label'] = (df['Popularity'] >= 50).astype(int)\n",
        "\n",
        "# Drop rows with missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Define X (features) and y (target)\n",
        "X = df[features].drop(columns=['Popularity'])  # Exclude the original popularity column\n",
        "y = df['Popularity_Label']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Classifier (Base Estimator: Decision Tree)\n",
        "bagging_model = BaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
        "                                  n_estimators=100, random_state=42)\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = bagging_model.predict(X_test)\n",
        "\n",
        "# Compute Evaluation Metrics\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print Performance Metrics\n",
        "print(f\"Bagging Classifier Performance:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "\n",
        "39. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy?\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"spotify_data.csv\")  # Update with actual file path\n",
        "\n",
        "# Select relevant features and target variable\n",
        "features = ['Duration (ms)', 'Danceability', 'Energy', 'Loudness', 'Tempo', 'Popularity']  # Example features\n",
        "target = 'Popularity'  # Assuming we classify tracks into popular (1) or not (0)\n",
        "\n",
        "# Convert popularity into a binary classification (Example: threshold = 50)\n",
        "df['Popularity_Label'] = (df['Popularity'] >= 50).astype(int)\n",
        "\n",
        "# Drop rows with missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Define X (features) and y (target)\n",
        "X = df[features].drop(columns=['Popularity'])  # Exclude the original popularity column\n",
        "y = df['Popularity_Label']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Experiment with different max_depth values\n",
        "max_depth_values = range(1, 21)  # Testing depths from 1 to 20\n",
        "accuracy_scores = []\n",
        "\n",
        "for depth in max_depth_values:\n",
        "    # Train Random Forest Classifier with current max_depth\n",
        "    rf_model = RandomForestClassifier(n_estimators=100, max_depth=depth, random_state=42)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on test data\n",
        "    y_pred = rf_model.predict(X_test)\n",
        "\n",
        "    # Compute accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_scores.append(accuracy)\n",
        "\n",
        "# Plot Accuracy vs. Max Depth\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(max_depth_values, accuracy_scores, marker='o', linestyle='-', color='b')\n",
        "plt.xlabel(\"Max Depth\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Effect of max_depth on Random Forest Accuracy\")\n",
        "plt.xticks(max_depth_values)\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare\n",
        "performance?\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"spotify_data.csv\")  # Update with actual file path\n",
        "\n",
        "# Select relevant features and target variable\n",
        "features = ['Danceability', 'Energy', 'Loudness', 'Tempo', 'Acousticness', 'Instrumentalness']\n",
        "target = 'Popularity'\n",
        "\n",
        "# Drop rows with missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Define X (features) and y (target)\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define base estimators\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "knn_regressor = KNeighborsRegressor()\n",
        "\n",
        "# Train Bagging Regressor with Decision Tree\n",
        "bagging_dt = BaggingRegressor(base_estimator=dt_regressor, n_estimators=50, random_state=42)\n",
        "bagging_dt.fit(X_train, y_train)\n",
        "y_pred_dt = bagging_dt.predict(X_test)\n",
        "\n",
        "# Train Bagging Regressor with KNeighbors\n",
        "bagging_knn = BaggingRegressor(base_estimator=knn_regressor, n_estimators=50, random_state=42)\n",
        "bagging_knn.fit(X_train, y_train)\n",
        "y_pred_knn = bagging_knn.predict(X_test)\n",
        "\n",
        "# Evaluate performance using MAE and R² Score\n",
        "mae_dt = mean_absolute_error(y_test, y_pred_dt)\n",
        "r2_dt = r2_score(y_test, y_pred_dt)\n",
        "\n",
        "mae_knn = mean_absolute_error(y_test, y_pred_knn)\n",
        "r2_knn = r2_score(y_test, y_pred_knn)\n",
        "\n",
        "# Print performance metrics\n",
        "print(\"Performance Comparison:\")\n",
        "print(f\"Bagging Regressor (Decision Tree) - MAE: {mae_dt:.4f}, R² Score: {r2_dt:.4f}\")\n",
        "print(f\"Bagging Regressor (KNeighbors) - MAE: {mae_knn:.4f}, R² Score: {r2_knn:.4f}\")\n",
        "\n",
        "# Visualize R² scores\n",
        "plt.bar([\"Decision Tree\", \"KNeighbors\"], [r2_dt, r2_knn], color=['blue', 'green'])\n",
        "plt.xlabel(\"Base Estimator\")\n",
        "plt.ylabel(\"R² Score\")\n",
        "plt.title(\"Comparison of Bagging Regressors\")\n",
        "plt.show()\n",
        "\n",
        "41. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score?\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"spotify_data.csv\")  # Update with actual file path\n",
        "\n",
        "# Select relevant features and target variable\n",
        "features = ['Duration (ms)', 'Danceability', 'Energy', 'Loudness', 'Tempo', 'Popularity']  # Example features\n",
        "target = 'Popularity'  # Assuming we classify tracks into popular (1) or not (0)\n",
        "\n",
        "# Convert popularity into a binary classification (Example: threshold = 50)\n",
        "df['Popularity_Label'] = (df['Popularity'] >= 50).astype(int)\n",
        "\n",
        "# Drop rows with missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Define X (features) and y (target)\n",
        "X = df[features].drop(columns=['Popularity'])  # Exclude the original popularity column\n",
        "y = df['Popularity_Label']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for ROC calculation\n",
        "y_probs = rf_model.predict_proba(X_test)[:, 1]  # Probability of class 1\n",
        "\n",
        "# Compute ROC-AUC Score\n",
        "roc_auc = roc_auc_score(y_test, y_probs)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Compute ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
        "\n",
        "# Plot ROC Curve\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(fpr, tpr, color='b', label=f\"ROC Curve (AUC = {roc_auc:.4f})\")\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve - Random Forest Classifier\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "42. Train a Bagging Classifier and evaluate its performance using cross-validatio.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"spotify_data.csv\")  # Update with actual file path\n",
        "\n",
        "# Select relevant features and target variable\n",
        "features = ['Duration (ms)', 'Danceability', 'Energy', 'Loudness', 'Tempo', 'Popularity']  # Example features\n",
        "target = 'Popularity'  # Assuming we classify tracks into popular (1) or not (0)\n",
        "\n",
        "# Convert popularity into a binary classification (Example: threshold = 50)\n",
        "df['Popularity_Label'] = (df['Popularity'] >= 50).astype(int)\n",
        "\n",
        "# Drop rows with missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Define X (features) and y (target)\n",
        "X = df[features].drop(columns=['Popularity'])  # Exclude the original popularity column\n",
        "y = df['Popularity_Label']\n",
        "\n",
        "# Initialize Bagging Classifier with Decision Tree as base estimator\n",
        "bagging_model = BaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
        "                                  n_estimators=100, random_state=42)\n",
        "\n",
        "# Perform 5-fold cross-validation\n",
        "cv_scores = cross_val_score(bagging_model, X, y, cv=5, scoring='accuracy')\n",
        "\n",
        "# Print mean accuracy and standard deviation\n",
        "print(f\"Cross-Validation Accuracy Scores: {cv_scores}\")\n",
        "print(f\"Mean Accuracy: {np.mean(cv_scores):.4f}\")\n",
        "print(f\"Standard Deviation: {np.std(cv_scores):.4f}\")\n",
        "\n",
        "43. Train a Random Forest Classifier and plot the Precision-Recall curv?\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve, auc, average_precision_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"spotify_data.csv\")  # Update with actual file path\n",
        "\n",
        "# Select relevant features and target variable\n",
        "features = ['Duration (ms)', 'Danceability', 'Energy', 'Loudness', 'Tempo', 'Popularity']  # Example features\n",
        "target = 'Popularity'  # Assuming we classify tracks into popular (1) or not (0)\n",
        "\n",
        "# Convert popularity into a binary classification (Example: threshold = 50)\n",
        "df['Popularity_Label'] = (df['Popularity'] >= 50).astype(int)\n",
        "\n",
        "# Drop rows with missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Define X (features) and y (target)\n",
        "X = df[features].drop(columns=['Popularity'])  # Exclude the original popularity column\n",
        "y = df['Popularity_Label']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for Precision-Recall calculation\n",
        "y_probs = rf_model.predict_proba(X_test)[:, 1]  # Probability of class 1\n",
        "\n",
        "# Compute Precision-Recall curve\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_probs)\n",
        "\n",
        "# Compute AUC for Precision-Recall Curve\n",
        "pr_auc = auc(recall, precision)\n",
        "avg_precision = average_precision_score(y_test, y_probs)\n",
        "\n",
        "# Plot Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(recall, precision, color='b', label=f\"PR Curve (AUC = {pr_auc:.4f})\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve - Random Forest Classifier\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Print Average Precision Score\n",
        "print(f\"Average Precision Score: {avg_precision:.4f}\")\n",
        "\n",
        "44. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy?\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"spotify_data.csv\")  # Update with actual file path\n",
        "\n",
        "# Select relevant features and target variable\n",
        "features = ['Duration (ms)', 'Danceability', 'Energy', 'Loudness', 'Tempo', 'Popularity']  # Example features\n",
        "target = 'Popularity'  # Assuming we classify tracks into popular (1) or not (0)\n",
        "\n",
        "# Convert popularity into a binary classification (Example: threshold = 50)\n",
        "df['Popularity_Label'] = (df['Popularity'] >= 50).astype(int)\n",
        "\n",
        "# Drop rows with missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Define X (features) and y (target)\n",
        "X = df[features].drop(columns=['Popularity'])  # Exclude the original popularity column\n",
        "y = df['Popularity_Label']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define base estimators\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Define Stacking Classifier with Logistic Regression as meta-classifier\n",
        "stacking_model = StackingClassifier(\n",
        "    estimators=[('rf', rf_model), ('lr', lr_model)],\n",
        "    final_estimator=LogisticRegression()\n",
        ")\n",
        "\n",
        "# Train models\n",
        "rf_model.fit(X_train, y_train)\n",
        "lr_model.fit(X_train, y_train)\n",
        "stacking_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "y_pred_lr = lr_model.predict(X_test)\n",
        "y_pred_stack = stacking_model.predict(X_test)\n",
        "\n",
        "# Compute accuracy\n",
        "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
        "acc_lr = accuracy_score(y_test, y_pred_lr)\n",
        "acc_stack = accuracy_score(y_test, y_pred_stack)\n",
        "\n",
        "# Print accuracy comparison\n",
        "print(f\"Random Forest Accuracy: {acc_rf:.4f}\")\n",
        "print(f\"Logistic Regression Accuracy: {acc_lr:.4f}\")\n",
        "print(f\"Stacking Classifier Accuracy: {acc_stack:.4f}\")\n",
        "\n",
        "45. Train a Bagging Regressor with different levels of bootstrap samples and compare performance.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"spotify_data.csv\")  # Update with actual file path\n",
        "\n",
        "# Select relevant features and target variable\n",
        "features = ['Danceability', 'Energy', 'Loudness', 'Tempo', 'Popularity']  # Example features\n",
        "target = 'Popularity'  # Regression task\n",
        "\n",
        "# Drop rows with missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Define X (features) and y (target)\n",
        "X = df[features].drop(columns=['Popularity'])  # Exclude the original popularity column\n",
        "y = df['Popularity']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define different bootstrap sample sizes\n",
        "bootstrap_sizes = [0.5, 0.7, 1.0]  # 50%, 70%, and 100% of training samples\n",
        "mse_scores = []\n",
        "\n",
        "# Train Bagging Regressor for different bootstrap sizes\n",
        "for size in bootstrap_sizes:\n",
        "    bagging_model = BaggingRegressor(\n",
        "        base_estimator=DecisionTreeRegressor(),\n",
        "        n_estimators=100,\n",
        "        max_samples=size,  # Varying bootstrap sample size\n",
        "        random_state=42\n",
        "    )\n",
        "    bagging_model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict and compute MSE\n",
        "    y_pred = bagging_model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_scores.append(mse)\n",
        "    print(f\"Bootstrap Sample Size {size*100:.0f}% - MSE: {mse:.4f}\")\n",
        "\n",
        "# Plot Bootstrap Sample Size vs. MSE\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(bootstrap_sizes, mse_scores, marker='o', linestyle='--', color='b')\n",
        "plt.xlabel(\"Bootstrap Sample Size\")\n",
        "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
        "plt.title(\"Effect of Bootstrap Sample Size on Bagging Regressor Performance\")\n",
        "plt.xticks(bootstrap_sizes)\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "O9aZfz18USMR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}