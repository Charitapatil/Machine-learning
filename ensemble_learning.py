# -*- coding: utf-8 -*-
"""Ensemble Learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S0Gw2xgiyDx1_E6DOXiWENLlGnIs5T6C
"""

Theoretical

1. Can we use Bagging for regression problems?
Yes, Bagging (Bootstrap Aggregating) can be used for regression problems! In fact, one of the most common examples of Bagging applied to regression is the Bagging Regressor, which is implemented in libraries like sklearn.

How Bagging Works in Regression
Bootstrapping: Multiple subsets of the training data are randomly sampled with replacement.
Base Learners: A regression model (like Decision Trees, SVR, or k-NN) is trained on each subset.
Aggregation: The final prediction is the average of all the individual models' predictions (instead of voting, which is used in classification).
Advantages of Bagging in Regression
Reduces variance: Helps smooth out fluctuations in predictions.
Improves stability: Works well with high-variance models like Decision Trees.
Handles overfitting: By averaging predictions, it avoids fitting too closely to any single dataset sample.
Common Bagging Regressor Example
In Python using sklearn:

python

from sklearn.ensemble import BaggingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import numpy as np

# Sample Data
X = np.random.rand(100, 5)
y = np.random.rand(100)

# Split Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Bagging with Decision Tree Regressor
bagging_reg = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=10, random_state=42)
bagging_reg.fit(X_train, y_train)

# Predictions
y_pred = bagging_reg.predict(X_test)

# Evaluate Model
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")
Alternative Ensemble Methods for Regression
Random Forest Regressor (uses Bagging with Decision Trees)
Boosting Methods like Gradient Boosting, AdaBoost, and XGBoost
Stacking & Blending for combining multiple models







2.What is the difference between multiple model training and single model training?


Difference Between Multiple Model Training and Single Model Training
Machine learning models can be trained in two main ways: Single Model Training and Multiple Model Training (Ensemble Learning). Each approach has its own advantages and use cases.

1. Single Model Training
This involves training one machine learning model on a dataset and using it to make predictions.

Characteristics:
A single algorithm (e.g., Decision Tree, Neural Network, SVM) is used to learn from the data.
The model optimizes its parameters to minimize error or maximize accuracy.
Computationally less expensive compared to multiple model training.
Simpler to interpret and deploy.
Advantages:
✅ Easy to implement and interpret
✅ Requires fewer computational resources
✅ Works well when the dataset is simple and well-structured

Disadvantages:
❌ Prone to overfitting (if the model is too complex) or underfitting (if the model is too simple)
❌ Less robust to noisy data
❌ Limited generalization power compared to ensemble models

2. Multiple Model Training (Ensemble Learning)
This involves training multiple models and combining their predictions to improve performance.

Types of Multiple Model Training:
Bagging (Bootstrap Aggregating)

Trains multiple models on different random subsets of the data.
Aggregates results (e.g., majority voting for classification, averaging for regression).
Example: Random Forest (bagging applied to Decision Trees).
Boosting

Models are trained sequentially, where each model learns from the errors of the previous one.
Focuses on hard-to-predict samples.
Examples: Gradient Boosting, AdaBoost, XGBoost, LightGBM.
Stacking

Combines predictions of multiple models using a meta-model.
Different base models are trained, and their predictions are used as inputs for another model.
Advantages of Multiple Model Training:
✅ Better Accuracy & Generalization: Reduces bias and variance
✅ More Robust to Overfitting: Since multiple models are used
✅ Handles Complex Patterns Better

Disadvantages:
❌ Computationally Expensive: Requires more processing power and memory
❌ Harder to Interpret: Since multiple models contribute to predictions
❌ More Complex to Tune and Deploy

Key Differences at a Glance
Feature	Single Model Training	Multiple Model Training (Ensemble)
Number of Models	One	Multiple
Performance	Can be good but limited	Generally higher accuracy
Risk of Overfitting	Higher for complex models	Lower due to averaging
Computational Cost	Lower	Higher
Interpretability	Easier	Harder
Examples	Decision Tree, SVM, Neural Networks	Random Forest, XGBoost, Stacking
When to Use Which Approach?
Use Single Model Training when:

The dataset is simple and small.
You need a fast, interpretable model.
You don’t have the computational resources for ensembles.
Use Multiple Model Training (Ensemble Learning) when:

The dataset is complex with high variance.
You need high accuracy and robustness.
Overfitting is a concern.
Computational resources are available.







3. Explain the concept of feature randomness in Random Forest?
Feature Randomness in Random Forest
Random Forest is an ensemble learning method that builds multiple decision trees and combines their predictions to improve accuracy and reduce overfitting. One of its key strengths is feature randomness, which enhances model diversity and robustness.

1. What is Feature Randomness?
Feature randomness means that instead of using all the features for splitting at each node, Random Forest selects a random subset of features at each split.

In a single Decision Tree, the best split is chosen based on all available features.
In Random Forest, each tree randomly selects a subset of features at every split to reduce correlation between trees.
How It Works:
Bootstrap Sampling: A random subset of data is drawn (with replacement) for each tree.
Random Feature Selection: At each split in a tree, only a random subset of features (not all features) is considered for finding the best split.
Independent Tree Growth: Each tree grows independently with different splits, leading to diverse decision boundaries.
Aggregation of Predictions: The final output is obtained by majority voting (classification) or averaging (regression).
2. Why is Feature Randomness Important?
✅ Reduces Overfitting
Without randomness, deep trees can memorize patterns, leading to overfitting.
Feature randomness ensures that no single feature dominates across all trees.
✅ Improves Generalization
Different trees learn different patterns, making the model more robust to unseen data.
✅ Increases Diversity Among Trees
If the same features are always chosen, trees become highly correlated.
Randomizing feature selection ensures that trees make different decisions, leading to better ensemble performance.
3. Feature Randomness Parameter in Random Forest
The number of features considered at each split is controlled by the max_features parameter in sklearn.

Common Settings for max_features:
max_features Value	Effect
"auto" (default) or "sqrt"	Uses √(total features) (default for classification).
"log2"	Uses log₂(total features).
None	Uses all features (no randomness, behaves like Bagging).
Integer value	Uses a fixed number of features per split.
4. Example in Python
python

from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris

# Load dataset
iris = load_iris()
X, y = iris.data, iris.target

# Random Forest with feature randomness
rf = RandomForestClassifier(n_estimators=10, max_features='sqrt', random_state=42)
rf.fit(X, y)

# Check feature importance
print(rf.feature_importances_)
5. Key Takeaways
Feature Randomness in Random Forest prevents trees from always selecting the most dominant features.
It decorrelates trees, reducing variance and improving generalization.
The max_features parameter controls how many features are considered at each split.
The default (sqrt for classification, log2 for regression) provides a good balance between accuracy and diversity.

4. What is OOB (Out-of-Bag) Score?
The Out-of-Bag (OOB) Score is a performance metric used in Random Forest models to estimate the generalization accuracy without needing a separate validation set. It is based on the idea of bootstrap aggregating (bagging).

How It Works:
In Random Forest, each tree is trained on a bootstrap sample (a random subset of the training data with replacement).
Some data points are left out (typically around 37% of the samples are not included in any given tree).
These out-of-bag (OOB) samples are then used to evaluate the model’s performance.
The OOB score is computed as the mean accuracy (or another relevant metric) of these OOB predictions across all trees.
Advantages of OOB Score:
No need for a separate validation set → Saves data for training.
Provides an unbiased estimate of the model’s performance.
Reduces overfitting risks, as it gives insight into how well the model generalizes.
When to Use:
OOB score is useful in classification and regression tasks when using Random Forest.
It works well when data is limited, since it acts as an internal cross-validation method.
Would you like an example of how to compute the OOB score in Python using sklearn?







	5. How can you measure the importance of features in a Random Forest model?
In a Random Forest model, you can measure feature importance using two main techniques:

1. Mean Decrease in Impurity (MDI) – Gini Importance
Also called "Feature Importance" in sklearn.
It measures how much a feature reduces impurity (e.g., Gini impurity or entropy in classification, variance in regression) across all trees.
Features that cause more splits and reduce impurity more are considered important.
Limitations: Can be biased toward features with many categories or high cardinality.
Implementation in Python:

python

from sklearn.ensemble import RandomForestClassifier
import pandas as pd

# Train a Random Forest model
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Get feature importance
feature_importances = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)
print(feature_importances)
2. Mean Decrease in Accuracy (MDA) – Permutation Importance
Also known as "Permutation Feature Importance".
Measures how much shuffling a feature’s values affects model performance (e.g., accuracy for classification, RMSE for regression).
More decrease in performance = more important the feature.
Advantage: More robust and works well with correlated features.
Implementation in Python:

python

from sklearn.inspection import permutation_importance

# Compute permutation importance
perm_importance = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=42)

# Convert to a Pandas Series
perm_importances = pd.Series(perm_importance.importances_mean, index=X_test.columns).sort_values(ascending=False)
print(perm_importances)
Comparison of Methods
Method	Pros	Cons
MDI (Gini Importance)	Fast, easy to compute	Biased towards features with more categories
MDA (Permutation Importance)	More reliable, works well with correlated features	Computationally expensive







6. Explain the working principle of a Bagging Classifier?
Bagging Classifier: Working Principle
A Bagging Classifier (short for Bootstrap Aggregating) is an ensemble learning method that improves the stability and accuracy of machine learning models, especially for high-variance models like decision trees. It reduces overfitting by averaging multiple predictions.

How Bagging Works
Bootstrap Sampling

The algorithm randomly selects multiple subsets of the training data with replacement.
Each subset is used to train a separate model (typically, weak learners like Decision Trees).
Model Training

Each base model (also called a weak learner) is trained on a different subset of data.
These models are independent of each other.
Aggregation (Averaging/Voting)

For classification, the final prediction is done via majority voting (the most common predicted class).
For regression, the final output is the average of all model predictions.
Advantages of Bagging
✅ Reduces Overfitting – Works well with high-variance models like Decision Trees.
✅ Improves Accuracy – By averaging multiple models, it creates a more robust prediction.
✅ Handles Noisy Data – Bootstrapping ensures each model sees different variations of the data.
✅ Parallelizable – Each model is independent, making bagging easy to parallelize.

Bagging Classifier in Python (sklearn)
python

from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Sample Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define Base Model (Weak Learner)
base_model = DecisionTreeClassifier()

# Bagging Classifier
bagging_clf = BaggingClassifier(base_estimator=base_model, n_estimators=100, random_state=42)
bagging_clf.fit(X_train, y_train)

# Predictions
y_pred = bagging_clf.predict(X_test)

# Accuracy
print("Bagging Classifier Accuracy:", accuracy_score(y_test, y_pred))
When to Use Bagging?
When you have a high-variance model (like Decision Trees).
When you want to reduce overfitting without increasing bias.
When you have enough computational resources to train multiple models in parallel.







7. How do you evaluate a Bagging Classifier’s performance?
Evaluating a Bagging Classifier’s Performance
To assess how well a Bagging Classifier performs, you can use various evaluation metrics and techniques, depending on whether you're solving a classification or regression problem.

1. Standard Evaluation Metrics
✅ For Classification:
Accuracy (if the classes are balanced)
Precision, Recall, F1-score (for imbalanced data)
ROC-AUC Score (for probabilistic outputs)
Example in Python:

python

from sklearn.metrics import accuracy_score, classification_report, roc_auc_score

# Predictions
y_pred = bagging_clf.predict(X_test)
y_pred_proba = bagging_clf.predict_proba(X_test)[:, 1]  # For ROC-AUC

# Accuracy
print("Accuracy:", accuracy_score(y_test, y_pred))

# Classification Report (Precision, Recall, F1-score)
print(classification_report(y_test, y_pred))

# ROC-AUC Score
print("ROC-AUC Score:", roc_auc_score(y_test, y_pred_proba))
✅ For Regression:
Mean Squared Error (MSE)
Mean Absolute Error (MAE)
R² Score (Coefficient of Determination)
Example in Python:

python

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Predictions
y_pred = bagging_regressor.predict(X_test)

# Regression Metrics
print("MSE:", mean_squared_error(y_test, y_pred))
print("MAE:", mean_absolute_error(y_test, y_pred))
print("R² Score:", r2_score(y_test, y_pred))
2. Cross-Validation
To get a more reliable performance estimate, use k-fold cross-validation:

python

from sklearn.model_selection import cross_val_score

# Perform 5-fold cross-validation
cv_scores = cross_val_score(bagging_clf, X, y, cv=5, scoring="accuracy")
print("Cross-Validation Accuracy:", cv_scores.mean())
3. Out-of-Bag (OOB) Score
Since Bagging uses bootstrap sampling, it leaves some data points out-of-bag (OOB), which can be used to evaluate performance without needing a separate validation set.

python

bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, oob_score=True, random_state=42)
bagging_clf.fit(X_train, y_train)

# OOB Score
print("OOB Score:", bagging_clf.oob_score_)
✅ Advantage: Saves data for training instead of using a separate validation set.

4. Bias-Variance Analysis
If the model has low training and test accuracy → High bias (underfitting).
If the model has high training accuracy but low test accuracy → High variance (overfitting).
Bagging usually reduces variance while maintaining low bias.







 8. How does a Bagging Regressor work?
Bagging Regressor: How It Works
A Bagging Regressor is an ensemble learning technique that improves regression model performance by reducing variance through bootstrap aggregating (bagging). It is particularly useful when using high-variance models like Decision Trees.

Working Principle
Bootstrap Sampling

The training dataset is randomly sampled with replacement to create multiple subsets.
Each subset has the same size as the original dataset but may contain duplicate instances.
Train Multiple Weak Learners

Each subset is used to train an independent base model (e.g., Decision Trees, SVMs, or other regressors).
The models are trained in parallel.
Aggregate Predictions (Averaging)

Each trained model makes a prediction on a test input.
The final prediction is obtained by averaging the outputs of all models:
𝑦
^
=
1
𝑁
∑
𝑖
=
1
𝑁
𝑦
^
𝑖
y
^
​
 =
N
1
​

i=1
∑
N
​

y
^
​

i
​

This reduces variance and improves stability compared to a single model.
Advantages of Bagging Regressor
✅ Reduces Overfitting – Especially useful for high-variance models like Decision Trees.
✅ More Stable Predictions – Averaging reduces fluctuations from individual models.
✅ Parallelizable – Each model is trained independently, making it efficient for parallel computing.

Bagging Regressor in Python (sklearn)
python

from sklearn.ensemble import BaggingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Load Data (Assuming X, y are defined)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define Base Regressor (Weak Learner)
base_regressor = DecisionTreeRegressor()

# Create Bagging Regressor
bagging_reg = BaggingRegressor(base_estimator=base_regressor, n_estimators=100, random_state=42)
bagging_reg.fit(X_train, y_train)

# Predictions
y_pred = bagging_reg.predict(X_test)

# Evaluate Performance
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)
Key Hyperparameters of BaggingRegressor
base_estimator: The weak learner (default is DecisionTreeRegressor()).
n_estimators: Number of models in the ensemble (higher = better generalization but more computation).
oob_score: Whether to use out-of-bag (OOB) samples for performance evaluation.
max_samples: Fraction of training samples used per model (default = 1.0).
max_features: Fraction of features used per model (default = 1.0).
When to Use Bagging Regressor?
✅ When you have high-variance models (e.g., Decision Trees).
✅ When you want better generalization compared to a single model.
✅ When you need a model that is robust to noise and outliers.








9. What is the main advantage of ensemble techniques?
Main Advantage of Ensemble Techniques
The primary advantage of ensemble techniques is that they combine multiple models to improve overall performance, making predictions more accurate, stable, and generalizable than individual models.

Key Benefits of Ensemble Techniques
✅ Higher Accuracy – Combining multiple models reduces individual weaknesses and leads to better performance.
✅ Reduced Overfitting – Methods like Bagging (e.g., Random Forest) reduce variance, while Boosting (e.g., XGBoost) reduces bias.
✅ Better Generalization – Ensembles perform well on unseen data, preventing over-reliance on training data patterns.
✅ Handles Noisy Data – Averaging or voting across models minimizes the impact of outliers and noise.
✅ Robustness – If one model fails or is weak, the ensemble can still perform well.
✅ Versatility – Can be applied to classification, regression, anomaly detection, and ranking tasks.

Comparison of Ensemble Methods
Ensemble Method	Reduces Variance?	Reduces Bias?	Example Algorithm
Bagging	✅ Yes	❌ No	Random Forest
Boosting	❌ No	✅ Yes	XGBoost, AdaBoost
Stacking	✅ Yes	✅ Yes	Stacked Generalization







10. What is the main challenge of ensemble methods?
Main Challenge of Ensemble Methods
While ensemble methods improve model performance, they also come with challenges. The biggest challenge is the increased computational complexity and interpretability compared to single models.

Key Challenges of Ensemble Methods
🚀 1. Computational Cost

Training multiple models (e.g., in Random Forest or Boosting) requires more processing power and memory.
Methods like Boosting (e.g., XGBoost, AdaBoost) are particularly slow for large datasets.
⚡ 2. Complexity & Interpretability

Ensembles (e.g., Stacking, Boosting) are harder to interpret than a single Decision Tree or Logistic Regression model.
Extracting feature importance and understanding decision-making is more complex.
🔄 3. Risk of Overfitting (Boosting)

Some ensemble techniques (like Boosting) can overfit if not properly regularized (e.g., too many weak learners).
Hyperparameter tuning is often required to prevent overfitting.
📊 4. Large Model Size

Models like Random Forest or Gradient Boosting can grow very large, making deployment and inference slower.
Not ideal for real-time applications where fast predictions are required.
🔢 5. Hyperparameter Tuning Complexity

Many hyperparameters (e.g., n_estimators, max_depth, learning_rate) must be tuned for optimal performance.
Techniques like Grid Search or Bayesian Optimization may be needed.
How to Mitigate These Challenges?
✅ Use feature selection to reduce dimensionality before applying ensembles.
✅ Optimize hyperparameters (e.g., tuning n_estimators to prevent overfitting).
✅ Use simpler base models if computational cost is a concern.
✅ Apply model compression techniques (e.g., pruning, quantization) for deployment.








11. Explain the key idea behind ensemble techniques?
Key Idea Behind Ensemble Techniques
The core idea behind ensemble techniques is to combine multiple models to create a stronger and more accurate predictive model than any single model alone.

Instead of relying on one weak learner, ensemble methods aggregate predictions from multiple models to reduce errors, improve generalization, and increase stability.

Why Does Ensemble Learning Work?
✅ Diversity → Different models make different mistakes, and combining them balances out individual errors.
✅ Reduction in Overfitting → Averaging predictions (Bagging) or sequential learning (Boosting) helps models generalize better.
✅ Higher Accuracy → Aggregating results reduces noise and improves overall performance.

Types of Ensemble Techniques & Their Key Ideas
Ensemble Method	Key Idea	Example Algorithms
Bagging (Bootstrap Aggregating)	Train multiple models on random subsets of data and combine predictions (reduces variance).	Random Forest, BaggingClassifier
Boosting	Train models sequentially, where each new model corrects errors of the previous one (reduces bias).	AdaBoost, XGBoost, Gradient Boosting
Stacking	Train multiple models and use a meta-model to learn from their predictions.	Stacked Generalization
Voting (Hard/Soft Voting)	Combine predictions from different models using majority voting (classification) or averaging (regression).	VotingClassifier
Analogy: "Wisdom of the Crowd"
Think of a quiz show where you can "Ask the Audience."

Instead of relying on one person’s answer, the audience collectively provides the best response.
Bagging is like averaging everyone's answers.
Boosting is like refining answers based on past mistakes.



12. What is a Random Forest Classifier?
Random Forest Classifier: Overview
A Random Forest Classifier is an ensemble learning method that builds multiple Decision Trees and combines their predictions to improve accuracy, reduce overfitting, and enhance generalization.

It is based on Bagging (Bootstrap Aggregating) and introduces additional randomness by selecting a random subset of features for each tree.

How Does a Random Forest Classifier Work?
1️⃣ Bootstrap Sampling (Bagging)

The training data is randomly sampled with replacement to create multiple subsets.
Each subset is used to train an independent Decision Tree.
2️⃣ Feature Randomness (Feature Subset Selection)

At each split, a random subset of features is selected instead of using all features.
This reduces correlation between trees and improves diversity.
3️⃣ Majority Voting (Aggregation)

Each tree makes a prediction for a test sample.
The final prediction is based on majority voting (classification) or averaging (regression).
Advantages of Random Forest Classifier
✅ Higher Accuracy – Reduces overfitting compared to a single Decision Tree.
✅ Handles Missing Data – Can handle incomplete datasets effectively.
✅ Works Well with High-Dimensional Data – Performs well even with many features.
✅ Resistant to Overfitting – Due to feature randomness and bagging.
✅ Robust to Noise and Outliers – Reduces variance in predictions.

Random Forest Classifier in Python (sklearn)
python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load Data (Assuming X, y are defined)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize Random Forest Classifier
rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)
rf_clf.fit(X_train, y_train)

# Predictions
y_pred = rf_clf.predict(X_test)

# Evaluate Accuracy
print("Random Forest Accuracy:", accuracy_score(y_test, y_pred))
Key Hyperparameters in Random Forest
Hyperparameter	Description
n_estimators	Number of Decision Trees (default: 100)
max_depth	Maximum depth of trees (prevents overfitting)
max_features	Number of features considered at each split
oob_score	Uses Out-of-Bag samples for evaluation
n_jobs	Number of CPU cores used for parallel processing







13. What are the main types of ensemble techniques?
Main Types of Ensemble Techniques
Ensemble learning methods combine multiple models to improve accuracy, reduce overfitting, and enhance generalization. The main types of ensemble techniques are:

1️⃣ Bagging (Bootstrap Aggregating)
📌 Key Idea: Train multiple independent models on random subsets of data and combine their predictions (reduces variance).
📌 Best for: High-variance models (e.g., Decision Trees).
📌 Example Algorithms:

Random Forest 🌳 (Multiple Decision Trees)
Bagging Classifier (Any base model)
🔹 How It Works:

Bootstraps (random subsets) are created from the dataset.
Each subset is used to train a weak learner (e.g., Decision Trees).
The final prediction is obtained via majority voting (classification) or averaging (regression).
python

from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)
2️⃣ Boosting
📌 Key Idea: Train models sequentially, where each new model corrects the errors of the previous ones (reduces bias).
📌 Best for: High-bias models (e.g., weak learners like shallow Decision Trees).
📌 Example Algorithms:

AdaBoost (Adaptive Boosting) 🔥
Gradient Boosting (GBM, XGBoost, LightGBM, CatBoost)
🔹 How It Works:

The first model is trained on the original dataset.
The next model focuses on misclassified samples by assigning higher weights to them.
The process continues iteratively, improving performance.
python

from sklearn.ensemble import AdaBoostClassifier

adaboost_clf = AdaBoostClassifier(n_estimators=50, random_state=42)
3️⃣ Stacking (Stacked Generalization)
📌 Key Idea: Train multiple different models and use a meta-model to combine their outputs.
📌 Best for: When diverse models (e.g., Logistic Regression + Decision Trees + SVM) perform well in different areas.
📌 Example Algorithm:

StackingClassifier (combines different models into one)
🔹 How It Works:

Multiple base models (e.g., Decision Trees, SVM, Neural Networks) are trained.
Their predictions become the input for a meta-model.
The meta-model learns how to best combine the base models’ outputs.
python

from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC

stacking_clf = StackingClassifier(
    estimators=[
        ('dt', DecisionTreeClassifier()),
        ('svm', SVC(probability=True))
    ],
    final_estimator=LogisticRegression()
)
4️⃣ Voting (Majority Voting)
📌 Key Idea: Combine predictions from multiple models using majority voting (classification) or averaging (regression).
📌 Best for: When different models perform well on different aspects of the problem.
📌 Example Algorithm:

VotingClassifier (for classification)
🔹 Types of Voting:

Hard Voting → Chooses the class with the most votes.
Soft Voting → Averages predicted probabilities (better when models provide probability outputs).
python

from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC

voting_clf = VotingClassifier(
    estimators=[
        ('lr', LogisticRegression()),
        ('dt', DecisionTreeClassifier()),
        ('svm', SVC(probability=True))
    ],
    voting='soft'  # Use 'hard' for majority voting
)
Comparison of Ensemble Techniques
Method	Reduces Variance?	Reduces Bias?	Examples
Bagging	✅ Yes	❌ No	Random Forest, BaggingClassifier
Boosting	❌ No	✅ Yes	AdaBoost, XGBoost, LightGBM
Stacking	✅ Yes	✅ Yes	StackingClassifier
Voting	✅ Yes	❌ No	VotingClassifier
Which Ensemble Method Should You Use?
✅ If your model overfits (high variance) → Use Bagging (e.g., Random Forest).
✅ If your model underfits (high bias) → Use Boosting (e.g., XGBoost).
✅ If you have diverse models → Use Stacking or Voting.








14. What is ensemble learning in machine learning?
Ensemble Learning in Machine Learning
📌 Definition:
Ensemble learning is a technique that combines multiple models (weak or strong learners) to produce a better predictive model than any individual model alone. The goal is to improve accuracy, stability, and generalization by reducing variance, bias, or both.

Why Use Ensemble Learning?
✅ Higher Accuracy → Combining models reduces individual weaknesses.
✅ Reduces Overfitting → Techniques like Bagging stabilize predictions.
✅ Reduces Underfitting → Techniques like Boosting refine weak models.
✅ Handles Noisy Data → Aggregating predictions minimizes the effect of outliers.
✅ Works with Any Model → Can combine Decision Trees, Neural Networks, SVMs, etc.

Types of Ensemble Learning
1️⃣ Bagging (Bootstrap Aggregating)

Goal: Reduces variance by training multiple models on random subsets of data.
Example: Random Forest (combining Decision Trees).
2️⃣ Boosting

Goal: Reduces bias by sequentially training models where each focuses on correcting previous mistakes.
Example: XGBoost, AdaBoost, Gradient Boosting.
3️⃣ Stacking (Stacked Generalization)

Goal: Combines multiple diverse models and trains a meta-model to learn from their outputs.
Example: StackingClassifier (combining SVMs, Decision Trees, etc.).
4️⃣ Voting

Goal: Aggregates predictions from multiple models using majority voting (classification) or averaging (regression).
Example: VotingClassifier.
Analogy: "Wisdom of the Crowd" 🤝
Think of ensemble learning like a panel of experts making a decision together:

Bagging → Ask multiple experts independently and take the average opinion.
Boosting → Ask one expert, let another improve their response, and repeat.
Stacking → Combine different expert opinions and let a final judge decide.
Python Example: Random Forest (Bagging)
python

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load Data (Assuming X, y are defined)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Random Forest Classifier
rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)
rf_clf.fit(X_train, y_train)

# Predictions & Evaluation
y_pred = rf_clf.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
When Should You Use Ensemble Learning?
✅ If a single model is not performing well.
✅ If you want to reduce overfitting (high variance models) → Use Bagging.
✅ If you want to reduce bias (weak models) → Use Boosting.
✅ If you have diverse models that perform well in different areas → Use Stacking or Voting.








15. When should we avoid using ensemble methods?
When to Avoid Using Ensemble Methods?
Although ensemble methods improve accuracy and generalization, they are not always the best choice. You should avoid using them in the following scenarios:

🚀 1. When Interpretability is Important
📌 Why?

Ensembles like Random Forest, XGBoost, and Stacking create black-box models, making it hard to explain predictions.
If model explainability is required (e.g., in healthcare, finance, legal applications), a simpler model like Logistic Regression or Decision Trees is better.
✅ Use Instead: Decision Trees, Logistic Regression, Rule-Based Models.

⏳ 2. When Computational Cost is Too High
📌 Why?

Training hundreds or thousands of models (e.g., in Random Forest or Gradient Boosting) requires significant computing power and memory.
Boosting models like XGBoost or LightGBM can be slow for large datasets.
✅ Use Instead: A single Decision Tree, SVM, or simpler models.

⚡ 3. When Real-Time Predictions are Needed
📌 Why?

Ensemble models are slower during inference, especially in real-time applications like fraud detection, recommendation systems, or self-driving cars.
A complex ensemble (like Stacking) might cause latency issues in production.
✅ Use Instead:

Lightweight models like Logistic Regression or Naïve Bayes.
If using ensembles, prune the model or use model distillation.
📊 4. When Data is Small or Clean
📌 Why?

If the dataset is small and noise-free, a single, well-tuned model may perform just as well.
Bagging-based ensembles (like Random Forest) work best with high variance models; on small datasets, they may overfit.
✅ Use Instead: A single Decision Tree, SVM, or even k-NN.

🔄 5. When a Single Strong Model is Sufficient
📌 Why?

If a single model performs well (e.g., a well-regularized Logistic Regression or a fine-tuned Neural Network), adding ensembles may not bring significant improvements.
Ensembles shine when individual models struggle.
✅ Use Instead: Fine-tune hyperparameters of a single model.

🚫 6. When Overfitting is a Concern (Boosting Models)
📌 Why?

Boosting methods (e.g., AdaBoost, XGBoost) focus on hard-to-classify samples, which can lead to overfitting, especially on noisy datasets.
✅ Use Instead: Bagging-based methods (Random Forest) or simpler models.

💡 Conclusion: When NOT to Use Ensembles
Scenario	Why Avoid Ensembles?	Better Alternative
Need interpretability	Ensembles are black-box models	Decision Trees, Logistic Regression
Limited computation	Training takes too long	Single Decision Tree, SVM
Real-time predictions	Inference is slow	Logistic Regression, LightGBM (optimized)
Small dataset	May not generalize well	Single model, k-NN
A single model performs well	Ensembles add unnecessary complexity	Fine-tune the single model
Noisy data	Boosting may overfit	Bagging or Regularization







16. How does Bagging help in reducing overfitting?
How Does Bagging Reduce Overfitting?
📌 Bagging (Bootstrap Aggregating) reduces overfitting by combining multiple independent models trained on random subsets of data. By averaging predictions (or majority voting), it smooths out noise, reducing variance while maintaining accuracy.

🔹 How Bagging Works (Step-by-Step)
1️⃣ Bootstrap Sampling (Data Randomization)

Bagging creates multiple random subsets of the original dataset (with replacement).
Each subset is used to train a separate model (e.g., Decision Trees in Random Forest).
2️⃣ Independent Model Training

Each model is trained separately on its subset, leading to diversity in learning.
Since models see different data samples, they learn different patterns, preventing overfitting to a specific subset.
3️⃣ Aggregation of Predictions

For classification → Uses majority voting (the most common class wins).
For regression → Uses averaging (reduces extreme predictions).
This aggregation reduces model variance, preventing overfitting to noisy patterns in the data.
🔹 Why Does Bagging Reduce Overfitting?
✅ Reduces Variance → By averaging over multiple models, extreme predictions are canceled out, leading to a more stable model.
✅ Prevents Learning Noise → Individual models may overfit noisy samples, but the combined model smooths out these errors.
✅ Creates Diversity → Different models learn different aspects of the data, making the ensemble more robust.
✅ Resistant to Outliers → Outliers affect individual models, but their impact is minimized in aggregation.

🔹 Example: Bagging Classifier in Python
python

from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load Data (Assuming X, y are defined)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Bagging Classifier (with Decision Tree as base model)
bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)
bagging_clf.fit(X_train, y_train)

# Predictions & Evaluation
y_pred = bagging_clf.predict(X_test)
print("Bagging Accuracy:", accuracy_score(y_test, y_pred))
🔹 Bagging vs. Boosting: When to Use?
Method	Overfitting Risk?	Best When...
Bagging	Low (reduces variance) ✅	The model is overfitting (high variance).
Boosting	High (can overfit) ⚠️	The model is underfitting (high bias).







17. Why is Random Forest better than a single Decision Tree?
Why is Random Forest Better Than a Single Decision Tree?
📌 Random Forest 🌳 is an ensemble method that combines multiple Decision Trees to produce a more accurate, stable, and generalized model. It improves upon a single Decision Tree by reducing overfitting, increasing robustness, and handling noise better.

🔹 Key Advantages of Random Forest Over a Decision Tree
Feature	Random Forest 🌳	Single Decision Tree 🌲
Overfitting	🚀 Reduces overfitting by averaging multiple trees ✅	⚠️ Prone to overfitting (deep trees memorize data) ❌
Accuracy	🔥 Higher accuracy (reduces variance)	Lower accuracy, especially on unseen data
Stability	✅ Stable (small changes in data don’t affect much)	❌ Unstable (small data changes → completely different tree)
Bias-Variance Tradeoff	✅ Lower variance (ensemble effect)	❌ High variance (sensitive to noise)
Feature Importance	✅ Provides feature importance insights	Limited interpretability
Resistant to Outliers	✅ Less sensitive to outliers	❌ Highly sensitive to outliers
Handles Large Datasets	✅ Scales well for high-dimensional data	❌ Can become slow for large datasets
🔹 Why Does Random Forest Perform Better?
✅ 1. Uses Bagging to Reduce Overfitting

A single Decision Tree memorizes data (high variance).
Random Forest averages multiple trees, making it more generalized.
✅ 2. Random Feature Selection Improves Diversity

Each Decision Tree in Random Forest uses a random subset of features, leading to decorrelated trees that don’t make the same mistakes.
This prevents a dominant feature from overpowering the learning process.
✅ 3. More Robust to Noise and Outliers

Decision Trees split based on training data, making them sensitive to noise.
Random Forest averages predictions, dampening the effect of outliers.
✅ 4. Better Generalization on Unseen Data

A single Decision Tree can perfectly fit training data but fails on test data.
Random Forest maintains high accuracy on test data by reducing variance.
🔹 Example: Random Forest vs. Decision Tree in Python
python

from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load Data (Assuming X, y are defined)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Single Decision Tree
tree_clf = DecisionTreeClassifier(random_state=42)
tree_clf.fit(X_train, y_train)
tree_pred = tree_clf.predict(X_test)
print("Decision Tree Accuracy:", accuracy_score(y_test, tree_pred))

# Random Forest
rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)
rf_clf.fit(X_train, y_train)
rf_pred = rf_clf.predict(X_test)
print("Random Forest Accuracy:", accuracy_score(y_test, rf_pred))
💡 Expected Result:
🚀 Random Forest Accuracy > Decision Tree Accuracy, proving its ability to generalize better.

🔹 When Should You Use a Decision Tree Instead?
❌ If interpretability is required → Decision Trees are easier to explain.
❌ If computation time is limited → Training Random Forest is slower.

However, in most cases, Random Forest is preferred for higher accuracy and stability.








18. What is the role of bootstrap sampling in Bagging?
Role of Bootstrap Sampling in Bagging
📌 Bootstrap sampling is a key component of Bagging (Bootstrap Aggregating). It helps create diverse training sets by randomly sampling data with replacement, making the ensemble more robust and reducing overfitting.

🔹 How Does Bootstrap Sampling Work?
1️⃣ Random Sampling with Replacement

Given a dataset of N samples, Bagging randomly selects N samples with replacement to train each base model.
Some samples may be repeated, while others may be left out.
2️⃣ Train Multiple Base Models on Different Subsets

Each model (e.g., Decision Trees in a Random Forest) is trained on a different bootstrapped dataset.
Since models see different data distributions, they learn different patterns.
3️⃣ Aggregate Predictions

For classification → Uses majority voting (most common prediction wins).
For regression → Uses averaging (reduces extreme predictions).
🔹 Why is Bootstrap Sampling Important?
✅ 1. Introduces Diversity → Different models see different data, preventing overfitting to a single pattern.
✅ 2. Reduces Overfitting → Individual models overfit less because their predictions are averaged.
✅ 3. Stabilizes Predictions → Reduces variance by ensuring no single model dominates the ensemble.
✅ 4. Improves Generalization → Helps the model perform better on unseen data.
✅ 5. Creates Out-of-Bag (OOB) Samples → The left-out samples can be used for internal validation (like a built-in cross-validation).

🔹 Example: Bootstrap Sampling in Python
python

import numpy as np

# Original dataset (10 samples)
data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])

# Bootstrap sampling (random selection with replacement)
np.random.seed(42)
bootstrap_sample = np.random.choice(data, size=len(data), replace=True)

print("Original Data:", data)
print("Bootstrap Sample:", bootstrap_sample)
Output Example:


Original Data: [1 2 3 4 5 6 7 8 9 10]
Bootstrap Sample: [7 4 8 5 7 9 1 8 9 9]
👉 Notice that some values repeat, while others are missing.

🔹 Bootstrap Sampling in Random Forest
python

from sklearn.ensemble import RandomForestClassifier

# Train a Random Forest Classifier (automatically uses Bootstrap Sampling)
rf_clf = RandomForestClassifier(n_estimators=100, bootstrap=True, random_state=42)
rf_clf.fit(X_train, y_train)
🔹 Summary: Why Bootstrap Sampling?
Advantage	Why It Matters?
Diversity	Each model sees a different subset of data.
Overfitting Reduction	Models don’t memorize data; they generalize better.
Bias-Variance Tradeoff	Reduces variance, making predictions more stable.
Internal Validation	OOB score estimates performance without extra validation data.








19. What are some real-world applications of ensemble techniques?
Real-World Applications of Ensemble Techniques
Ensemble techniques are widely used across various industries because they boost accuracy, reduce overfitting, and enhance model stability. Below are some key applications:

1️⃣ Fraud Detection (Finance & Banking) 💰
Problem: Identifying fraudulent transactions in real-time.
Ensemble Solution: Random Forest + Gradient Boosting (XGBoost, LightGBM)
Why?
✅ Detects fraud patterns in noisy data.
✅ Combines weak models to improve anomaly detection.
Example: Credit card fraud detection in banks like Visa, MasterCard, and PayPal.
2️⃣ Recommendation Systems (E-commerce & Entertainment) 🎥🛒
Problem: Suggesting relevant products or content to users.
Ensemble Solution: Hybrid models (Collaborative Filtering + XGBoost)
Why?
✅ Improves personalization by blending multiple models.
✅ Reduces bias from a single recommendation model.
Example:
Netflix, Spotify → Movie & music recommendations.
Amazon, eBay → Personalized shopping recommendations.
3️⃣ Medical Diagnosis & Disease Prediction 🏥
Problem: Diagnosing diseases using medical records & imaging.
Ensemble Solution: Random Forest + Neural Networks + Boosting
Why?
✅ Higher accuracy than single models.
✅ Reduces false positives in medical tests.
Example:
Cancer detection (using MRI scans).
COVID-19 detection (X-ray + CT scans).
Diabetes risk prediction (based on patient history).
4️⃣ Stock Market Prediction 📈
Problem: Forecasting stock prices & financial trends.
Ensemble Solution: Stacking (LSTMs + Random Forest + XGBoost)
Why?
✅ Reduces the risk of overfitting to volatile markets.
✅ Improves accuracy by combining multiple models.
Example: Hedge funds, financial institutions, and algorithmic trading firms.
5️⃣ Sentiment Analysis & NLP (Social Media & Customer Feedback) 📝
Problem: Understanding emotions & opinions from text (tweets, reviews, etc.).
Ensemble Solution: Stacking (BERT + XGBoost + Random Forest)
Why?
✅ Captures complex patterns in text data.
✅ Enhances accuracy in spam detection & hate speech filtering.
Example:
Twitter & Facebook → Hate speech & misinformation detection.
Amazon & Yelp → Product review sentiment analysis.
6️⃣ Autonomous Vehicles (Self-Driving Cars) 🚗
Problem: Real-time object detection & decision-making in autonomous driving.
Ensemble Solution: Boosting (XGBoost) + Deep Learning (CNNs)
Why?
✅ Improves image recognition for pedestrians, signs, and vehicles.
✅ Reduces false positives in obstacle detection.
Example: Tesla Autopilot, Waymo (Google’s self-driving project).
7️⃣ Cybersecurity & Intrusion Detection 🔒
Problem: Detecting and preventing cyber-attacks & malware threats.
Ensemble Solution: Bagging (Random Forest) + Boosting (XGBoost, AdaBoost)
Why?
✅ Detects anomalies in network traffic & system logs.
✅ Reduces false alarms in intrusion detection systems.
Example: SIEM systems in large corporations (e.g., IBM, Splunk).
🔹 Summary: Why Use Ensemble Learning?
Industry	Application	Ensemble Methods Used
Finance & Banking	Fraud detection	Random Forest, XGBoost
E-commerce	Recommendation Systems	Collaborative Filtering + Boosting
Healthcare	Disease diagnosis	Random Forest + Neural Networks
Stock Market	Price prediction	Stacking (LSTMs + XGBoost)
Social Media	Sentiment analysis	NLP Models + Boosting
Autonomous Vehicles	Self-driving car vision	CNNs + Boosting
Cybersecurity	Intrusion detection	Random Forest + Boosting








20. What is the difference between Bagging and Boosting?

Bagging vs. Boosting: Key Differences
Both Bagging and Boosting are ensemble learning techniques that combine multiple models to improve performance, but they work in different ways.

Feature	Bagging 🌳	Boosting 🔥
Objective	Reduce variance (overfitting) ✅	Reduce bias (underfitting) ✅
How It Works?	Trains models independently on random subsets (bootstrap sampling)	Trains models sequentially, correcting previous mistakes
Model Training	Parallel (all models train at the same time) ⚡	Sequential (each model learns from previous errors) 🔄
Data Sampling	Bootstrap Sampling (with replacement)	Uses the entire dataset, but reweights misclassified samples
Base Model Type	High-variance models (e.g., Decision Trees) 🌲	Weak learners (e.g., Shallow Trees) 🌱
Final Prediction	Majority Voting (classification) or Averaging (regression)	Weighted combination of all models
Reduces Overfitting?	✅ Yes, by averaging multiple models	❌ Can overfit if too many models are used
Computation Time	Faster (models train in parallel) ⚡	Slower (models train sequentially) 🕒
Best For	High variance models (overfitting)	High bias models (underfitting)
Example Algorithms	Random Forest, Bagging Classifier	AdaBoost, Gradient Boosting, XGBoost
🔹 How They Work: Example
1️⃣ Bagging (Bootstrap Aggregating)
Trains multiple models (e.g., Decision Trees) in parallel.
Each model sees a random subset of data (bootstrap sampling).
Final prediction:
Classification → Majority voting 🗳️
Regression → Averaging 📊
Example Algorithm: Random Forest
python

from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)
bagging.fit(X_train, y_train)
2️⃣ Boosting (Sequential Learning)
Models train sequentially, learning from previous mistakes.
Misclassified samples get higher weights in the next iteration.
Final prediction: Weighted sum of weak learners.
Example Algorithm: AdaBoost, Gradient Boosting, XGBoost
python

from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

boosting = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)
boosting.fit(X_train, y_train)
🔹 When to Use Bagging vs. Boosting?
Scenario	Use Bagging 🌳	Use Boosting 🔥
Overfitting (High Variance)?	✅ Reduces overfitting	❌ Can overfit if not tuned properly
Underfitting (High Bias)?	❌ Not ideal	✅ Improves weak models
Fast Training Needed?	✅ Parallel processing	❌ Slower (sequential training)
Small Data Size?	❌ Needs large data	✅ Works well with small datasets
Interpretability?	✅ Easy to understand	❌ Harder to interpret
🔹 Summary: Key Takeaways
Bagging → Reduces variance, best for unstable models (e.g., Decision Trees).
Boosting → Reduces bias, best for weak models (e.g., shallow trees, logistic regression).
Bagging = Parallel, Boosting = Sequential.
Bagging is safer (less overfitting), while Boosting is more powerful but needs tuning.


















Practical
21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy?
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Load dataset
iris = load_iris()
X, y = iris.data, iris.target

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a Bagging Classifier with Decision Trees
bagging_clf = BaggingClassifier(
    base_estimator=DecisionTreeClassifier(),
    n_estimators=50,  # Number of trees in the ensemble
    random_state=42
)

# Train the model
bagging_clf.fit(X_train, y_train)

# Make predictions
y_pred = bagging_clf.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy:.2f}")

22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)?
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.ensemble import BaggingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error

# Load dataset
diabetes = load_diabetes()
X, y = diabetes.data, diabetes.target

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a Bagging Regressor with Decision Trees
bagging_reg = BaggingRegressor(
    base_estimator=DecisionTreeRegressor(),
    n_estimators=50,  # Number of trees in the ensemble
    random_state=42
)

# Train the model
bagging_reg.fit(X_train, y_train)

# Make predictions
y_pred = bagging_reg.predict(X_test)

# Calculate Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse:.2f}")

23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores?
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
import numpy as np

# Load dataset
breast_cancer = load_breast_cancer()
X, y = breast_cancer.data, breast_cancer.target

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a Random Forest Classifier
rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
rf_clf.fit(X_train, y_train)

# Print feature importance scores
feature_importances = rf_clf.feature_importances_
feature_names = breast_cancer.feature_names

# Sort feature importances in descending order
sorted_indices = np.argsort(feature_importances)[::-1]

print("Feature Importances:")
for idx in sorted_indices:
    print(f"{feature_names[idx]}: {feature_importances[idx]:.4f}")

23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores?
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
import numpy as np

# Load dataset
breast_cancer = load_breast_cancer()
X, y = breast_cancer.data, breast_cancer.target

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a Random Forest Classifier
rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
rf_clf.fit(X_train, y_train)

# Print feature importance scores
feature_importances = rf_clf.feature_importances_
feature_names = breast_cancer.feature_names

# Sort feature importances in descending order
sorted_indices = np.argsort(feature_importances)[::-1]

print("Feature Importances:")
for idx in sorted_indices:
    print(f"{feature_names[idx]}: {feature_importances[idx]:.4f}")

24. Train a Random Forest Regressor and compare its performance with a single Decision Tree?
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, r2_score

# Load dataset
df = pd.read_csv("spotify_data.csv")  # Update with actual file path

# Select relevant features and target variable
features = ['Duration (ms)', 'Popularity']  # Update with more relevant features
target = 'Popularity'  # Assuming we predict track popularity

# Drop rows with missing values
df = df.dropna()

# Define X (features) and y (target)
X = df[features].drop(columns=['Popularity'])  # Features
y = df[target]  # Target variable

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Decision Tree Regressor
dt_model = DecisionTreeRegressor(random_state=42)
dt_model.fit(X_train, y_train)

# Train Random Forest Regressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Predictions
dt_preds = dt_model.predict(X_test)
rf_preds = rf_model.predict(X_test)

# Evaluate models
def evaluate(y_true, y_pred, model_name):
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    print(f"{model_name} Performance:")
    print(f"Mean Absolute Error: {mae:.2f}")
    print(f"R² Score: {r2:.4f}\n")

evaluate(y_test, dt_preds, "Decision Tree Regressor")
evaluate(y_test, rf_preds, "Random Forest Regressor")

25. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier?
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load dataset
df = pd.read_csv("spotify_data.csv")  # Update with actual file path

# Select relevant features and target variable
features = ['Duration (ms)', 'Popularity']  # Update with more relevant features
target = 'Popularity'  # Assuming we classify tracks into popular (1) or not (0)

# Convert popularity into a binary classification (Example: threshold = 50)
df['Popularity_Label'] = (df['Popularity'] >= 50).astype(int)

# Drop rows with missing values
df = df.dropna()

# Define X (features) and y (target)
X = df[features]
y = df['Popularity_Label']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Random Forest Classifier with OOB enabled
rf_model = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)
rf_model.fit(X_train, y_train)

# Compute OOB score
oob_score = rf_model.oob_score_
print(f"Out-of-Bag (OOB) Score: {oob_score:.4f}")

# Evaluate on test data
y_pred = rf_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Test Accuracy: {accuracy:.4f}")

26. Train a Bagging Classifier using SVM as a base estimator and print accuracy?
import pandas as pd
import numpy as np
from sklearn.ensemble import BaggingClassifier
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load dataset
df = pd.read_csv("spotify_data.csv")  # Update with actual file path

# Select relevant features and target variable
features = ['Duration (ms)', 'Popularity']  # Update with more relevant features
target = 'Popularity'  # Assuming we classify tracks into popular (1) or not (0)

# Convert popularity into a binary classification (Example: threshold = 50)
df['Popularity_Label'] = (df['Popularity'] >= 50).astype(int)

# Drop rows with missing values
df = df.dropna()

# Define X (features) and y (target)
X = df[features]
y = df['Popularity_Label']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create Bagging Classifier with SVM as base estimator
base_svm = SVC(kernel='rbf', probability=True)  # Using RBF kernel with probability=True for Bagging
bagging_model = BaggingClassifier(base_estimator=base_svm, n_estimators=10, random_state=42)
bagging_model.fit(X_train, y_train)

# Predictions
y_pred = bagging_model.predict(X_test)

# Compute Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Bagging Classifier with SVM Accuracy: {accuracy:.4f}")

27. Train a Random Forest Classifier with different numbers of trees and compare accuracy?
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load dataset
df = pd.read_csv("spotify_data.csv")  # Update with actual file path

# Select relevant features and target variable
features = ['Duration (ms)', 'Popularity']  # Update with more relevant features
target = 'Popularity'  # Assuming we classify tracks into popular (1) or not (0)

# Convert popularity into a binary classification (Example: threshold = 50)
df['Popularity_Label'] = (df['Popularity'] >= 50).astype(int)

# Drop rows with missing values
df = df.dropna()

# Define X (features) and y (target)
X = df[features]
y = df['Popularity_Label']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# List of different numbers of trees to test
n_trees = [10, 50, 100, 200, 500]
accuracy_scores = []

# Train and evaluate Random Forest with different numbers of trees
for n in n_trees:
    rf_model = RandomForestClassifier(n_estimators=n, random_state=42)
    rf_model.fit(X_train, y_train)
    y_pred = rf_model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    accuracy_scores.append(accuracy)
    print(f"Random Forest with {n} trees - Accuracy: {accuracy:.4f}")

# Plot accuracy vs number of trees
plt.figure(figsize=(8, 5))
plt.plot(n_trees, accuracy_scores, marker='o', linestyle='-', color='b')
plt.xlabel("Number of Trees (n_estimators)")
plt.ylabel("Accuracy")
plt.title("Random Forest Accuracy vs. Number of Trees")
plt.grid(True)
plt.show()

 28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score?
import pandas as pd
import numpy as np
from sklearn.ensemble import BaggingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

# Load dataset
df = pd.read_csv("spotify_data.csv")  # Update with actual file path

# Select relevant features and target variable
features = ['Duration (ms)', 'Popularity']  # Update with more relevant features
target = 'Popularity'  # Assuming we classify tracks into popular (1) or not (0)

# Convert popularity into a binary classification (Example: threshold = 50)
df['Popularity_Label'] = (df['Popularity'] >= 50).astype(int)

# Drop rows with missing values
df = df.dropna()

# Define X (features) and y (target)
X = df[features]
y = df['Popularity_Label']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create Bagging Classifier with Logistic Regression as base estimator
base_lr = LogisticRegression(max_iter=1000)  # Ensuring convergence
bagging_model = BaggingClassifier(base_estimator=base_lr, n_estimators=10, random_state=42)
bagging_model.fit(X_train, y_train)

# Predict probabilities for AUC calculation
y_prob = bagging_model.predict_proba(X_test)[:, 1]  # Get probability of class 1

# Compute AUC score
auc_score = roc_auc_score(y_test, y_prob)
print(f"Bagging Classifier with Logistic Regression - AUC Score: {auc_score:.4f}")

29. Train a Random Forest Regressor and analyze feature importance scores?
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

# Load dataset
df = pd.read_csv("spotify_data.csv")  # Update with actual file path

# Select relevant features and target variable
features = ['Duration (ms)', 'Popularity']  # Update with more relevant features
target = 'Popularity'  # Predicting track popularity

# Drop rows with missing values
df = df.dropna()

# Define X (features) and y (target)
X = df[features].drop(columns=['Popularity'])  # Features
y = df[target]  # Target variable

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Random Forest Regressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Extract feature importance
feature_importance = rf_model.feature_importances_
feature_names = X.columns

# Create a DataFrame for visualization
importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Print feature importance scores
print("Feature Importance Scores:")
print(importance_df)

# Plot feature importance
plt.figure(figsize=(8, 5))
plt.barh(importance_df['Feature'], importance_df['Importance'], color='royalblue')
plt.xlabel("Importance Score")
plt.ylabel("Feature")
plt.title("Feature Importance in Random Forest Regressor")
plt.gca().invert_yaxis()
plt.show()

30. Train an ensemble model using both Bagging and Random Forest and compare accuracy.
import pandas as pd
import numpy as np
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load dataset
df = pd.read_csv("spotify_data.csv")  # Update with actual file path

# Select relevant features and target variable
features = ['Duration (ms)', 'Popularity']  # Update with more relevant features
target = 'Popularity'  # Assuming we classify tracks into popular (1) or not (0)

# Convert popularity into a binary classification (Example: threshold = 50)
df['Popularity_Label'] = (df['Popularity'] >= 50).astype(int)

# Drop rows with missing values
df = df.dropna()

# Define X (features) and y (target)
X = df[features]
y = df['Popularity_Label']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Bagging Classifier (Base Estimator: Decision Tree)
bagging_model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)
bagging_model.fit(X_train, y_train)

# Train Random Forest Classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Predictions
y_pred_bagging = bagging_model.predict(X_test)
y_pred_rf = rf_model.predict(X_test)

# Compute Accuracy
accuracy_bagging = accuracy_score(y_test, y_pred_bagging)
accuracy_rf = accuracy_score(y_test, y_pred_rf)

# Print results
print(f"Bagging Classifier Accuracy: {accuracy_bagging:.4f}")
print(f"Random Forest Classifier Accuracy: {accuracy_rf:.4f}")

31.Train a Random Forest Classifier and tune hyperparameters using GridSearchCV.
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score

# Load dataset
df = pd.read_csv("spotify_data.csv")  # Update with actual file path

# Select relevant features and target variable
features = ['Duration (ms)', 'Popularity']  # Update with more relevant features
target = 'Popularity'  # Assuming we classify tracks into popular (1) or not (0)

# Convert popularity into a binary classification (Example: threshold = 50)
df['Popularity_Label'] = (df['Popularity'] >= 50).astype(int)

# Drop rows with missing values
df = df.dropna()

# Define X (features) and y (target)
X = df[features]
y = df['Popularity_Label']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define Random Forest model
rf_model = RandomForestClassifier(random_state=42)

# Define hyperparameter grid
param_grid = {
    'n_estimators': [50, 100, 200],         # Number of trees
    'max_depth': [None, 10, 20],            # Maximum depth of trees
    'min_samples_split': [2, 5, 10],        # Minimum samples to split a node
    'min_samples_leaf': [1, 2, 4],          # Minimum samples in a leaf node
    'bootstrap': [True, False]              # Whether to use bootstrap sampling
}

# Use GridSearchCV to find the best hyperparameters
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid,
                           cv=5, n_jobs=-1, verbose=2, scoring='accuracy')

# Fit the model
grid_search.fit(X_train, y_train)

# Get the best model and hyperparameters
best_rf_model = grid_search.best_estimator_
print("Best Hyperparameters:", grid_search.best_params_)

# Evaluate on test set
y_pred = best_rf_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Best Random Forest Model Accuracy: {accuracy:.4f}")

32. Train a Bagging Regressor with different numbers of base estimators and compare performance?
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import BaggingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score

# Load dataset
df = pd.read_csv("spotify_data.csv")  # Update with actual file path

# Select relevant features and target variable
features = ['Duration (ms)', 'Popularity']  # Update with more relevant features
target = 'Popularity'  # Predicting track popularity

# Drop rows with missing values
df = df.dropna()

# Define X (features) and y (target)
X = df[features].drop(columns=['Popularity'])  # Features
y = df[target]  # Target variable

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# List of different numbers of base estimators to test
n_estimators_list = [10, 50, 100, 200]
mae_scores = []
r2_scores = []

# Train and evaluate Bagging Regressor with different estimators
for n in n_estimators_list:
    bagging_model = BaggingRegressor(base_estimator=DecisionTreeRegressor(),
                                     n_estimators=n, random_state=42)
    bagging_model.fit(X_train, y_train)
    y_pred = bagging_model.predict(X_test)

    # Compute metrics
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    mae_scores.append(mae)
    r2_scores.append(r2)

    print(f"Bagging Regressor with {n} estimators - MAE: {mae:.4f}, R² Score: {r2:.4f}")

# Plot performance comparison
plt.figure(figsize=(8, 5))
plt.plot(n_estimators_list, mae_scores, marker='o', linestyle='-', color='b', label="MAE")
plt.plot(n_estimators_list, r2_scores, marker='s', linestyle='-', color='r', label="R² Score")
plt.xlabel("Number of Base Estimators")
plt.ylabel("Performance Metric")
plt.title("Bagging Regressor Performance vs. Number of Base Estimators")
plt.legend()
plt.grid(True)
plt.show()

33. Train a Random Forest Classifier and analyze misclassified samples?

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix

# Load dataset
df = pd.read_csv("spotify_data.csv")  # Update with actual file path

# Select relevant features and target variable
features = ['Duration (ms)', 'Popularity']  # Update with more relevant features
target = 'Popularity'  # Assuming we classify tracks into popular (1) or not (0)

# Convert popularity into a binary classification (Example: threshold = 50)
df['Popularity_Label'] = (df['Popularity'] >= 50).astype(int)

# Drop rows with missing values
df = df.dropna()

# Define X (features) and y (target)
X = df[features]
y = df['Popularity_Label']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Random Forest Classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Predictions
y_pred = rf_model.predict(X_test)

# Compute Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Random Forest Classifier Accuracy: {accuracy:.4f}")

# Identify misclassified samples
misclassified_indices = np.where(y_pred != y_test)[0]
misclassified_samples = X_test.iloc[misclassified_indices].copy()
misclassified_samples['Actual'] = y_test.iloc[misclassified_indices].values
misclassified_samples['Predicted'] = y_pred[misclassified_indices]

# Print first few misclassified samples
print("\nMisclassified Samples:")
print(misclassified_samples.head())

# Compute confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print(conf_matrix)

34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier?

import pandas as pd
import numpy as np
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load dataset
df = pd.read_csv("spotify_data.csv")  # Update with actual file path

# Select relevant features and target variable
features = ['Duration (ms)', 'Popularity']  # Update with more relevant features
target = 'Popularity'  # Assuming we classify tracks into popular (1) or not (0)

# Convert popularity into a binary classification (Example: threshold = 50)
df['Popularity_Label'] = (df['Popularity'] >= 50).astype(int)

# Drop rows with missing values
df = df.dropna()

# Define X (features) and y (target)
X = df[features]
y = df['Popularity_Label']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Decision Tree Classifier
dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train, y_train)
y_pred_dt = dt_model.predict(X_test)
accuracy_dt = accuracy_score(y_test, y_pred_dt)

# Train Bagging Classifier (Base Estimator: Decision Tree)
bagging_model = BaggingClassifier(base_estimator=DecisionTreeClassifier(),
                                  n_estimators=100, random_state=42)
bagging_model.fit(X_train, y_train)
y_pred_bagging = bagging_model.predict(X_test)
accuracy_bagging = accuracy_score(y_test, y_pred_bagging)

# Print accuracy comparison
print(f"Decision Tree Accuracy: {accuracy_dt:.4f}")
print(f"Bagging Classifier Accuracy: {accuracy_bagging:.4f}")

35. Train a Random Forest Classifier and visualize the confusion matrix?

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix

# Load dataset
df = pd.read_csv("spotify_data.csv")  # Update with actual file path

# Select relevant features and target variable
features = ['Duration (ms)', 'Popularity']  # Update with more relevant features
target = 'Popularity'  # Assuming we classify tracks into popular (1) or not (0)

# Convert popularity into a binary classification (Example: threshold = 50)
df['Popularity_Label'] = (df['Popularity'] >= 50).astype(int)

# Drop rows with missing values
df = df.dropna()

# Define X (features) and y (target)
X = df[features]
y = df['Popularity_Label']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Random Forest Classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Predictions
y_pred = rf_model.predict(X_test)

# Compute Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Random Forest Classifier Accuracy: {accuracy:.4f}")

# Compute Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Plot Confusion Matrix
plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=["Not Popular", "Popular"], yticklabels=["Not Popular", "Popular"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - Random Forest Classifier")
plt.show()

36. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy?

import pandas as pd
import numpy as np
from sklearn.ensemble import StackingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load dataset
df = pd.read_csv("spotify_data.csv")  # Update with actual file path

# Select relevant features and target variable
features = ['Duration (ms)', 'Popularity']  # Update with more relevant features
target = 'Popularity'  # Assuming we classify tracks into popular (1) or not (0)

# Convert popularity into a binary classification (Example: threshold = 50)
df['Popularity_Label'] = (df['Popularity'] >= 50).astype(int)

# Drop rows with missing values
df = df.dropna()

# Define X (features) and y (target)
X = df[features]
y = df['Popularity_Label']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define base learners
base_learners = [
    ('decision_tree', DecisionTreeClassifier(random_state=42)),
    ('svm', SVC(probability=True, random_state=42)),
    ('log_reg', LogisticRegression(max_iter=1000, random_state=42))
]

# Define Stacking Classifier (Meta-learner: Logistic Regression)
stacking_model = StackingClassifier(estimators=base_learners, final_estimator=LogisticRegression())

# Train Stacking Classifier
stacking_model.fit(X_train, y_train)
y_pred_stack = stacking_model.predict(X_test)
accuracy_stack = accuracy_score(y_test, y_pred_stack)

# Train individual models for comparison
models = {
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "SVM": SVC(probability=True, random_state=42),
    "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42)
}

accuracy_scores = {}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy_scores[name] = accuracy_score(y_test, y_pred)

# Print accuracy comparison
print("\nAccuracy Scores:")
for model_name, acc in accuracy_scores.items():
    print(f"{model_name}: {acc:.4f}")

print(f"Stacking Classifier Accuracy: {accuracy_stack:.4f}")

37. Train a Random Forest Classifier and print the top 5 most important features?

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Load dataset
df = pd.read_csv("spotify_data.csv")  # Update with actual file path

# Select relevant features and target variable
features = ['Duration (ms)', 'Danceability', 'Energy', 'Loudness', 'Tempo', 'Popularity']  # Example features
target = 'Popularity'  # Assuming we classify tracks into popular (1) or not (0)

# Convert popularity into a binary classification (Example: threshold = 50)
df['Popularity_Label'] = (df['Popularity'] >= 50).astype(int)

# Drop rows with missing values
df = df.dropna()

# Define X (features) and y (target)
X = df[features].drop(columns=['Popularity'])  # Exclude the original popularity column
y = df['Popularity_Label']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Random Forest Classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Extract feature importance scores
feature_importances = rf_model.feature_importances_

# Create a DataFrame to store feature importance
feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})

# Sort features by importance (descending order)
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Print top 5 most important features
print("Top 5 Most Important Features:")
print(feature_importance_df.head())

# Plot feature importance
plt.figure(figsize=(8, 5))
plt.barh(feature_importance_df['Feature'].head(5), feature_importance_df['Importance'].head(5), color='b')
plt.xlabel("Feature Importance Score")
plt.ylabel("Feature")
plt.title("Top 5 Most Important Features - Random Forest")
plt.gca().invert_yaxis()  # Invert y-axis for better visualization
plt.show()

38. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score?

import pandas as pd
import numpy as np
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score

# Load dataset
df = pd.read_csv("spotify_data.csv")  # Update with actual file path

# Select relevant features and target variable
features = ['Duration (ms)', 'Danceability', 'Energy', 'Loudness', 'Tempo', 'Popularity']  # Example features
target = 'Popularity'  # Assuming we classify tracks into popular (1) or not (0)

# Convert popularity into a binary classification (Example: threshold = 50)
df['Popularity_Label'] = (df['Popularity'] >= 50).astype(int)

# Drop rows with missing values
df = df.dropna()

# Define X (features) and y (target)
X = df[features].drop(columns=['Popularity'])  # Exclude the original popularity column
y = df['Popularity_Label']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Bagging Classifier (Base Estimator: Decision Tree)
bagging_model = BaggingClassifier(base_estimator=DecisionTreeClassifier(),
                                  n_estimators=100, random_state=42)
bagging_model.fit(X_train, y_train)

# Predictions
y_pred = bagging_model.predict(X_test)

# Compute Evaluation Metrics
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
accuracy = accuracy_score(y_test, y_pred)

# Print Performance Metrics
print(f"Bagging Classifier Performance:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")

39. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy?

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load dataset
df = pd.read_csv("spotify_data.csv")  # Update with actual file path

# Select relevant features and target variable
features = ['Duration (ms)', 'Danceability', 'Energy', 'Loudness', 'Tempo', 'Popularity']  # Example features
target = 'Popularity'  # Assuming we classify tracks into popular (1) or not (0)

# Convert popularity into a binary classification (Example: threshold = 50)
df['Popularity_Label'] = (df['Popularity'] >= 50).astype(int)

# Drop rows with missing values
df = df.dropna()

# Define X (features) and y (target)
X = df[features].drop(columns=['Popularity'])  # Exclude the original popularity column
y = df['Popularity_Label']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Experiment with different max_depth values
max_depth_values = range(1, 21)  # Testing depths from 1 to 20
accuracy_scores = []

for depth in max_depth_values:
    # Train Random Forest Classifier with current max_depth
    rf_model = RandomForestClassifier(n_estimators=100, max_depth=depth, random_state=42)
    rf_model.fit(X_train, y_train)

    # Predict on test data
    y_pred = rf_model.predict(X_test)

    # Compute accuracy
    accuracy = accuracy_score(y_test, y_pred)
    accuracy_scores.append(accuracy)

# Plot Accuracy vs. Max Depth
plt.figure(figsize=(8, 5))
plt.plot(max_depth_values, accuracy_scores, marker='o', linestyle='-', color='b')
plt.xlabel("Max Depth")
plt.ylabel("Accuracy")
plt.title("Effect of max_depth on Random Forest Accuracy")
plt.xticks(max_depth_values)
plt.grid()
plt.show()

40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare
performance?
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import BaggingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score

# Load dataset
df = pd.read_csv("spotify_data.csv")  # Update with actual file path

# Select relevant features and target variable
features = ['Danceability', 'Energy', 'Loudness', 'Tempo', 'Acousticness', 'Instrumentalness']
target = 'Popularity'

# Drop rows with missing values
df = df.dropna()

# Define X (features) and y (target)
X = df[features]
y = df[target]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define base estimators
dt_regressor = DecisionTreeRegressor(random_state=42)
knn_regressor = KNeighborsRegressor()

# Train Bagging Regressor with Decision Tree
bagging_dt = BaggingRegressor(base_estimator=dt_regressor, n_estimators=50, random_state=42)
bagging_dt.fit(X_train, y_train)
y_pred_dt = bagging_dt.predict(X_test)

# Train Bagging Regressor with KNeighbors
bagging_knn = BaggingRegressor(base_estimator=knn_regressor, n_estimators=50, random_state=42)
bagging_knn.fit(X_train, y_train)
y_pred_knn = bagging_knn.predict(X_test)

# Evaluate performance using MAE and R² Score
mae_dt = mean_absolute_error(y_test, y_pred_dt)
r2_dt = r2_score(y_test, y_pred_dt)

mae_knn = mean_absolute_error(y_test, y_pred_knn)
r2_knn = r2_score(y_test, y_pred_knn)

# Print performance metrics
print("Performance Comparison:")
print(f"Bagging Regressor (Decision Tree) - MAE: {mae_dt:.4f}, R² Score: {r2_dt:.4f}")
print(f"Bagging Regressor (KNeighbors) - MAE: {mae_knn:.4f}, R² Score: {r2_knn:.4f}")

# Visualize R² scores
plt.bar(["Decision Tree", "KNeighbors"], [r2_dt, r2_knn], color=['blue', 'green'])
plt.xlabel("Base Estimator")
plt.ylabel("R² Score")
plt.title("Comparison of Bagging Regressors")
plt.show()

41. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score?

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, roc_curve, auc

# Load dataset
df = pd.read_csv("spotify_data.csv")  # Update with actual file path

# Select relevant features and target variable
features = ['Duration (ms)', 'Danceability', 'Energy', 'Loudness', 'Tempo', 'Popularity']  # Example features
target = 'Popularity'  # Assuming we classify tracks into popular (1) or not (0)

# Convert popularity into a binary classification (Example: threshold = 50)
df['Popularity_Label'] = (df['Popularity'] >= 50).astype(int)

# Drop rows with missing values
df = df.dropna()

# Define X (features) and y (target)
X = df[features].drop(columns=['Popularity'])  # Exclude the original popularity column
y = df['Popularity_Label']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Random Forest Classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Predict probabilities for ROC calculation
y_probs = rf_model.predict_proba(X_test)[:, 1]  # Probability of class 1

# Compute ROC-AUC Score
roc_auc = roc_auc_score(y_test, y_probs)
print(f"ROC-AUC Score: {roc_auc:.4f}")

# Compute ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_probs)

# Plot ROC Curve
plt.figure(figsize=(8, 5))
plt.plot(fpr, tpr, color='b', label=f"ROC Curve (AUC = {roc_auc:.4f})")
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - Random Forest Classifier")
plt.legend()
plt.grid()
plt.show()

42. Train a Bagging Classifier and evaluate its performance using cross-validatio.

import pandas as pd
import numpy as np
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score, train_test_split

# Load dataset
df = pd.read_csv("spotify_data.csv")  # Update with actual file path

# Select relevant features and target variable
features = ['Duration (ms)', 'Danceability', 'Energy', 'Loudness', 'Tempo', 'Popularity']  # Example features
target = 'Popularity'  # Assuming we classify tracks into popular (1) or not (0)

# Convert popularity into a binary classification (Example: threshold = 50)
df['Popularity_Label'] = (df['Popularity'] >= 50).astype(int)

# Drop rows with missing values
df = df.dropna()

# Define X (features) and y (target)
X = df[features].drop(columns=['Popularity'])  # Exclude the original popularity column
y = df['Popularity_Label']

# Initialize Bagging Classifier with Decision Tree as base estimator
bagging_model = BaggingClassifier(base_estimator=DecisionTreeClassifier(),
                                  n_estimators=100, random_state=42)

# Perform 5-fold cross-validation
cv_scores = cross_val_score(bagging_model, X, y, cv=5, scoring='accuracy')

# Print mean accuracy and standard deviation
print(f"Cross-Validation Accuracy Scores: {cv_scores}")
print(f"Mean Accuracy: {np.mean(cv_scores):.4f}")
print(f"Standard Deviation: {np.std(cv_scores):.4f}")

43. Train a Random Forest Classifier and plot the Precision-Recall curv?

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_curve, auc, average_precision_score

# Load dataset
df = pd.read_csv("spotify_data.csv")  # Update with actual file path

# Select relevant features and target variable
features = ['Duration (ms)', 'Danceability', 'Energy', 'Loudness', 'Tempo', 'Popularity']  # Example features
target = 'Popularity'  # Assuming we classify tracks into popular (1) or not (0)

# Convert popularity into a binary classification (Example: threshold = 50)
df['Popularity_Label'] = (df['Popularity'] >= 50).astype(int)

# Drop rows with missing values
df = df.dropna()

# Define X (features) and y (target)
X = df[features].drop(columns=['Popularity'])  # Exclude the original popularity column
y = df['Popularity_Label']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Random Forest Classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Predict probabilities for Precision-Recall calculation
y_probs = rf_model.predict_proba(X_test)[:, 1]  # Probability of class 1

# Compute Precision-Recall curve
precision, recall, _ = precision_recall_curve(y_test, y_probs)

# Compute AUC for Precision-Recall Curve
pr_auc = auc(recall, precision)
avg_precision = average_precision_score(y_test, y_probs)

# Plot Precision-Recall Curve
plt.figure(figsize=(8, 5))
plt.plot(recall, precision, color='b', label=f"PR Curve (AUC = {pr_auc:.4f})")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve - Random Forest Classifier")
plt.legend()
plt.grid()
plt.show()

# Print Average Precision Score
print(f"Average Precision Score: {avg_precision:.4f}")

44. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy?

import pandas as pd
import numpy as np
from sklearn.ensemble import StackingClassifier, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score

# Load dataset
df = pd.read_csv("spotify_data.csv")  # Update with actual file path

# Select relevant features and target variable
features = ['Duration (ms)', 'Danceability', 'Energy', 'Loudness', 'Tempo', 'Popularity']  # Example features
target = 'Popularity'  # Assuming we classify tracks into popular (1) or not (0)

# Convert popularity into a binary classification (Example: threshold = 50)
df['Popularity_Label'] = (df['Popularity'] >= 50).astype(int)

# Drop rows with missing values
df = df.dropna()

# Define X (features) and y (target)
X = df[features].drop(columns=['Popularity'])  # Exclude the original popularity column
y = df['Popularity_Label']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define base estimators
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
lr_model = LogisticRegression(max_iter=1000)

# Define Stacking Classifier with Logistic Regression as meta-classifier
stacking_model = StackingClassifier(
    estimators=[('rf', rf_model), ('lr', lr_model)],
    final_estimator=LogisticRegression()
)

# Train models
rf_model.fit(X_train, y_train)
lr_model.fit(X_train, y_train)
stacking_model.fit(X_train, y_train)

# Make predictions
y_pred_rf = rf_model.predict(X_test)
y_pred_lr = lr_model.predict(X_test)
y_pred_stack = stacking_model.predict(X_test)

# Compute accuracy
acc_rf = accuracy_score(y_test, y_pred_rf)
acc_lr = accuracy_score(y_test, y_pred_lr)
acc_stack = accuracy_score(y_test, y_pred_stack)

# Print accuracy comparison
print(f"Random Forest Accuracy: {acc_rf:.4f}")
print(f"Logistic Regression Accuracy: {acc_lr:.4f}")
print(f"Stacking Classifier Accuracy: {acc_stack:.4f}")

45. Train a Bagging Regressor with different levels of bootstrap samples and compare performance.

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import BaggingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Load dataset
df = pd.read_csv("spotify_data.csv")  # Update with actual file path

# Select relevant features and target variable
features = ['Danceability', 'Energy', 'Loudness', 'Tempo', 'Popularity']  # Example features
target = 'Popularity'  # Regression task

# Drop rows with missing values
df = df.dropna()

# Define X (features) and y (target)
X = df[features].drop(columns=['Popularity'])  # Exclude the original popularity column
y = df['Popularity']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define different bootstrap sample sizes
bootstrap_sizes = [0.5, 0.7, 1.0]  # 50%, 70%, and 100% of training samples
mse_scores = []

# Train Bagging Regressor for different bootstrap sizes
for size in bootstrap_sizes:
    bagging_model = BaggingRegressor(
        base_estimator=DecisionTreeRegressor(),
        n_estimators=100,
        max_samples=size,  # Varying bootstrap sample size
        random_state=42
    )
    bagging_model.fit(X_train, y_train)

    # Predict and compute MSE
    y_pred = bagging_model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    mse_scores.append(mse)
    print(f"Bootstrap Sample Size {size*100:.0f}% - MSE: {mse:.4f}")

# Plot Bootstrap Sample Size vs. MSE
plt.figure(figsize=(8, 5))
plt.plot(bootstrap_sizes, mse_scores, marker='o', linestyle='--', color='b')
plt.xlabel("Bootstrap Sample Size")
plt.ylabel("Mean Squared Error (MSE)")
plt.title("Effect of Bootstrap Sample Size on Bagging Regressor Performance")
plt.xticks(bootstrap_sizes)
plt.grid()
plt.show()